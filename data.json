{
  "papers": [
    {
      "id": "https://arxiv.org/pdf/2403.12122.pdf",
      "paragraphs": [
        {
          "source": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
          "translations": [
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine a spinning disk of primordial planetesimals, the building blocks of planets. The way this disk disperses influences the orbital evolution of the giant planets in our solar system. Let's take a look at Table 3, where we've noted some important factors in the final layouts of these giant planets after 4 billion years of evolution.\n\nIn the first column, we've noted the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48. However, in our simulated systems, we've seen a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this primarily impacts the inner solar system, not the Kuiper belt.\n\nNow, the structure of the Kuiper belt is influenced by the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97 to 2.31, with an average of 2.13.\n\nWe've also looked at a large number of simulations from previous studies, and found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain, or simply a matter of chance among our 12 simulated systems.\n\nHowever, it's important to note that if Uranus and Neptune's orbits cross at a 2:1 ratio, it can destabilize much of the Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a desirable feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 1
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "The movement of an original planet disk can cause changes in the orbits of other giant planets. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of changes. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1–2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the ratio of Neptune’s to Uranus’ orbital period, which we've listed in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97– 2.31, with an average of 2.13. Looking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of under 2. It's unclear whether this is a problem with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations meant to study Kuiper belt formation.",
              "rank": 2
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The movement of an original planet-like disk influences the development of other giant planets' orbits. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of development. The first column shows the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1 to 2.7, with an average of 2.39. While this exact ratio affects the locations of various resonances, it mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. These original systems had ratios ranging from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we found that about half of all 4-planet instability outcomes ended with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. \n\nHowever, the crossing of Uranus and Neptune's 2:1 resonance can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 3
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. Key parameters in the final architectures of these planets after 4 Gyrs of evolution include the ratio of Saturn’s to Jupiter’s orbital period and the ratio of Neptune’s to Uranus’ orbital period. The former has dynamical consequences primarily for the inner solar system, while the latter affects the Kuiper belt’s architecture. In simulations, the Saturn to Jupiter period ratio ranged from 2.1–2.7, with a median of 2.39. The Neptune to Uranus period ratio in original systems ranged from 1.97– 2.31, with a median of 2.13. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize the resonant Kuiper belt population, limiting the utility of simulation results.",
              "rank": 4
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. The study lists key parameters in the final architectures of these planets after 4 billion years of evolution. The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48, but the simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39. This ratio primarily affects the inner solar system rather than the Kuiper belt. However, the Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period. In the simulations, all but one system evolved above a period ratio of 2. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.",
              "rank": 5
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets.\n- Key parameters of the final architectures of the giant planets after 4 Gyrs of evolution are listed in Table 3.\n- The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48.\n- The simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39.\n- The exact ratio has dynamical consequences primarily for the inner solar system, not the Kuiper belt.\n- The Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period.\n- In the original simulations, all but one system evolved above a period ratio of 2, with values ranging from 1.97– 2.31 and a median of 2.13.\n- About half of all 4-planet instability outcomes from prior works finish with a Neptune-to-Uranus period ratio of under 2.\n- The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.\n- This crossing is a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 6
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of the other giant planets too. In Table 3, we provide important parameters in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn’s orbital period to Jupiter’s. In the actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. Although the positions of various secular resonances depend on this exact ratio, this mainly affects the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune’s orbital period to Uranus’. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, the crossing of Uranus and Neptune’s 2:1 mean motion resonance (MMR) can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 7
            },
            {
              "id": "06-simplify-college.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of other giant planets. Table 3 outlines crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems display a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the exact ratio impacts the locations of various secular resonances, this primarily affects the inner solar system rather than the Kuiper belt. \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. The second column of Table 3 presents these final ratios for our systems. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon reviewing numerous simulations from previous studies, we found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's uncertain whether this is a systematic issue with our specific 5-planet resonant chain or simply an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 mean motion resonance (MMR) can destabilize a significant portion of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore an undesirable feature for simulations intended to study Kuiper belt formation.",
              "rank": 8
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "The scattering of an original planetesimal disk instigates orbital evolution among the remaining giant planets. Table 3 outlines the crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column of the table presents the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems exhibit a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the precise locations of various secular resonances hinge on this exact ratio, the dynamical implications are primarily for the inner solar system, not the Kuiper belt (see Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020). \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We present these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these original systems varied from 1.97 to 2.31, with a median of 2.13. Upon reviewing a significant number of approximately 1000-particle simulations from previous studies (Clement et al. 2018, 2021c,a), we discovered that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains uncertain whether this is an unobserved systematic issue with our specific 5-planet resonant chain or merely an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize a large portion of the resonant Kuiper belt population, thereby limiting the usefulness of our simulation results. Consequently, it is an undesirable feature for simulations intended to investigate Kuiper belt formation (Graham & Volk 2024).",
              "rank": 9
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"The scattering of an initial planetesimal disk influences the orbital evolution of other giant planets. Table 3 shows key parameters in the final structures of these planets after 4 Gyrs of evolution. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In the actual solar system, this ratio is about 2.48, but our simulated systems range from 2.1–2.7, with a median of 2.39. This ratio mainly affects the inner solar system's dynamics rather than the Kuiper belt. However, the Kuiper belt’s structure relies on the ratio of Neptune’s to Uranus’ orbital period, listed in Table 3's second column. In our initial simulations, all but one system (2500Pb) had a period ratio above 2. These original systems ranged from 1.97– 2.31, with a median of 2.13. Reviewing numerous ∼1000-particle simulations from previous studies, we find about half of all 4-planet instability results end with a Neptune-to-Uranus period ratio below 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, Uranus and Neptune’s 2:1 MMR crossing can destabilize much of the resonant Kuiper belt population, limiting our simulation results' usefulness. Therefore, it's not ideal for simulations studying Kuiper belt formation.\"",
              "rank": 10
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital changes of other giant planets too. Table 3 outlines the main factors in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio affects the locations of various secular resonances, this primarily impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 11
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to guide you in understanding the following passage:\n\nThe movement of an original planetesimal disk influences the orbital evolution of the giant planets. In Table 3, we present crucial parameters in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In our solar system, this ratio is approximately 2.48, but our simulated systems show a range of ratios from 2.1–2.7, with a median value of 2.39. While the exact ratio impacts the locations of various secular resonances, this primarily affects the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune’s to Uranus’ orbital period, which we present in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these original systems varied from 1.97– 2.31, with a median of 2.13. After reviewing numerous simulations from previous studies, we found that about half of all 4-planet instability outcomes ended with a Neptune-to-Uranus period ratio of under 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or simply an unfortunate outcome among our 12 simulated systems. However, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results, and is therefore not ideal for simulations intended to study Kuiper belt formation.",
              "rank": 12
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "When a planet breaks apart, it can change the paths of other big planets too. In Table 3, we show important details about how the big planets changed after 4 billion years. In the first column, we show how long it takes Saturn to orbit the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times longer, with an average of 2.39 times. This can affect the inner solar system more than the outer part, where the Kuiper belt is.\n\nHowever, the structure of the Kuiper belt depends on how long it takes Neptune to orbit the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times longer, with an average of 2.13 times. \n\nLooking at other computer models with about 1000 particles, we found that about half of all 4-planet systems ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our 5-planet model or just bad luck in our 12 models. \n\nBut when Neptune and Uranus cross each other's paths, it can mess up a lot of the Kuiper belt. This makes our models less useful for studying how the Kuiper belt was formed.",
              "rank": 13
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the cosmic ballet of the planets, the scattering of an ancient disk of planetesimals, the building blocks of planets, influences the orbital dance of the gas giants as well. In the cosmic ledger we call Table 3, we've noted the defining characteristics of the final arrangements of these gas giants after 4 billion years of cosmic evolution. \n\nFirstly, we've noted the rhythm of their dance - the ratio of Saturn's orbital period to Jupiter's. In our own cosmic neighborhood, this ratio is approximately 2.48. However, in our digital cosmos, the ratios vary, ranging from 2.1 to 2.7, with a median value of 2.39. While the exact ratio influences the locations of various gravitational harmonies, its impact is felt primarily in the inner solar system, rather than the distant Kuiper belt.\n\nHowever, the architectural blueprint of the Kuiper belt is influenced by the rhythm of Neptune's dance with Uranus, and we've noted these final ratios in the second column of our cosmic ledger. In our initial digital cosmos, all but one system evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon examining a multitude of simulations from previous works, we find that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains unclear whether this is a systematic issue with our specific 5-planet resonant chain or simply a stroke of cosmic misfortune among our 12 simulated systems. \n\nRegardless, the crossing of Uranus and Neptune's 2:1 gravitational harmony can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. This makes it a less than ideal feature for simulations intended to study the birth of the Kuiper belt.",
              "rank": 14
            },
            {
              "id": "source",
              "content": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
              "rank": 15
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "The movement of an early group of small planets can affect the paths of larger planets too. In Table 3, we've noted important details about the final positions of the large planets after 4 billion years. The first column shows the comparison between the time it takes Saturn and Jupiter to orbit the sun. In our actual solar system, Saturn takes about 2.48 times longer than Jupiter, but in our computer models, this time varied between 2.1 and 2.7 times, with an average of 2.39 times. This difference can affect the inner solar system more than the outer region known as the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the comparison between the time it takes Neptune and Uranus to orbit the sun. We've noted these comparisons for our models in the second column of Table 3. In our initial models, all but one showed Neptune taking more than twice as long as Uranus. The times varied between 1.97 and 2.31 times, with an average of 2.13 times. Looking at many previous models, we found that about half ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our specific model or just bad luck in our 12 models. However, if Neptune and Uranus cross paths at a 2:1 ratio, it can disrupt much of the Kuiper belt, which makes our model results less useful. This is not a good feature for models meant to study the formation of the Kuiper belt.",
              "rank": 16
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "A group of early planets can change the paths of other big planets. In Table 3, we show important details about the final paths of the big planets after 4 billion years. The first column shows how long it takes Saturn to go around the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times, with an average of 2.39 times. This difference mainly affects the inner solar system, not the outer area called the Kuiper belt.\n\nHowever, the Kuiper belt is affected by how long Neptune takes to go around the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times, with an average of 2.13 times. \n\nLooking at many other computer models, we found that about half of them had Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our model or just bad luck in our 12 systems. But if Neptune and Uranus get too close, it can mess up the Kuiper belt. This makes our model less useful for studying how the Kuiper belt was formed.",
              "rank": 17
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of scientific diagrams, the paragraph could be represented as follows:\n\n[Diagram 1: A timeline showing 4 Gyrs of evolution, with a primordial planetesimal disk at the start and the final architectures of the giant planets at the end.]\n\n[Table 3: A table with two columns. The first column is labeled \"Ratio of Saturn's to Jupiter's orbital period\" and lists a range of ratios from 2.1 to 2.7, with a median value of 2.39. The second column is labeled \"Ratio of Neptune's to Uranus' orbital period\" and lists a range of ratios from 1.97 to 2.31, with a median value of 2.13.]\n\n[Diagram 2: A graph showing the locations of various secular resonances, with a note indicating that these have dynamical consequences primarily for the inner solar system.]\n\n[Diagram 3: A graph showing the architecture of the Kuiper belt, with a note indicating that this is dependent on the ratio of Neptune's to Uranus' orbital period.]\n\n[Diagram 4: A pie chart showing the outcomes of ~1000-particle simulations, with roughly half finishing with a Neptune-to-Uranus period ratio of under 2.]\n\n[Diagram 5: A graph showing the crossing of Uranus and Neptune's 2:1 MMR, with a note indicating that this can destabilize much of the resonant Kuiper belt population.]\n\n[Text Box: A note stating that the crossing of Uranus and Neptune's 2:1 MMR is a poor feature for simulations meant to study Kuiper belt formation.]\n\nTranslation to English: The timeline, table, and diagrams represent the evolution of a primordial planetesimal disk and the final architectures of the giant planets after 4 Gyrs. The table lists key parameters, including the ratios of Saturn's to Jupiter's orbital period and Neptune's to Uranus' orbital period. The diagrams show the locations of various secular resonances, the architecture of the Kuiper belt, the outcomes of ~1000-particle simulations, and the crossing of Uranus and Neptune's 2:1 MMR. The text box notes that the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population and is therefore a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 18
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12183.pdf",
      "paragraphs": [
        {
          "source": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
          "translations": [
            {
              "id": "01-writing-simple-improve.ts",
              "content": "This study examines the resilience of stable matchings when faced with any kind of disruption, even minor ones. These disruptions can occur in both decentralized and centralized settings. Over time, changes in participants' preferences, market composition, or the termination of matched partnerships can occur. For example, in labor markets, a family emergency might cause a worker to prefer a different geographical location; new job positions may become available, or workers may retire; an employer and employee might mutually agree to end their employment contract. While these changes can alter the set of stable matchings, they can also lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. \n\nIn some cases, decentralized interactions cannot break certain partnerships. Consider a two-sided market divided into two submarkets, New York and Los Angeles. If every person in Los Angeles prefers any other person in Los Angeles to any New Yorker, and vice versa, and if the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings, then the entire market naturally has two stable matchings as well. If all people in Los Angeles are paired according to one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched people in Los Angeles. As a result, the stabilization dynamics cannot achieve another stable matching in the entire market. \n\nIn this scenario, people in Los Angeles can be matched stably within the group, with each person preferring their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable matchings in the entire market match participants inside the fragment in the same way. Specifically, people in Los Angeles constitute a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot lead to certain stable matchings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable matching. \n\nTheorem 1 suggests one type of vulnerability of stable matchings. A small disruption of a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The well-known Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable matching, the best stable matching for one side of the market, doctors and students, respectively. Our results imply that any disruption of such a clearinghouse’s outcome does not necessarily revert back to the intended extreme stable matching; decentralized interactions can instead lead to the other extreme stable matching, or anything in between."
            },
            {
              "id": "source",
              "content": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between."
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study examines the stability of matchings in response to potential perturbations, such as changes in participants' preferences or market composition. These changes could lead to instability, with participants being mismatched or unmatched. The study introduces the concept of a 'fragment', a group where members prefer to be matched within the group. Fragments can be 'trivial' or 'non-trivial', with non-trivial fragments being the only restraints on stabilization dynamics. The absence of non-trivial fragments suggests that most markets can attain any stable matching. The study also suggests that a small perturbation of a stable matching could result in any stable matching, and that decentralized interactions can lead to different outcomes than intended."
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"We examine the stability of matchings under any perturbations, which can occur in both decentralized and centralized settings. Changes such as shifts in participants' preferences, alterations in market composition, or termination of partnerships can affect the set of stable matchings, potentially leading to instability. Some participants may be mismatched or unmatched under the current market conditions. In certain situations, decentralized interactions cannot dissolve certain partnerships. \n\nConsider a two-sided market split into two submarkets, New York and Los Angeles, where every resident prefers any local to any outsider. If the New York submarket has one stable matching and the Los Angeles submarket has two, the entire market naturally has two stable matchings. If all Los Angeles residents are paired according to one of the two stable matchings for their submarket, decentralized interactions cannot unpair any such matched pair. Consequently, the stabilization dynamics cannot achieve another stable matching in the entire market. \n\nIn this scenario, Los Angeles residents can be stably matched within the group, with every member preferring their stable partner to anyone outside Los Angeles. We term such a group a fragment. New Yorkers also form a fragment. A fragment is trivial if all stable matchings in the entire market match participants within the fragment in the same way. Los Angeles residents form a non-trivial fragment, while New Yorkers form a trivial one. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot produce certain stable matchings. Theorem 1 shows the reverse is also true. Without nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. Their absence is a mild condition, as simulations suggest they are rare in large random markets. Therefore, in most markets, the decentralized process can achieve any stable matching. \n\nTheorem 1 indicates one type of fragility of stable matchings. A minor perturbation of a stable matching may result in any stable matching, near or far from the original one. The renowned Deferred Acceptance mechanism, used to match doctors to hospitals and students to schools, aims to implement an extremal stable matching, the best one for one market side. Our results suggest that any perturbation of such a clearinghouse’s outcome doesn't have to revert back to the intended extremal stable matching; decentralized interactions can lead to the other extremal stable matching, or anything in between.\""
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "We're looking at how stable matchings, or pairings, hold up under various changes. These changes could be small or large and can happen in different settings. Over time, people's preferences might change, the makeup of the market might shift, or paired participants might decide to end their partnerships. For example, in job markets, a change in a worker's family situation might cause them to prefer a different location; new jobs might open up and workers might become available or retire; an employer and an employee might mutually decide to end their working relationship. While these changes might also affect the set of stable matchings, they could all lead to instability. Specifically, some participants that should be paired under the current market conditions might be mismatched or not matched at all. In some cases, certain partnerships can't be broken up by decentralized interactions. \n\nLet's imagine a market split into two submarkets, New York and Los Angeles, where everyone in LA prefers any LA resident to any New Yorker, and vice versa. Let's say that the New York submarket has one stable matching and the LA submarket has two. The entire market naturally has two stable matchings as well. If all LA residents are paired according to one of the two stable matchings for the LA submarket, and therefore for the entire market, decentralized interactions can't unmatch any such pair of matched LA residents. As a result, the stabilization dynamics can't reach another stable matching in the entire market. In this case, LA residents can be matched in a stable way within the group so that every insider prefers their stable partner to anyone outside LA. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In this case, LA residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings can't lead to certain stable matchings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to reach any stable matching. \n\nTheorem 1 suggests one type of fragility of stable matchings. A small change in a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any change in such a clearinghouse’s outcome doesn't have to revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between."
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "We're looking at how stable matches can handle any kind of changes, even small ones. These changes can happen in different ways. For example, people's preferences might change over time, the market might change, or people who were matched might decide to stop being partners. In job markets, a change in a worker's family situation might make them want to move to a different location. New jobs might open up, or workers might become available or retire. An employer and an employee might decide together to end their work relationship. While these changes might also change the set of stable matches, they could all cause instability. Specifically, some people who should be matched under the current market conditions might be mismatched or not matched at all. In some cases, certain partnerships can't be broken up by decentralized interactions.\n\nLet's imagine a market that's split into two submarkets, New York and Los Angeles. Everyone in Los Angeles prefers anyone in Los Angeles to anyone in New York, and the same is true for New Yorkers. Let's say that the New York submarket has one stable match and the Los Angeles submarket has two. The whole market naturally has two stable matches as well. If all the people in Los Angeles are paired according to one of the two stable matches for the Los Angeles submarket, and therefore for the whole market, decentralized interactions can't unmatch any pair of matched people in Los Angeles. As a result, the dynamics of stabilization can't reach another stable match in the whole market. In this case, people in Los Angeles can be matched in a stable way within the group so that everyone inside prefers their stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matches in the whole market match participants inside the fragment in the same way. In this case, people in Los Angeles make up a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable matches can't lead to certain stable matches. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable match can lead to any stable match through decentralized interactions. Non-trivial fragments are the only limits on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. So, in most markets, the decentralized process is flexible enough to reach any stable match. Theorem 1 suggests one type of fragility of stable matches. A small change in a stable match might end up in any stable match, whether it's close to or far from the original stable match. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable match, the best stable match for one side of the market, doctors and students, respectively. Our results imply that any change in such a clearinghouse’s outcome doesn't have to go back to the intended extreme stable match; decentralized interactions can instead lead to the other extreme stable match, or anything in between."
            },
            {
              "id": "06-simplify-college.ts",
              "content": "We examine the resilience of stable pairings in the face of arbitrary, potentially minor, disruptions. These disruptions can occur in both decentralized and centralized environments. Over time, changes in participants' preferences, market composition, or the dissolution of matched partnerships can occur. For example, in job markets, a change in a worker's family situation may lead to new geographical preferences; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be matched under current market conditions may be mismatched or not matched at all. In some cases, decentralized interactions cannot dissolve certain partnerships. Consider a two-sided market split into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has one stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Los Angeles residents. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within the group so that every member prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers also form a fragment. A fragment is considered trivial if all stable pairings in the entire market match participants within the fragment in the same way. Specifically, Los Angeles residents constitute a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can result in any stable pairing through decentralized interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. Theorem 1 suggests one form of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extremal stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse's outcome does not necessarily have to revert back to the intended extremal stable pairing; decentralized interactions can instead lead to the other extremal stable pairing, or anything in between."
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "We're looking at how stable pairings hold up under different changes, which could be small or large. These changes can happen in different ways. For example, people's likes and dislikes might change over time, the makeup of a group might change, or people who were paired up might decide to stop being a pair. In job markets, a change in a worker's family situation might make them want to move to a different location; new jobs might come up and workers might start or stop working; a boss and a worker might decide to end their working relationship. While these changes might also change the stable pairings, they could all cause instability. Specifically, some people who should be paired up based on the current situation might end up not paired up or paired up with the wrong person. In some cases, certain pairings can't be broken up through normal interactions. \n\nLet's say there's a market split into two smaller markets, New York and Los Angeles, where everyone in Los Angeles prefers anyone in Los Angeles over anyone in New York, and the same goes for New Yorkers. If the New York market has one stable pairing and the Los Angeles market has two, then the whole market naturally has two stable pairings. If all the people in Los Angeles are paired up according to one of the two stable pairings for the Los Angeles market, and therefore for the whole market, normal interactions can't break up any of these Los Angeles pairs. As a result, the process of stabilizing can't reach another stable pairing in the whole market. In this case, people in Los Angeles can be paired up in a stable way within their group, so that everyone in the group prefers their stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is simple if all stable pairings in the whole market pair up people inside the fragment in the same way. Specifically, people in Los Angeles make up a complex fragment, while New Yorkers make up a simple fragment. If a market has a complex fragment, some unstable pairings can't reach certain stable pairings. Our first rule shows that the opposite is also true. When there are no complex fragments, any unstable pairing can reach any stable pairing through normal interactions. Complex fragments are the only limits on the process of stabilizing. The lack of complex fragments is a mild condition: tests suggest that they are rare in large random markets. So, in most markets, the normal process is flexible enough to reach any stable pairing. \n\nOur first rule suggests one way that stable pairings can be fragile. A small change in a stable pairing might end up in any stable pairing, whether it's close to or far from the original stable pairing. The famous Deferred Acceptance method, which is used to pair up doctors with hospitals and students with schools, aims to achieve the best stable pairing for one side of the market, either doctors or students. Our results suggest that any change in such a system's result doesn't have to go back to the intended best stable pairing; normal interactions can instead lead to the other best stable pairing, or anything in between."
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "We're looking at how stable matches can handle changes. These changes can happen in different ways. For example, people's likes and dislikes might change, the number of people in the market might change, or people who were matched might decide to stop being matched. This can happen in job markets. A worker might need to move because of their family, new jobs might open up, or a worker might retire. An employer and a worker might also decide to stop working together. These changes can mess up the stable matches. Some people who should be matched might not be, or they might be matched with the wrong person. Sometimes, it's not possible to break up certain matches. \n\nLet's say there's a market split into two parts: New York and Los Angeles. Everyone in Los Angeles prefers people from Los Angeles, and everyone in New York prefers people from New York. If New York has one stable match and Los Angeles has two, then the whole market has two stable matches. If all the people in Los Angeles are matched according to one of the two stable matches, then it's not possible to unmatch any of them. This means that it's not possible to get to the other stable match. In this case, the people in Los Angeles can be matched in a stable way within their group, so that everyone prefers their match to anyone outside of Los Angeles. We call this a \"fragment\". In this example, the people in New York are also a fragment. A fragment is \"trivial\" if all the stable matches in the whole market match people inside the fragment in the same way. In this case, the people in Los Angeles are a non-trivial fragment, and the people in New York are a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matches can't get to certain stable matches. But, if there are no non-trivial fragments, any unstable match can get to any stable match. Non-trivial fragments are the only thing that can stop this from happening. They don't happen very often in big random markets. So, in most markets, it's possible to get to any stable match. \n\nThis shows one way that stable matches can be fragile. A small change to a stable match can lead to any other stable match, whether it's close to or far from the original match. The famous Deferred Acceptance method, which is used to match doctors to hospitals and students to schools, tries to get the best stable match for one side of the market. But, our results show that any change to this match doesn't have to go back to the original match. It could lead to the other best match, or anything in between."
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "We examine the resilience of stable pairings when faced with any kind of disruption, even minor ones. These disruptions can occur in both individual and collective settings. Over time, the preferences of participants may change, the makeup of the market may shift, or existing partnerships may dissolve. For example, in job markets, a personal event may cause a worker to prefer a different location; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, individual interactions cannot break certain partnerships. \n\nConsider a market split into two submarkets, New York and Los Angeles, where everyone in each city prefers someone from their own city over someone from the other city. If the New York submarket has one stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for their submarket, and therefore for the entire market, individual interactions cannot unpair any such Los Angeles pair. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within their group so that everyone prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable pairings in the entire market pair participants within the fragment in the same way. Specifically, Los Angeles residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable pairings cannot lead to certain stable pairings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can lead to any stable pairing through individual interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The lack of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the individual process is flexible enough to achieve any stable pairing. \n\nTheorem 1 suggests one type of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse’s outcome does not necessarily have to revert back to the intended extreme stable pairing; individual interactions can instead lead to the other extreme stable pairing, or anything in between."
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the cosmic dance of stable matchings, we explore the resilience of these pairings in the face of unpredictable, perhaps even infinitesimal, disturbances. These disturbances can emerge in both decentralized and centralized environments. As the universe unfolds, the preferences of the participants may evolve, the composition of the market may transform, or the matched participants may choose to dissolve their partnerships. \n\nConsider, for instance, the labor market, a celestial body in its own right. A sudden change in a family's circumstances may create new geographical preferences for workers, akin to a comet altering its trajectory. New positions may materialize like stars being born, and workers may become available or retire, much like stars fading into oblivion. An employer and an employee might mutually agree to terminate their employment relationship, much like two celestial bodies drifting apart. \n\nWhile these changes may also modify the set of stable matchings, they could all potentially lead to instability. In particular, some participants who should be paired under the current market conditions might find themselves mismatched or unmatched. In some scenarios, decentralized interactions cannot break certain partnerships. \n\nImagine a market divided into two submarkets, much like a binary star system, New York and Los Angeles. In this system, every inhabitant of Los Angeles prefers any fellow Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market, much like a galaxy, naturally has two stable matchings as well. \n\nIf all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the dynamics of stabilization cannot achieve another stable matching in the entire market. \n\nIn this cosmic dance, Angelenos can be matched in a stable way within their group, so that every insider prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. \n\nThe absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching. \n\nTheorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between."
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In our exploration, we delve into the resilience of stable pairings when faced with arbitrary, even minimal, disruptions. These disruptions can occur in both decentralized and centralized environments. Over time, the preferences of participants may shift, the composition of the market may alter, or matched participants may choose to dissolve their partnerships. For example, in the labor market, a family crisis may create new geographical preferences for workers; new positions may emerge and workers may either become available or retire; an employer and an employee might mutually agree to end their employment relationship. While these changes may also modify the set of stable pairings, they could all lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, decentralized interactions cannot break certain partnerships. Consider a two-sided market split into two submarkets, New York and Los Angeles, where every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Let's say that the New York submarket has a unique stable pairing and the Los Angeles submarket has two stable pairings. The entire market naturally has two stable pairings as well. If all Angelenos are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Angelenos. Consequently, the stabilization dynamics cannot achieve another stable pairing in the entire market. In this scenario, Angelenos can be paired in a stable manner within the group so that every insider prefers her stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable pairings in the entire market match participants inside the fragment in the same way. Specifically, Angelenos make up a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot yield certain stable pairings. Theorem 1 demonstrates that the reverse is also true. When there are no nontrivial fragments, any unstable pairing can yield any stable pairing through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. Theorem 1 suggests one type of fragility of stable pairings. A small disruption of a stable pairing may result in any stable pairing, close to or distant from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable pairing, the best stable pairing for one market side, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse’s outcome need not revert back to the intended extremal stable pairing; decentralized interactions can instead lead to the other extremal stable pairing, or anything in between."
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Let's consider the concept of stable matchings and their resilience to changes, which could be minor or significant. These changes can occur in both individual and collective scenarios. Over time, the preferences of the participants may change, the composition of the market may shift, or the matched participants might decide to part ways. For example, in job markets, a change in a worker's family situation might lead to new location preferences; new job roles may emerge and workers may either become available or retire; an employer and an employee might mutually agree to end their work relationship. While these changes might also affect the set of stable matchings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might end up being mismatched or not matched at all. In some cases, individual interactions cannot break certain partnerships. \n\nLet's imagine a market divided into two submarkets, New York and Los Angeles, where every person from Los Angeles prefers any other person from Los Angeles over any New Yorker, and vice versa. Let's say that the New York submarket has one unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all people from Los Angeles are paired according to one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, individual interactions cannot unpair any such pair of matched people from Los Angeles. As a result, the dynamics of stabilization cannot achieve another stable matching in the entire market. In this scenario, people from Los Angeles can be matched stably within the group so that every person prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable matchings in the entire market match participants within the fragment in the same way. Specifically, people from Los Angeles constitute a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot lead to certain stable matchings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through individual interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the individual process is flexible enough to achieve any stable matching. \n\nTheorem 1 suggests one type of vulnerability of stable matchings. A minor change in a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extreme stable matching, the best stable matching for one side of the market, doctors and students, respectively. Our findings imply that any change in such a clearinghouse’s outcome does not necessarily have to revert back to the intended extreme stable matching; individual interactions can instead lead to the other extreme stable matching, or anything in between."
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- The study focuses on the robustness of stable matchings in the face of arbitrary or minimal perturbations.\n- These perturbations can occur in both decentralized and centralized settings, and can be due to shifts in participants' preferences, changes in market composition, or termination of partnerships.\n- Such changes can lead to instability, with participants being mismatched or unmatched under current market conditions.\n- In some situations, decentralized interactions cannot break certain partnerships, leading to the concept of a 'fragment'.\n- A fragment is a group where members can be stably matched within the group, with each member preferring their stable partner to anyone outside the group.\n- A fragment is 'trivial' if all stable matchings in the entire market match participants inside the fragment in the same way.\n- If a market has a 'non-trivial' fragment, some unstable matchings cannot yield certain stable matchings.\n- The absence of non-trivial fragments is a mild condition, and simulations suggest they are rare in large random markets.\n- Theorem 1 suggests that stable matchings can be fragile, with a small perturbation potentially leading to any stable matching.\n- The Deferred Acceptance mechanism aims to implement an extremal stable matching, but any perturbation of the outcome may not revert back to the intended extremal stable matching."
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The study examines the stability of matchings in various settings, such as labor markets, and how they can be affected by changes like shifts in preferences, market composition, or termination of partnerships. The research introduces the concept of a \"fragment\", a group where members prefer to be matched within the group rather than outside. Fragments can be \"trivial\" or \"non-trivial\", with the latter affecting the ability to achieve certain stable matchings. The study suggests that in most markets, the decentralized process can achieve any stable matching due to the rarity of non-trivial fragments. The research also highlights that small perturbations of a stable matching can result in any stable matching, which can have implications for mechanisms like the Deferred Acceptance mechanism used in matching doctors to hospitals and students to schools."
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "We are looking at how stable matches can handle changes. Stable matches are when things or people are paired up in a way that works well for everyone. But sometimes, things can change. People might change their minds about what they want. Or, new people might join, or old ones might leave. For example, in a job market, a worker might want to move to a new place because of family reasons. New jobs might open up, or workers might retire. A worker and a boss might decide to stop working together. These changes can mess up the stable matches. Some people might end up with a match that doesn't fit the current situation, or they might not have a match at all.\n\nSometimes, it's hard to break up certain matches. Let's say we have a market split into two parts: New York and Los Angeles. Everyone in Los Angeles prefers to be matched with someone else in Los Angeles, and the same goes for New York. If all the people in Los Angeles are matched up in a way that works well for them, it's hard to break up those matches. This means that it's hard to find a new set of matches that works well for everyone. In this case, the people in Los Angeles are a group that can be matched up well within itself. We call this a \"fragment\". New York is also a fragment. A fragment is \"trivial\" if all the good matches in the whole market match people within the fragment in the same way. In our example, Los Angeles is a non-trivial fragment, and New York is a trivial fragment.\n\nIf a market has a non-trivial fragment, some bad matches can't be turned into good matches. But if there are no non-trivial fragments, any bad match can be turned into a good match. Non-trivial fragments are the only thing that can stop this from happening. But, it's not common to have non-trivial fragments. So, in most markets, it's possible to find a good match for everyone.\n\nThis shows that stable matches can be fragile. A small change can lead to a completely different set of good matches. There's a famous method for matching doctors to hospitals and students to schools. It tries to find the best match for one side (the doctors or the students). But our findings show that a small change can lead to a different set of matches. It might not go back to the best match for the doctors or students. Instead, it could lead to the best match for the other side (the hospitals or schools), or something in between."
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of mathematics, the paragraph could be represented as follows:\n\nLet M be a set of stable matchings in a market. We consider the robustness of M under arbitrary perturbations, denoted as P. P can occur in both decentralized (D) and centralized (C) settings. Over time, factors such as changes in participants' preferences (ΔP), market composition (ΔM), or termination of partnerships (T) can cause P. \n\nFor example, in a labor market, a family shock (FS) can cause ΔP, new positions (NP) can cause ΔM, and mutual agreement to terminate employment (TE) can cause T. While M can be altered by these changes, they can also lead to instability (I). In particular, some participants that should be matched under current market conditions might be mismatched (MM) or unmatched (UM). \n\nConsider a two-sided market divided into two submarkets, New York (NY) and Los Angeles (LA), where every participant in LA prefers any participant in LA to any participant in NY, and vice versa. If NY has a unique stable matching (USM) and LA has two stable matchings (TSM), the entire market has TSM as well. If all participants in LA are paired according to one of the TSM, D cannot unmatch any such pair. This results in a situation where the stabilization dynamics cannot attain another stable matching in the entire market. \n\nWe define a group where participants can be matched in a stable way inside the group, and every insider prefers her stable partner to anyone outside the group, as a fragment (F). In this example, both NY and LA form F. A F is trivial (TF) if all stable matchings in the entire market match participants inside the F in the same way. \n\nIf a market has a non-trivial fragment (NTF), some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no NTF, any unstable matching can yield any stable matching through D. NTF are the only restraints on the stabilization dynamics. \n\nThe absence of NTF is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the D process is fluid enough to attain any stable matching. Theorem 1 suggests one type of fragility of stable matchings. A small P of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. \n\nThe Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching (ESM), the best stable matching for one market side. Our results imply that any P of such a clearinghouse’s outcome need not revert back to the intended ESM; D can instead lead to the other ESM, or anything in between."
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to help you comprehend this complex topic. We are examining the resilience of stable matchings when faced with arbitrary or minimal disruptions. These disruptions can occur in both decentralized and centralized environments. Over time, changes can occur such as shifts in participants' preferences, alterations in market composition, or the dissolution of matched partnerships. \n\nFor example, in labor markets, a family crisis might create new geographical preferences for workers. New positions may emerge, and workers may become available or retire. An employer and an employee might mutually agree to end their employment relationship. While these changes may also affect the set of stable matchings, they could all lead to instability. Specifically, some participants who should be paired under current market conditions might be mismatched or unmatched. \n\nIn some cases, decentralized interactions cannot break certain partnerships. Consider a two-sided market divided into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has a unique stable matching and the Los Angeles submarket has two, the entire market naturally has two stable matchings as well. If all Los Angeles residents are paired according to one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Los Angeles residents. As a result, the stabilization dynamics cannot achieve another stable matching in the entire market. \n\nIn this scenario, Los Angeles residents can be matched stably within the group so that every insider prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching. \n\nTheorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between."
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12147.pdf",
      "paragraphs": [
        {
          "source": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]",
          "translations": [
            {
              "id": "source",
              "content": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]"
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "This passage discusses the movement of electrons within a crystal lattice made up of oppositely charged ions. As these electrons move, they create distortions in the lattice, known as phonons. These phonons then interact with the electrons, resulting in each electron being surrounded by a cloud of phonons that reduce its mobility. This electron-phonon combination is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich proposed a model to explain the dynamics of multi-polarons. In this model, electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom. Phonons, on the other hand, are described as non-relativistic bosonic quantum fields that can be created and destroyed over time.\n\nThe study of polaron models has primarily used functional integration, a technique first introduced by Feynman. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. It focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space.\n\nThe article also considers the interaction of electrons via repulsive Coulomb potentials and the treatment of polarons in external electric and magnetic fields. It works under optimal conditions on the electrostatic potential and the magnetic vector potential, allowing the definition of the Hamiltonian via semibounded quadratic forms.\n\nIn some studies, electrons are confined to open regions of Euclidean space for technical reasons, and sometimes both electrons and phonons are confined. This article, therefore, works under general hypotheses on the electron-phonon interaction, covering these situations as well as the original Fröhlich model.\n\nThe Feynman–Kac formulas established in this article, along with the inequalities, can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "Electrons moving in a crystal lattice create lattice distortions, or phonons, which affect the electrons' mobility. This composite object is known as a polaron, and when multiple electrons are involved, they are referred to as multi-polarons. H. Fröhlich introduced a Hamiltonian that governs the dynamics of multi-polarons, treating electrons as non-relativistic quantum mechanical particles and phonons as a non-relativistic bosonic quantum field. The investigation of polaron models often uses functional integration, a technique introduced by Feynman. This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. The study also considers the effects of repulsive Coulomb potentials and external electric and magnetic fields on polarons. The findings can be used for further studies of the semigroup and ground state eigenvectors in polaron models."
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"Electrons moving in a crystal lattice of oppositely charged ions create lattice distortions, or phonons, which affect the electrons through their polarization. This results in each electron being surrounded by a phonon cloud, reducing its mobility. This composite object is known as a polaron; when multiple electrons are involved, we refer to multi-polarons. H. Fröhlich introduced a Hamiltonian in [Frö54] to govern multi-polaron dynamics. In his model, electrons are non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field. Functional integration has been a key technique in polaron model investigation since Feynman's work [Fey55]. We will soon reference mathematical papers that use various Feynman–Kac formulas for vacuum expectation values of semigroup members generated by Fröhlich’s Hamiltonian. This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space. We will work under optimal conditions on the electrostatic potential and the magnetic vector potential, allowing us to define the Hamiltonian through semibounded quadratic forms. In some studies, electrons are confined to open regions of Euclidean space [AL13, FLST11], and sometimes both electrons and phonons are confined [FS21, BM23]. Therefore, we will work under general assumptions on the electron-phonon interaction, covering these situations and the original Fröhlich model. Our Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models, similar to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22].\""
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The author is discussing the movement of electrons within a crystal lattice, which is made up of ions with opposite charges. When these electrons move, they cause distortions in the lattice, known as phonons. These phonons then interact with the electrons, causing each electron to be surrounded by a cloud of phonons, which reduces its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich proposed a model in 1954 to explain the dynamics of multi-polarons. In this model, the electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom, while the phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the introduction of Fröhlich's model, functional integration has been a key technique in the study of polaron models, both in theoretical physics and mathematics. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory, and focuses on deriving Feynman–Kac formulas in Fröhlich's multipolaron model.\n\nThe author also discusses the interaction of electrons via repulsive Coulomb potentials and the treatment of polarons in external electric and magnetic fields. The author notes that in some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, the author works under general assumptions about the interaction between electrons and phonons.\n\nThe author concludes by stating that the Feynman–Kac formulas established in this article can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "When electrons move within a crystal structure made up of ions with opposite charges, they cause distortions in the structure around them. These distortions, known as phonons, in turn affect the electrons due to the polarization they carry. This results in each electron being surrounded by a cloud of phonons, which reduces its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, we refer to it as multi-polarons.\n\nH. Fröhlich introduced a mathematical model in 1954 to explain the behavior of multi-polarons. In his model, electrons are considered as non-relativistic quantum mechanical particles without spin, while the phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the introduction of Fröhlich's model, one of the main methods used to study polaron models has been functional integration. This method has been widely used in both theoretical physics and mathematics. We will provide numerous references to mathematical papers that have used various Feynman–Kac formulas to study the behavior of multi-polarons.\n\nIn this article, we will focus on deriving Feynman–Kac formulas for Fröhlich's multi-polaron model. We will consider the effects of repulsive Coulomb potentials and the influence of external electric and magnetic fields on polarons. We will also consider situations where the electrons and phonons are confined to specific regions of space.\n\nOur findings, along with the formulas we derive, can serve as a foundation for further studies of the behavior of polarons. This could lead to a better understanding of the behavior of electrons in various environments, similar to the theory of magnetic Schrödinger semigroups and its extensions to related models in non-relativistic quantum electrodynamics and nucleonmeson interactions."
            },
            {
              "id": "06-simplify-college.ts",
              "content": "In a crystal lattice composed of ions with opposing charges, the movement of electrons generates lattice distortions, or phonons, in their immediate vicinity. These phonons, in turn, influence the electrons through the polarization they carry, resulting in each electron being surrounded by a phonon cloud that reduces its mobility. This electron-phonon combination is referred to as a polaron, and when multiple electrons are involved, the term multi-polarons is used. \n\nH. Fröhlich proposed a Hamiltonian in [Frö54] to govern the dynamics of multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field. \n\nFunctional integration has been a primary method in the study of polaron models since Feynman's pioneering work [Fey55], both in theoretical physics and mathematics. In Section 1.3, we will provide numerous references to mathematical papers that utilize various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian. \n\nThis article is dedicated to deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space, building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a]. \n\nGiven that electrons interact through repulsive Coulomb potentials and polarons often interact with external electric and magnetic fields, we will work under nearly optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, while still allowing the Hamiltonian to be defined via semibounded quadratic forms. \n\nIn some studies, electrons are confined to open regions of Euclidean space [AL13, FLST11] for technical reasons, and sometimes both electrons and phonons are confined [FS21, BM23]. Therefore, we will operate under general assumptions about the electron-phonon interaction that cover these two scenarios as well as the original Fröhlich model. \n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further exploration of the semigroup and ground state eigenvectors (if any) in polaron models, similar to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]."
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "When tiny particles called electrons move within a structure made up of differently charged particles, they cause changes in the structure around them. These changes can affect the electrons themselves. This can cause each electron to be surrounded by a group of these changes, which can slow down the electron's movement. This combination of an electron and the changes around it is called a polaron. When we talk about multiple electrons, we call them multi-polarons.\n\nH. Fröhlich, in his 1954 work, introduced a concept that explains how multi-polarons behave. In his explanation, he treats the electrons as particles that follow the rules of quantum mechanics, a branch of physics that deals with the smallest particles in the universe. He also talks about the changes in the structure, which can appear and disappear over time, and describes them using another branch of quantum mechanics.\n\nSince the work of Feynman in 1955, one main way to study polaron models has been through a method called functional integration. This method has been used in both theoretical physics and mathematics. In this article, we will refer to many mathematical papers that use various formulas related to functional integration to study the behavior of multi-polarons.\n\nIn this article, we will focus on deriving these formulas for multi-polarons. We will consider the fact that electrons interact with each other through a force that pushes them apart, and that polarons can be affected by external electric and magnetic fields. We will also consider situations where the electrons and the changes in the structure are limited to certain areas.\n\nWith the information and formulas we establish in this article, we can further study the behavior of multi-polarons. This can be compared to the study of other models in quantum mechanics, such as the Schrödinger model, the Pauli-Fierz model, and Nelson's model."
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "When electrons move in a crystal made of ions with different charges, they cause changes in the crystal's structure. These changes affect the electrons, making them move slower. This combination of an electron and the changes it causes is called a polaron. When there are many electrons involved, we call it multi-polarons.\n\nH. Fröhlich created a model to explain how multi-polarons work. In his model, electrons are treated as particles that follow the rules of quantum mechanics, but don't have spin. The changes in the crystal, which can be created and destroyed over time, are described by a quantum field.\n\nSince Fröhlich's model was introduced, one main way to study polarons has been through functional integration. This is a method used in both physics and math. We will give many examples of math papers that use this method to study Fröhlich's model.\n\nIn this article, we will use recent math studies to explain how to use functional integration in Fröhlich's multi-polaron model. We will consider how electrons interact with each other and how polarons behave in electric and magnetic fields.\n\nSometimes, for technical reasons, the electrons are limited to certain areas. Sometimes both the electrons and the changes in the crystal are limited. We will consider all these situations in our study.\n\nWith the information in this article, we can further study polarons. This can help us understand other models in quantum mechanics, like the model of magnetic Schrödinger semigroups, the Pauli–Fierz model of quantum electrodynamics, and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "Electrons moving within a crystal lattice, made up of ions with opposite charges, cause distortions in the lattice, known as phonons. These phonons, in turn, affect the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. This combined entity of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich introduced a mathematical model in 1954 to explain the behavior of multi-polarons. In this model, electrons are considered as non-relativistic quantum mechanical particles without spin, while phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nThe study of polaron models has primarily involved functional integration, a technique first used by Feynman in 1955. This method has been widely used in both theoretical physics and mathematics. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. We focus on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space.\n\nElectrons interact through repulsive Coulomb potentials and polarons are often exposed to external electric and magnetic fields. We will work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential. In some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. We will work under general assumptions on the electron-phonon interaction covering these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the intricate dance of electrons within a crystal lattice, a fascinating phenomenon unfolds. As these electrons traverse the lattice, composed of ions of opposing charges, they generate distortions in the lattice structure, known as phonons. These phonons, in turn, influence the electrons through the polarization they carry, resulting in each electron being enveloped by a cloud of phonons that hampers its mobility. This composite entity is referred to as a polaron, and when multiple electrons are involved, we delve into the realm of multi-polarons.\n\nIn the mid-20th century, H. Fröhlich proposed a mathematical model to describe the dynamics of these multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles devoid of spin degrees of freedom, while the phonons, capable of creation and annihilation over time, are represented by a non-relativistic bosonic quantum field.\n\nThe exploration of polaron models has been greatly influenced by the pioneering work of Richard Feynman, who introduced the concept of functional integration. This technique has been widely used in both theoretical physics and mathematics, and has been instrumental in the study of Fröhlich's Hamiltonian, the mathematical function that describes the total energy of the system.\n\nIn this article, we build upon recent mathematical studies to derive Feynman–Kac formulas for Fröhlich's multipolaron model. These formulas are crucial for understanding the behavior of the system under various conditions, including the influence of repulsive Coulomb potentials and the effects of external electric and magnetic fields on polarons.\n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions about the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further exploration of the semigroup and ground state eigenvectors in polaron models. This could potentially lead to new insights into the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In the realm of crystal lattices, electrons in motion create a ripple effect, distorting the lattice structure, or phonons, in their vicinity. This distortion, in turn, impacts the electrons through the polarization they carry, resulting in each electron being enveloped by a cloud of phonons that hampers its mobility. This electron-phonon composite is referred to as a polaron, and when multiple electrons are involved, we term it multi-polarons.\n\nH. Fröhlich, in his 1954 work, proposed a Hamiltonian model to govern the dynamics of these multi-polarons. In this model, electrons are depicted as non-relativistic quantum mechanical particles devoid of spin degrees of freedom, while the phonons, capable of creation and annihilation over time, are represented by a non-relativistic bosonic quantum field.\n\nThe investigation of polaron models has largely been driven by functional integration, a technique pioneered by Feynman in 1955. This approach has been widely used in both theoretical physics and mathematics. In the following section, we will delve into numerous mathematical papers that have utilized various Feynman–Kac formulas to calculate vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.\n\nThis article is dedicated to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. We will be working under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, which still allows us to define the Hamiltonian via semibounded quadratic forms.\n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will operate under general hypotheses on the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, coupled with the inequalities established in this article, can lay the groundwork for further exploration of the semigroup and ground state eigenvectors in polaron models. This is akin to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine electrons moving within a crystal lattice, a structure made up of oppositely charged ions. As they move, they cause distortions in the lattice, creating what we call phonons. These phonons, in turn, affect the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. We refer to this electron-phonon combination as a polaron. When we consider multiple electrons, we talk about multi-polarons.\n\nIn 1954, H. Fröhlich introduced a model to explain the dynamics of these multi-polarons. In his model, electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom. The phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the pioneering work of Feynman in 1955, functional integration has been a key technique in studying polaron models, both in theoretical physics and mathematics. We will soon provide numerous references to mathematical papers that use various Feynman–Kac formulas to calculate the vacuum expectation values of members of the semigroup generated by Fröhlich’s model.\n\nIn this article, we will focus on deriving Feynman–Kac formulas for Fröhlich’s multipolaron model. We will consider semigroup members applied to any vectors in the underlying Hilbert space. We will work under optimal conditions for the electrostatic potential and the magnetic vector potential, which are important as electrons interact via repulsive Coulomb potentials and polarons are often exposed to external electric and magnetic fields.\n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions about the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nWith the inequalities established in this article, our Feynman–Kac formulas can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Electrons moving in a crystal lattice create lattice distortions (phonons) which affect the electrons' mobility.\n- These composite objects are called polarons; multiple electrons create multi-polarons.\n- H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons.\n- In Fröhlich's model, electrons are non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons are described by a non-relativistic bosonic quantum field.\n- Functional integration has been a main technique in investigating polaron models, as initiated by Feynman.\n- This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space.\n- The study considers optimal conditions on the electrostatic potential and the magnetic vector potential.\n- Some studies confine electrons to open regions of Euclidean space, and sometimes both electrons and phonons are confined.\n- This study works under general hypotheses on the electron-phonon interaction, covering various situations including the original Fröhlich model.\n- The Feynman–Kac formulas established in this article can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models."
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The passage discusses the dynamics of multi-polarons, composite objects formed when electrons moving in a crystal lattice create lattice distortions (phonons) that lower the electrons' mobility. H. Fröhlich introduced a Hamiltonian model to govern these dynamics, treating electrons as non-relativistic quantum mechanical particles and phonons as a non-relativistic bosonic quantum field. The article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. The authors work under general hypotheses on the electron-phonon interaction, covering various situations including the original Fröhlich model. The derived Feynman–Kac formulas, along with established inequalities, can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models."
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "Electrons move around in a crystal made of ions with different charges. This movement changes the shape of the crystal. These changes also affect the electrons because they carry a charge. This makes the electrons move slower. When an electron and the changes it causes are together, we call it a polaron. If there are many electrons, we call them multi-polarons. \n\nH. Fröhlich made a rule to understand how multi-polarons move. In his rule, he thought of electrons as tiny particles that don't move very fast. He also thought of the changes in the crystal as something that can come and go over time. \n\nPeople have used a method called functional integration to understand polarons. This method was first used by Feynman. We will talk about many papers that used this method to understand Fröhlich's rule. \n\nWe will use this method to understand Fröhlich's rule for multi-polarons. We will use it for any kind of situation. Electrons push each other away and polarons can be affected by electric and magnetic fields. We will use the best conditions for the electric field and the magnetic field. \n\nSometimes, electrons are only allowed to move in certain areas. Sometimes, both the electrons and the changes in the crystal are only allowed in certain areas. We will use rules that cover all these situations and Fröhlich's original rule. \n\nWith the rules we make in this article, we can understand more about how polarons work. We can also understand more about the lowest energy state of polarons. This is similar to understanding how magnetic fields affect tiny particles and how light and matter interact at a very small scale."
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "Given the highly technical nature of the paragraph, it would be best represented in English with additional visual aids such as diagrams and mathematical formulas to help illustrate the concepts being discussed. However, as text-based AI, I am unable to provide visual aids. Here is a simplified English version of the paragraph:\n\n\"Electrons moving in a crystal lattice made of ions with opposite charges cause distortions in the lattice, known as phonons. These phonons affect the electrons due to the polarization they carry, resulting in each electron being surrounded by a cloud of phonons that reduce its mobility. This combination of an electron and its phonon cloud is called a polaron. When multiple electrons are involved, we refer to it as multi-polarons.\n\nH. Fröhlich proposed a mathematical model in 1954 to describe the behavior of multi-polarons. In this model, electrons are considered as non-relativistic quantum particles without spin, while phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince Feynman's pioneering work in 1955, functional integration has been a key method in studying polaron models in both theoretical physics and mathematics. We will provide more references to mathematical papers that use various Feynman–Kac formulas to calculate the expected values of members of the semigroup generated by Fröhlich’s model.\n\nIn this article, we will use recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory to derive Feynman–Kac formulas for Fröhlich’s multipolaron model. We will consider the effects of repulsive Coulomb potentials between electrons and the influence of external electric and magnetic fields on polarons.\n\nIn some studies, the electrons are confined to open regions of Euclidean space for technical reasons, and sometimes both the electrons and the phonons are confined. We will consider these scenarios as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can be used as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.\""
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to assist you in comprehending this complex subject matter. \n\nElectrons, when moving within a crystal lattice made up of ions with opposing charges, generate lattice distortions, or phonons, in their vicinity. These phonons, through their polarization, influence the electrons, resulting in each electron being surrounded by a phonon cloud that reduces its mobility. This combined entity is known as a polaron. When multiple electrons are involved, we refer to them as multi-polarons. \n\nH. Fröhlich proposed a Hamiltonian in his work [Frö54] to govern the dynamics of multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field. \n\nFunctional integration has been a primary method in the study of polaron models since Feynman's pioneering work [Fey55], both in theoretical physics and mathematics. We will soon provide numerous references to mathematical papers that utilize various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian. \n\nThis article is dedicated to deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. We will work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, allowing us to define the Hamiltonian via semibounded quadratic forms. \n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general hypotheses on the electron-phonon interaction covering these situations as well as the original Fröhlich model. \n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models, similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions."
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12869.pdf",
      "paragraphs": [
        {
          "source": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation.",
          "translations": [
            {
              "id": "source",
              "content": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation."
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library and gathered over a thousand Vampire proving strategies. Each of these strategies was optimized in advance to perform well on a single problem. With these strategies, we could easily build a comprehensive schedule that covers most of the solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a memory-based competition, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the discovered strategies, we suggested a greedy schedule construction method that can compete with optimal approaches. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule can solve, which takes more than 16 hours to generate. For shorter time budgets, creating an optimal schedule is no longer viable, but the greedy construction still produces relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation."
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, using over a thousand Vampire proving strategies from the FOF fragment of the TPTP library. The study suggests that the CASC competition should use a substantial set of previously unseen problems each year to avoid being a competition in memorization. A proposed greedy schedule construction procedure can compete with optimal approaches, taking less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate. The strength of the greedy scheduler can be reinforced by regularization mechanisms, which are the main contribution of this work. These mechanisms allow us to outperform the optimal schedule on unseen problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation."
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "In this study, we independently evaluated the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each optimized for a single problem. These strategies allowed us to build a comprehensive schedule covering most solvable problems within the CASC competition's budget. This implies that CASC should incorporate a significant set of new problems each year to avoid becoming a memorization contest. \n\nWe suggested a greedy schedule construction method to create robust schedules using the discovered strategies, which can rival optimal methods. With a time budget of about 2 minutes, the greedy algorithm takes less than a minute to generate a schedule that solves nearly as many problems as an optimal schedule, which requires over 16 hours to create. For shorter time budgets, optimal schedule construction becomes unfeasible, while the greedy method continues to produce strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be enhanced by various regularization mechanisms, which are the main contribution of this study. A well-selected regularization allows us to surpass the optimal schedule on new problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation."
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "In this study, we independently assessed a strategy known as Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, gathering over a thousand problem-solving strategies, each one specifically designed to excel at a single problem. With these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems solvable within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust plans using the strategies we discovered, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the best plan is no longer possible, while our quick method still produces relatively strong plans. \n\nThe surprising effectiveness of our quick planning method can be further enhanced by various adjustment mechanisms, which are the main focus of this study. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the speed and simplicity of our quick planning method and the adjustment techniques make them appealing for reuse and further exploration."
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "In this project, we independently assessed a strategy called Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, known as the FOF fragment. We gathered over a thousand problem-solving strategies from a tool called Vampire, each one specifically designed to excel at a single problem. \n\nWith these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo create robust plans using the strategies we discovered, we suggested a method called greedy schedule construction. This method can compete with the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan can, which takes over 16 hours to create. For shorter time limits, creating the best plan is no longer possible, while the greedy method still produces relatively strong plans.\n\nThe surprising effectiveness of the greedy method can be further enhanced by various adjustment mechanisms, which are the main focus of this project. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy method and the adjustment techniques make them appealing for reuse and further exploration."
            },
            {
              "id": "06-simplify-college.ts",
              "content": "In this study, we independently assessed the Spider-style [33] approach to strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, constructing a comprehensive schedule that addresses most solvable problems within the CASC competition's budget is straightforward. This implies that to prevent CASC from becoming primarily a memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can rival optimal approaches. Given a time budget of roughly two minutes, the greedy algorithm requires less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time budgets, constructing an optimal schedule becomes impractical, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected efficacy of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization enables us to surpass the optimal schedule in solving unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration."
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "In this study, we tested a method called Spider-style strategy discovery and schedule creation. We focused on a specific part of the TPTP library, gathering over a thousand strategies from a system called Vampire. Each of these strategies was designed to work well on one specific problem. With these strategies, we can easily create a single, comprehensive plan that can solve most of the problems that can be solved within the time limit set by the CASC competition. This suggests that to keep the CASC competition from being just a memory test, it's important to use a lot of new problems each year. \n\nTo create strong plans using the strategies we found, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves almost as many problems as the best plan, which takes over 16 hours to create. For shorter time limits, creating the best plan isn't possible, but our quick method still creates relatively strong plans. \n\nThis surprising effectiveness of our quick method can be improved even more by using various adjustment mechanisms, which is the main focus of this study. Choosing the right adjustment allows us to do better than the best plan on new problems. Lastly, the quickness and simplicity of our quick plan creation method and the adjustment techniques make them good options for reuse and further testing."
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "In this project, we tested a strategy called Spider-style [33] to solve problems and make plans. We used a part of the TPTP library called the FOF fragment and gathered over a thousand different ways to solve problems using a tool called Vampire. Each of these ways was designed to work best on one specific problem. With these strategies, we can make a big plan that can solve most of the problems that we know can be solved within the time limit set by the CASC competition. This shows that for the CASC competition to be fair, it's important to use new problems each year that haven't been seen before. \n\nTo make strong plans using the strategies we found, we suggested a method that picks the best option available at each step, which we call a greedy schedule construction procedure. This method can compete with the best possible approaches. If we have about 2 minutes, this method takes less than a minute to make a plan that solves almost as many problems as the best possible plan, which takes over 16 hours to make. If we have less time, making the best possible plan isn't doable, but our method can still make pretty good plans. \n\nThe surprising power of our method can be made even stronger by using different techniques, which is the main thing we're showing in this project. If we pick the right technique, we can do better than the best possible plan on problems we haven't seen before. Lastly, our method is fast and simple, which makes it a good choice for use in other projects and for more testing."
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, we can easily create a comprehensive schedule that can solve most of the problems within the time limit set by the CASC competition. This implies that to prevent CASC from becoming a memory contest, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can compete with the best approaches. Given a time limit of about 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time limits, creating an optimal schedule becomes impractical, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main focus of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the quick runtime and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration."
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In this cosmic endeavor, we embarked on an autonomous exploration of the Spider-style [33] strategy discovery and schedule creation, akin to a solitary spacecraft traversing the vast expanse of the universe. Our focus was the FOF fragment of the TPTP library, a celestial body of knowledge, from which we harvested over a thousand Vampire proving strategies. Each of these strategies, like a star in the cosmos, was uniquely optimized to shine brightly on a single problem.\n\nWith these strategies in our cosmic toolkit, we found it straightforward to assemble a single, monolithic schedule, much like a constellation that encompasses most of the problems known to be solvable within the budgetary constraints of the CASC competition. This observation led us to the realization that the CASC competition, to avoid becoming a mere contest of memory, must incorporate a substantial set of fresh, uncharted problems each year.\n\nTo construct robust schedules using the unearthed strategies, we proposed a greedy schedule construction procedure, akin to a black hole, consuming all in its path, yet capable of competing with the most optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm, with the speed of a comet, takes less than a minute to produce a schedule that solves over 99.0% of the problems that an optimal schedule, which requires more than 16 hours to generate, can solve. As the time budget shrinks, optimal schedule construction becomes as unfeasible as traveling faster than light, while the greedy construction continues to produce relatively strong schedules.\n\nThe surprising potency of the greedy scheduler, much like the unexpected power of a supernova, can be further amplified by various regularization mechanisms. These mechanisms, like the gravitational forces that hold our universe together, form the core contribution of this work. An astutely selected regularization, like a well-placed telescope, allows us to outperform the optimal schedule on unseen problems.\n\nIn conclusion, the runtime speed and simplicity of the greedy schedule construction algorithm, along with the regularization techniques, are as attractive for reuse and further experimentation as the mysteries of the cosmos are to an astronomer."
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In our exploration, we embarked on an autonomous assessment of the Spider-style [33] approach to strategy discovery and schedule creation. Our focus was the FOF fragment of the TPTP library, from which we gathered over a thousand Vampire proving strategies, each one meticulously optimized to excel on a singular problem. With these strategies in hand, the construction of a single, all-encompassing schedule that covers the majority of solvable problems within the CASC competition's budget becomes a straightforward task. This implies that to prevent CASC from becoming a mere memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the unearthed strategies, we put forth a greedy schedule construction method, capable of holding its own against optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm takes less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which requires over 16 hours to create, can solve. As the time budget shrinks, the feasibility of constructing an optimal schedule diminishes, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected potency of the greedy scheduler can be further bolstered by various regularization mechanisms, which form the crux of our contribution in this study. A carefully selected regularization enables us to surpass the optimal schedule's performance on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm, coupled with the regularization techniques, make them appealing for reuse and further exploration."
            },
            {
              "id": "12-role-teacher.ts",
              "content": "In this study, we independently assessed a method known as the Spider-style strategy for problem-solving and planning. We focused on a specific section of the TPTP library, known as the FOF fragment. From this, we gathered over a thousand strategies from a system called Vampire, each one fine-tuned to excel at solving a single problem. \n\nWith these strategies, it's quite straightforward to build a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to prevent the CASC from becoming a mere memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo build robust plans using the strategies we discovered, we suggested a method called the greedy schedule construction procedure. This method can hold its own against the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that an ideal plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the ideal plan is no longer practical, while the greedy method continues to produce relatively strong plans.\n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various balancing mechanisms, which are the main focus of this study. A carefully selected balancing mechanism allows us to outperform the ideal plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy plan creation method and the balancing techniques make them appealing for reuse and further exploration."
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Conducted an independent evaluation of Spider-style strategy discovery and schedule creation.\n- Collected over a thousand Vampire proving strategies, each optimized for a single problem.\n- Constructed a single schedule covering most solvable problems within the CASC competition budget.\n- Emphasized the need for CASC to use a substantial set of unseen problems each year to avoid being a memorization competition.\n- Proposed a greedy schedule construction procedure that can compete with optimal approaches.\n- The greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate.\n- For shorter time budgets, optimal schedule construction is not feasible, but greedy construction still produces strong schedules.\n- The strength of the greedy scheduler can be reinforced by various regularization mechanisms, which is the main contribution of this work.\n- An appropriately chosen regularization can outperform the optimal schedule on unseen problems.\n- The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation."
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. Over a thousand Vampire proving strategies were collected, each optimized for a single problem. These strategies can be used to construct a single schedule that covers most solvable problems within the CASC competition's budget. The study suggests that CASC should use a substantial set of new problems each year to avoid becoming a memorization competition. A greedy schedule construction procedure was proposed, which can compete with optimal approaches. The greedy algorithm takes less than a minute to produce a schedule that solves over 99% of problems, compared to an optimal schedule that takes over 16 hours to generate. The strength of the greedy scheduler can be enhanced by regularization mechanisms, which is the main contribution of this work. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation."
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "In this study, we tested a method called Spider-style to find and make plans. We used a part of the TPTP library called FOF and gathered over a thousand strategies from a tool called Vampire. Each strategy was made to work well on one problem. With these strategies, we can make a big plan that can solve most of the problems that can be solved within the time limit of the CASC competition. This shows that the CASC competition should use new problems each year, so it's not just about remembering old solutions. \n\nTo make good plans with the strategies we found, we suggested a method that quickly makes a plan. This method can compete with the best methods. If we have about 2 minutes, this quick method takes less than a minute to make a plan that solves almost as many problems as the best plan, which takes more than 16 hours to make. If we have less time, making the best plan is not possible, but our quick method still makes good plans. \n\nThe strength of our quick method can be made even better with some adjustments. These adjustments are the main part of our study. If we choose the right adjustment, we can do better than the best plan on new problems. Lastly, our quick method and the adjustments are easy to use and fast, which makes them good for using again and trying new things."
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In this study, we independently assessed the Spider-style [33] method of strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each one a priori optimized for a single problem. With these strategies, it's simple to build a single, comprehensive schedule that can solve most of the problems solvable within the CASC competition's budget. This implies that to prevent CASC from becoming primarily a memory contest, it's crucial to introduce a significant number of new problems each year. To build robust schedules using the discovered strategies, we suggested a greedy schedule construction process that can compete with optimal methods. Given a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to generate. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. The surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation."
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to assist you in comprehending this complex subject matter. \n\nIn our study, we independently assessed the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each individually optimized for a single problem. With these strategies, we can easily create a comprehensive schedule that addresses most solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a mere memory contest, it's crucial to introduce a significant set of new problems each year. \n\nWe suggested a greedy schedule construction method to build robust schedules using the discovered strategies, which can rival optimal methods. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to create. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization allows us to surpass the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation."
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12768.pdf",
      "paragraphs": [
        {
          "source": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
          "translations": [
            {
              "id": "source",
              "content": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language."
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "To promote creative thinking and personalized learning, teachers can create different scripts and use various prompts. This allows them to tailor the learning content to meet specific educational goals. When a teacher selects an item from the Stickers panel, the prompt linked to that sticker appears in the Refine panel. Teachers can then modify this prompt and create new stickers that better suit the learning context, while also reducing the chance of creating inappropriate content. \n\nFurthermore, by using a range of themes and contexts, teachers can make the learning content more engaging. This not only strengthens vocabulary learning but also encourages students to adapt their language use, as demonstrated in Figure 4. By providing a wide variety of learning materials, teachers can encourage more active vocabulary learning and strengthen students' connection with the language."
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "Educators can encourage divergent thinking by generating variant scripts and using alternative prompts to customize learning content. They can select and edit prompts in the Stickers panel to better fit the context and avoid inappropriate content. By using varied themes, educators can create engaging content that reinforces vocabulary and promotes linguistic adaptability. This diverse array of learning materials can stimulate dynamic vocabulary engagement and deepen learners' connections with the language."
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"To promote divergent thinking and customization, educators can create different scripts and use alternative prompts, broadening the potential to tailor content to specific learning objectives. When an item is selected in the Stickers panel, the associated prompt is displayed in the Refine panel. Educators can modify the prompt and create new stickers to suit the context, reducing the risk of inappropriate content. Additionally, using various themes and contexts, educators can make learning content more engaging, reinforcing vocabulary and promoting linguistic adaptability, as shown in Fig. 4. Providing a range of learning materials can encourage dynamic vocabulary engagement and strengthen learners’ language connections.\""
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "To make learning more creative and personalized, teachers can create different scripts and use various prompts. This allows them to adjust the content to match specific learning goals. When a teacher selects an item in the Stickers panel, the current prompt linked to that sticker shows up in the Refine panel. Teachers can change this prompt and create new stickers that better suit the context they're teaching in. This also helps avoid creating content that might be inappropriate. \n\nFurthermore, by using a range of themes and contexts, teachers can make learning more interesting. This not only helps students learn new words but also helps them adapt to different language uses, as shown in Fig. 4. By providing a wide variety of learning materials, teachers can encourage students to engage with vocabulary in more dynamic ways and strengthen their relationship with the language."
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "To boost creative thinking and allow for personalization, teachers can create different scripts and use various prompts. This widens the scope for adjusting the content to match specific learning goals. When a sticker is chosen from the Stickers panel, the prompt linked to that sticker shows up in the Refine panel. Teachers can modify this prompt and create new stickers that better suit the intended context, while also reducing the chance of creating potentially unsuitable content. Furthermore, by using a range of themes and contexts, teachers can make learning more interesting. This not only strengthens vocabulary learning but also encourages flexibility in language use, as shown in Fig. 4. By providing a wide variety of learning materials, teachers can encourage more active engagement with vocabulary and strengthen students' relationship with the language."
            },
            {
              "id": "06-simplify-college.ts",
              "content": "To promote innovative thinking and enable personalization, teachers can create diverse scripts and employ different prompts, thereby broadening the scope for customizing the produced content to align with specific educational goals. When a sticker is chosen from the Stickers panel, the prompt linked to that sticker appears in the Refine panel. Teachers have the ability to modify this prompt and create new stickers that are more suitable for the intended scenario, while also reducing the likelihood of producing potentially unsuitable content. Furthermore, by utilizing a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages linguistic flexibility, as demonstrated in Figure 4. By providing a wide variety of learning resources, teachers can encourage more interactive forms of vocabulary engagement and strengthen students' relationship with the language."
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "To help students think creatively and personalize their learning, teachers can create different lesson plans and use a variety of questions. This allows them to adjust the lessons to meet specific learning goals. When a teacher chooses an item from the Stickers panel, the current question linked to that sticker shows up in the Refine panel. Teachers can change this question and create new stickers to better suit the lesson, while also avoiding any potentially unsuitable content. Furthermore, by using different themes and situations, teachers can make learning more interesting. This not only helps students learn new words but also helps them adapt to different language uses, as shown in Fig. 4. By providing a wide range of learning materials, teachers can encourage students to engage more actively with vocabulary and strengthen their relationship with the language."
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "To help students think creatively and make learning more personal, teachers can create different lesson plans and use different questions. This allows them to make the lessons fit the students' learning goals better. When a teacher picks an item from the Stickers panel, the question that goes with that sticker shows up in the Refine panel. Teachers can change this question and make new stickers that fit the lesson better. This also helps avoid any content that might not be suitable. \n\nAlso, by using different themes and topics, teachers can make learning more fun. This not only helps students learn new words but also helps them use the language in different ways, as shown in Fig. 4. By providing a variety of learning materials, teachers can make vocabulary learning more active and help students feel more connected to the language."
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "To promote creative thinking and allow for personalization, teachers can create different scripts and use various prompts, broadening the scope for customizing the material to meet specific educational goals. When a sticker is chosen from the Stickers panel, the current prompt linked to that sticker appears in the Refine panel. Teachers can modify this prompt and create new stickers to better suit the intended setting, while also reducing the chance of creating potentially unsuitable content. Furthermore, by using a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages language adaptability, as demonstrated in Fig. 4. By providing a wide variety of learning resources, teachers can encourage more active vocabulary engagement and strengthen students' relationship with the language."
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the grand cosmos of education, the nurturing of divergent thought and the cultivation of customization can be achieved by educators through the creation of variant scripts and the employment of alternative prompts. This expands the universe of possibilities, allowing the tailoring of the output content to orbit around specific learning objectives. \n\nWhen an item is chosen from the celestial body known as the Stickers panel, the current prompt associated with the selected sticker is illuminated in the Refine panel, much like a star in the night sky. Educators, acting as the architects of this educational cosmos, can modify the prompt and give birth to new stickers that better align with the intended context. This process also serves as a protective shield, mitigating the risk of generating content that may stray into the realm of inappropriateness. \n\nMoreover, by harnessing the power of varied themes and contexts, educators can construct more captivating learning content. This not only strengthens the gravitational pull of vocabulary acquisition but also fosters linguistic adaptability, as depicted in our cosmic map, Fig. 4. \n\nBy providing a diverse galaxy of learning materials, educators can ignite more dynamic forms of vocabulary engagement, deepening learners' bonds with the language, much like the gravitational pull between celestial bodies. In this vast universe of education, every word is a star waiting to be discovered, every sentence a constellation waiting to be explored."
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In the spirit of Saul Kahn, let's consider this: To stimulate the birth of unique ideas and nurture personalization, teachers have the power to create a multitude of different scripts and employ a variety of prompts. This broadens the horizon for tailoring the end product to meet specific educational goals. When a teacher chooses an item from the Stickers panel, the prompt linked to that sticker appears in the Refine panel. Teachers have the freedom to modify this prompt and create new stickers that better suit the context they're aiming for, all while reducing the chance of producing content that may be unsuitable. \n\nFurthermore, by harnessing a range of themes and contexts, teachers can craft learning materials that are not only more captivating but also reinforce vocabulary learning and encourage linguistic flexibility, as illustrated in Fig. 4. By providing a rich variety of learning resources, teachers can spark more vibrant forms of vocabulary interaction and deepen students' bond with the language."
            },
            {
              "id": "12-role-teacher.ts",
              "content": "As a distinguished educator, I would like to share some strategies to inspire creative thinking and promote personalization in learning. We, as teachers, can create a variety of lesson plans and use different questions or prompts, thereby broadening the scope for customizing the learning content to meet specific educational goals. \n\nLet's take an example. When you choose an item from the Stickers panel, the current question or prompt linked to that sticker appears in the Refine panel. As teachers, we have the ability to modify this prompt and create new stickers that are more suitable for the context we're teaching in. This not only helps us to ensure the content is relevant and appropriate, but also reduces the chance of generating any content that may be unsuitable.\n\nFurthermore, by using a range of themes and contexts, we can make the learning content more interesting and engaging. This not only strengthens vocabulary learning but also encourages students to adapt their language use to different situations, as illustrated in Figure 4. \n\nBy providing a wide variety of learning resources, we can encourage more active engagement with vocabulary and strengthen students' relationship with the language."
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Educators can encourage divergent thinking and customization by generating variant scripts and using alternative prompts.\n- This expands possibilities for tailoring output content to specific learning objectives.\n- The current prompt associated with a selected item in the Stickers panel is displayed in the Refine panel.\n- Educators can edit the prompt and regenerate stickers to better fit the intended context and avoid generating inappropriate content.\n- Leveraging varied themes and contexts can create more engaging learning contents.\n- These contents not only reinforce vocabulary acquisition but also promote linguistic adaptability.\n- Offering a diverse array of learning materials can stimulate dynamic vocabulary engagement and deepen learners' connections with the language."
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "Educators can encourage divergent thinking and customization by generating different scripts and using alternative prompts to tailor content to specific learning objectives. They can select an item in the Stickers panel, view the current prompt in the Refine panel, and edit it to better fit the context, reducing the risk of inappropriate content. By using varied themes and contexts, educators can create engaging learning content that reinforces vocabulary acquisition and promotes linguistic adaptability. This diverse range of learning materials can stimulate dynamic vocabulary engagement and deepen learners' connection with the language."
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "\"Teachers can make learning more fun and personal by using different ways to teach and ask questions. This can help students learn better. When a teacher picks a sticker from the Stickers panel, they see a question related to that sticker in the Refine panel. Teachers can change this question to make it fit better with what they are teaching. This also helps avoid any content that may not be suitable. Teachers can also use different topics and situations to make learning more interesting. This can help students learn new words and how to use them in different ways, as shown in Fig. 4. By using a variety of teaching materials, teachers can make learning words more exciting and help students feel more connected to the language.\""
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In a visual language like a flowchart:\n\n1. Start: Encourage divergent thinking and foster customization\n2. Action: Educators generate variant scripts and utilize alternative prompts\n3. Result: Expanding possibilities for tailoring output content to specific learning objectives\n4. Action: Select an item in the Stickers panel\n5. Result: The current prompt associated with the chosen sticker is displayed in the Refine panel\n6. Action: Educators edit the prompt and regenerate stickers\n7. Result: Better fit for the intended context and reduced risk of generating potentially inappropriate content\n8. Action: Leverage varied themes and contexts\n9. Result: Creation of more engaging learning contents that reinforce vocabulary acquisition and promote linguistic adaptability (See Fig. 4)\n10. Action: Offer a diverse array of learning materials\n11. Result: Stimulation of more dynamic forms of vocabulary engagement and deepening learners’ connections with the language\n12. End\n\nTranslation to English: \n\nStart by encouraging divergent thinking and fostering customization. Educators can then generate variant scripts and utilize alternative prompts, which expands the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can then edit the prompt and regenerate stickers, which better fits the intended context and reduces the risk of generating potentially inappropriate content. By leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language."
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to guide you through this complex topic. To foster creative thinking and personalization, teachers can create diverse scripts and use different prompts, thereby increasing the potential to adapt the produced content to specific educational goals. When a sticker is chosen from the Stickers panel, the prompt linked to that sticker appears in the Refine panel. Teachers have the ability to modify this prompt and create new stickers that are more suitable for the intended context, while also reducing the chance of producing unsuitable content. \n\nFurthermore, by utilizing a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages linguistic flexibility, as demonstrated in Figure 4. By providing a wide variety of learning resources, teachers can encourage more interactive forms of vocabulary engagement and strengthen students' relationship with the language."
            }
          ]
        }
      ]
    }
  ]
}
