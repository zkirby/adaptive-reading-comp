{
  "papers": [
    {
      "id": "https://arxiv.org/pdf/2403.12122.pdf",
      "paragraphs": [
        {
          "source": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
          "translations": [
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine a spinning disk of primordial planetesimals, the building blocks of planets. The way this disk disperses influences the orbital evolution of the giant planets in our solar system. Let's take a look at Table 3, where we've noted some important factors in the final layouts of these giant planets after 4 billion years of evolution.\n\nIn the first column, we've noted the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48. However, in our simulated systems, we've seen a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this primarily impacts the inner solar system, not the Kuiper belt.\n\nNow, the structure of the Kuiper belt is influenced by the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97 to 2.31, with an average of 2.13.\n\nWe've also looked at a large number of simulations from previous studies, and found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain, or simply a matter of chance among our 12 simulated systems.\n\nHowever, it's important to note that if Uranus and Neptune's orbits cross at a 2:1 ratio, it can destabilize much of the Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a desirable feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 1
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. Key parameters in the final architectures of these planets after 4 Gyrs of evolution include the ratio of Saturn’s to Jupiter’s orbital period and the ratio of Neptune’s to Uranus’ orbital period. The former has dynamical consequences primarily for the inner solar system, while the latter affects the Kuiper belt’s architecture. In simulations, the Saturn to Jupiter period ratio ranged from 2.1–2.7, with a median of 2.39. The Neptune to Uranus period ratio in original systems ranged from 1.97– 2.31, with a median of 2.13. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize the resonant Kuiper belt population, limiting the utility of simulation results.",
              "rank": 2
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of the other giant planets too. In Table 3, we provide important parameters in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn’s orbital period to Jupiter’s. In the actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. Although the positions of various secular resonances depend on this exact ratio, this mainly affects the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune’s orbital period to Uranus’. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, the crossing of Uranus and Neptune’s 2:1 mean motion resonance (MMR) can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 3
            },
            {
              "id": "06-simplify-college.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of other giant planets. Table 3 outlines crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems display a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the exact ratio impacts the locations of various secular resonances, this primarily affects the inner solar system rather than the Kuiper belt. \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. The second column of Table 3 presents these final ratios for our systems. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon reviewing numerous simulations from previous studies, we found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's uncertain whether this is a systematic issue with our specific 5-planet resonant chain or simply an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 mean motion resonance (MMR) can destabilize a significant portion of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore an undesirable feature for simulations intended to study Kuiper belt formation.",
              "rank": 4
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital changes of other giant planets too. Table 3 outlines the main factors in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio affects the locations of various secular resonances, this primarily impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 5
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "The movement of an original planet disk can cause changes in the orbits of other giant planets. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of changes. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1–2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the ratio of Neptune’s to Uranus’ orbital period, which we've listed in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97– 2.31, with an average of 2.13. Looking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of under 2. It's unclear whether this is a problem with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations meant to study Kuiper belt formation.",
              "rank": 6
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The movement of an original planet-like disk influences the development of other giant planets' orbits. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of development. The first column shows the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1 to 2.7, with an average of 2.39. While this exact ratio affects the locations of various resonances, it mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. These original systems had ratios ranging from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we found that about half of all 4-planet instability outcomes ended with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. \n\nHowever, the crossing of Uranus and Neptune's 2:1 resonance can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 7
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. The study lists key parameters in the final architectures of these planets after 4 billion years of evolution. The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48, but the simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39. This ratio primarily affects the inner solar system rather than the Kuiper belt. However, the Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period. In the simulations, all but one system evolved above a period ratio of 2. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.",
              "rank": 8
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets.\n- Key parameters of the final architectures of the giant planets after 4 Gyrs of evolution are listed in Table 3.\n- The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48.\n- The simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39.\n- The exact ratio has dynamical consequences primarily for the inner solar system, not the Kuiper belt.\n- The Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period.\n- In the original simulations, all but one system evolved above a period ratio of 2, with values ranging from 1.97– 2.31 and a median of 2.13.\n- About half of all 4-planet instability outcomes from prior works finish with a Neptune-to-Uranus period ratio of under 2.\n- The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.\n- This crossing is a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 9
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "The scattering of an original planetesimal disk instigates orbital evolution among the remaining giant planets. Table 3 outlines the crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column of the table presents the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems exhibit a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the precise locations of various secular resonances hinge on this exact ratio, the dynamical implications are primarily for the inner solar system, not the Kuiper belt (see Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020). \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We present these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these original systems varied from 1.97 to 2.31, with a median of 2.13. Upon reviewing a significant number of approximately 1000-particle simulations from previous studies (Clement et al. 2018, 2021c,a), we discovered that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains uncertain whether this is an unobserved systematic issue with our specific 5-planet resonant chain or merely an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize a large portion of the resonant Kuiper belt population, thereby limiting the usefulness of our simulation results. Consequently, it is an undesirable feature for simulations intended to investigate Kuiper belt formation (Graham & Volk 2024).",
              "rank": 10
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"The scattering of an initial planetesimal disk influences the orbital evolution of other giant planets. Table 3 shows key parameters in the final structures of these planets after 4 Gyrs of evolution. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In the actual solar system, this ratio is about 2.48, but our simulated systems range from 2.1–2.7, with a median of 2.39. This ratio mainly affects the inner solar system's dynamics rather than the Kuiper belt. However, the Kuiper belt’s structure relies on the ratio of Neptune’s to Uranus’ orbital period, listed in Table 3's second column. In our initial simulations, all but one system (2500Pb) had a period ratio above 2. These original systems ranged from 1.97– 2.31, with a median of 2.13. Reviewing numerous ∼1000-particle simulations from previous studies, we find about half of all 4-planet instability results end with a Neptune-to-Uranus period ratio below 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, Uranus and Neptune’s 2:1 MMR crossing can destabilize much of the resonant Kuiper belt population, limiting our simulation results' usefulness. Therefore, it's not ideal for simulations studying Kuiper belt formation.\"",
              "rank": 11
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "When a planet breaks apart, it can change the paths of other big planets too. In Table 3, we show important details about how the big planets changed after 4 billion years. In the first column, we show how long it takes Saturn to orbit the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times longer, with an average of 2.39 times. This can affect the inner solar system more than the outer part, where the Kuiper belt is.\n\nHowever, the structure of the Kuiper belt depends on how long it takes Neptune to orbit the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times longer, with an average of 2.13 times. \n\nLooking at other computer models with about 1000 particles, we found that about half of all 4-planet systems ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our 5-planet model or just bad luck in our 12 models. \n\nBut when Neptune and Uranus cross each other's paths, it can mess up a lot of the Kuiper belt. This makes our models less useful for studying how the Kuiper belt was formed.",
              "rank": 12
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the cosmic ballet of the planets, the scattering of an ancient disk of planetesimals, the building blocks of planets, influences the orbital dance of the gas giants as well. In the cosmic ledger we call Table 3, we've noted the defining characteristics of the final arrangements of these gas giants after 4 billion years of cosmic evolution. \n\nFirstly, we've noted the rhythm of their dance - the ratio of Saturn's orbital period to Jupiter's. In our own cosmic neighborhood, this ratio is approximately 2.48. However, in our digital cosmos, the ratios vary, ranging from 2.1 to 2.7, with a median value of 2.39. While the exact ratio influences the locations of various gravitational harmonies, its impact is felt primarily in the inner solar system, rather than the distant Kuiper belt.\n\nHowever, the architectural blueprint of the Kuiper belt is influenced by the rhythm of Neptune's dance with Uranus, and we've noted these final ratios in the second column of our cosmic ledger. In our initial digital cosmos, all but one system evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon examining a multitude of simulations from previous works, we find that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains unclear whether this is a systematic issue with our specific 5-planet resonant chain or simply a stroke of cosmic misfortune among our 12 simulated systems. \n\nRegardless, the crossing of Uranus and Neptune's 2:1 gravitational harmony can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. This makes it a less than ideal feature for simulations intended to study the birth of the Kuiper belt.",
              "rank": 13
            },
            {
              "id": "source",
              "content": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
              "rank": 14
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "The movement of an early group of small planets can affect the paths of larger planets too. In Table 3, we've noted important details about the final positions of the large planets after 4 billion years. The first column shows the comparison between the time it takes Saturn and Jupiter to orbit the sun. In our actual solar system, Saturn takes about 2.48 times longer than Jupiter, but in our computer models, this time varied between 2.1 and 2.7 times, with an average of 2.39 times. This difference can affect the inner solar system more than the outer region known as the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the comparison between the time it takes Neptune and Uranus to orbit the sun. We've noted these comparisons for our models in the second column of Table 3. In our initial models, all but one showed Neptune taking more than twice as long as Uranus. The times varied between 1.97 and 2.31 times, with an average of 2.13 times. Looking at many previous models, we found that about half ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our specific model or just bad luck in our 12 models. However, if Neptune and Uranus cross paths at a 2:1 ratio, it can disrupt much of the Kuiper belt, which makes our model results less useful. This is not a good feature for models meant to study the formation of the Kuiper belt.",
              "rank": 15
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "A group of early planets can change the paths of other big planets. In Table 3, we show important details about the final paths of the big planets after 4 billion years. The first column shows how long it takes Saturn to go around the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times, with an average of 2.39 times. This difference mainly affects the inner solar system, not the outer area called the Kuiper belt.\n\nHowever, the Kuiper belt is affected by how long Neptune takes to go around the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times, with an average of 2.13 times. \n\nLooking at many other computer models, we found that about half of them had Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our model or just bad luck in our 12 systems. But if Neptune and Uranus get too close, it can mess up the Kuiper belt. This makes our model less useful for studying how the Kuiper belt was formed.",
              "rank": 16
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of scientific diagrams, the paragraph could be represented as follows:\n\n[Diagram 1: A timeline showing 4 Gyrs of evolution, with a primordial planetesimal disk at the start and the final architectures of the giant planets at the end.]\n\n[Table 3: A table with two columns. The first column is labeled \"Ratio of Saturn's to Jupiter's orbital period\" and lists a range of ratios from 2.1 to 2.7, with a median value of 2.39. The second column is labeled \"Ratio of Neptune's to Uranus' orbital period\" and lists a range of ratios from 1.97 to 2.31, with a median value of 2.13.]\n\n[Diagram 2: A graph showing the locations of various secular resonances, with a note indicating that these have dynamical consequences primarily for the inner solar system.]\n\n[Diagram 3: A graph showing the architecture of the Kuiper belt, with a note indicating that this is dependent on the ratio of Neptune's to Uranus' orbital period.]\n\n[Diagram 4: A pie chart showing the outcomes of ~1000-particle simulations, with roughly half finishing with a Neptune-to-Uranus period ratio of under 2.]\n\n[Diagram 5: A graph showing the crossing of Uranus and Neptune's 2:1 MMR, with a note indicating that this can destabilize much of the resonant Kuiper belt population.]\n\n[Text Box: A note stating that the crossing of Uranus and Neptune's 2:1 MMR is a poor feature for simulations meant to study Kuiper belt formation.]\n\nTranslation to English: The timeline, table, and diagrams represent the evolution of a primordial planetesimal disk and the final architectures of the giant planets after 4 Gyrs. The table lists key parameters, including the ratios of Saturn's to Jupiter's orbital period and Neptune's to Uranus' orbital period. The diagrams show the locations of various secular resonances, the architecture of the Kuiper belt, the outcomes of ~1000-particle simulations, and the crossing of Uranus and Neptune's 2:1 MMR. The text box notes that the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population and is therefore a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 17
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12183.pdf",
      "paragraphs": [
        {
          "source": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
          "translations": [
            {
              "id": "09-simplify-level12.ts",
              "content": "We examine the resilience of stable pairings when faced with any kind of disruption, even minor ones. These disruptions can occur in both individual and collective settings. Over time, the preferences of participants may change, the makeup of the market may shift, or paired participants may choose to end their partnerships. For example, in job markets, a change in a worker's family situation may lead to new location preferences; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, individual interactions cannot break certain partnerships. Consider a market split into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has one stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, individual interactions cannot unpair any such pair of matched Los Angeles residents. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within the group so that every member prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable pairings in the entire market pair participants within the fragment in the same way. Specifically, Los Angeles residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can result in any stable pairing through individual interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The lack of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the individual process is flexible enough to achieve any stable pairing. Theorem 1 suggests one form of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse’s outcome does not necessarily have to revert back to the intended extreme stable pairing; individual interactions can instead lead to the other extreme stable pairing, or anything in between.",
              "rank": 1
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Let's think about the concept of stable matchings and how they can be affected by various changes or disruptions. These disruptions could be minor or major, and they can occur in different types of settings, whether they're organized or not. Over time, people's preferences might change, the makeup of a market might shift, or existing partnerships might dissolve. \n\nFor example, in the job market, a personal event might cause a worker to prefer a different location; new job roles might be created and workers might become available or retire; an employer and an employee might mutually decide to end their working relationship. These changes could alter the set of stable matchings, potentially leading to instability. This could result in some people being mismatched or not matched at all, even though they should be paired according to the current market conditions. \n\nIn some cases, certain partnerships can't be broken up through individual interactions. Let's imagine a market split into two submarkets, New York and Los Angeles, where everyone in each city prefers their fellow city-dwellers over anyone from the other city. If the New York submarket has one stable matching and the Los Angeles submarket has two, then the whole market naturally has two stable matchings as well. If all the people in Los Angeles are paired according to one of the two stable matchings for their submarket, and therefore for the whole market, individual interactions can't break up any of these Los Angeles pairs. This means that the process of stabilizing the market can't achieve another stable matching for the whole market. \n\nIn this scenario, the people in Los Angeles can be stably matched within their group, with everyone preferring their stable partner over anyone outside Los Angeles. We refer to such a group as a fragment. In this example, the people in New York form a fragment as well. A fragment is considered trivial if all stable matchings in the whole market pair people within the fragment in the same way. In this case, the Los Angeles group is a non-trivial fragment, while the New York group is a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings can't lead to certain stable matchings. However, our first theorem shows that the opposite is also true. If there are no non-trivial fragments, any unstable matching can lead to any stable matching through individual interactions. Non-trivial fragments are the only limitations on the process of stabilizing the market. The absence of non-trivial fragments is a mild condition, as simulations suggest that they're rare in large random markets. Therefore, in most markets, the process of individual interactions is flexible enough to achieve any stable matching. \n\nOur first theorem also suggests a certain vulnerability of stable matchings. A small disruption of a stable matching could result in any stable matching, whether it's similar to or different from the original stable matching. The famous Deferred Acceptance mechanism, which is used to match doctors to hospitals and students to schools, aims to achieve the best stable matching for one side of the market, either the doctors or the students. Our findings suggest that any disruption of the outcome of such a system doesn't necessarily have to return to the intended best stable matching. Individual interactions could instead lead to the other best stable matching, or anything in between.",
              "rank": 2
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study examines the stability of matchings in response to potential perturbations, such as changes in participants' preferences or market composition. These changes could lead to instability, with some participants being mismatched or unmatched. The study introduces the concept of a 'fragment', a group where members prefer to be matched within the group rather than outside. Fragments can be 'trivial' or 'non-trivial', with non-trivial fragments being the only restraints on stabilization dynamics. The absence of non-trivial fragments suggests that most markets can attain any stable matching. The study also suggests that a small perturbation of a stable matching may result in any stable matching, not necessarily reverting back to the original stable matching.",
              "rank": 3
            },
            {
              "id": "06-simplify-college.ts",
              "content": "We examine the resilience of stable pairings in the face of arbitrary, potentially minor, disruptions. These disruptions can occur in both decentralized and centralized environments. Over time, changes in participants' preferences, market composition, or the dissolution of matched partnerships can occur. For example, in job markets, a change in a worker's family situation may lead to new geographical preferences; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under current market conditions may be incorrectly matched or not matched at all. In some cases, decentralized interactions cannot dissolve certain partnerships. Consider a two-sided market split into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has a unique stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Los Angeles residents. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within the group so that every member prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers also form a fragment. A fragment is considered trivial if all stable pairings in the entire market match participants within the fragment in the same way. Specifically, Los Angeles residents constitute a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can result in any stable pairing through decentralized interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. Theorem 1 suggests one form of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings suggest that any disruption of such a clearinghouse's outcome does not necessarily have to revert back to the intended extreme stable pairing; decentralized interactions can instead lead to the other extreme stable pairing, or anything in between.",
              "rank": 4
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "This study examines the resilience of stable matchings when faced with any kind of disruption, even minor ones. These disruptions can occur in both decentralized and centralized settings. Over time, changes in participants' preferences, market composition, or the termination of partnerships between matched participants can occur. For example, in labor markets, a family emergency might cause a worker to prefer a different geographical location; new job positions may become available, or workers may retire; an employer and employee might mutually agree to end their employment contract. While these changes can also affect the set of stable matchings, they can all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, decentralized interactions cannot break certain partnerships.\n\nConsider a two-sided market divided into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has one stable matching and the Los Angeles submarket has two, the entire market naturally has two stable matchings as well. If all Los Angeles residents are paired according to one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Los Angeles residents. As a result, the stabilization dynamics cannot achieve another stable matching in the entire market. In this scenario, Los Angeles residents can be stably matched within their group, with each person preferring their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable matchings in the entire market match participants within the fragment in the same way. In this case, Los Angeles residents form a non-trivial fragment, while New Yorkers form a trivial fragment.\n\nIf a market has a non-trivial fragment, some unstable matchings cannot lead to certain stable matchings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable matching.\n\nTheorem 1 suggests one type of vulnerability of stable matchings. A small disruption of a stable matching may result in any stable matching, whether similar to or different from the original stable matching. The well-known Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable matching, the best stable matching for one side of the market, doctors and students, respectively. Our results imply that any disruption of such a clearinghouse's outcome does not necessarily have to revert back to the intended extreme stable matching; decentralized interactions can instead lead to the other extreme stable matching, or anything in between.",
              "rank": 5
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "We're looking at how stable matchings, or pairings, can withstand changes or disruptions. These disruptions can happen in any setting and can be caused by a variety of factors. For example, in job markets, a worker might need to move due to family reasons, new jobs might open up, or an employer and employee might agree to end their working relationship. These changes can affect the stability of matchings, potentially leading to mismatches or unmatches. \n\nLet's consider a hypothetical scenario where a market is divided into two submarkets, New York and Los Angeles. Everyone in Los Angeles prefers to be paired with someone from Los Angeles, and the same goes for New York. If the New York market has one stable matching and the Los Angeles market has two, the entire market will also have two stable matchings. If all Los Angeles residents are paired according to one of the two stable matchings, these pairings can't be broken up by decentralized interactions. This means that the market can't reach another stable matching. \n\nIn this scenario, we can see that Los Angeles residents can be stably matched within their group, with everyone preferring their partner over anyone from outside Los Angeles. We call this a fragment. New Yorkers form a fragment too. A fragment is considered trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In this case, Los Angeles forms a non-trivial fragment, while New York forms a trivial one. \n\nIf a market has a non-trivial fragment, some unstable matchings can't lead to certain stable matchings. However, the opposite is also true. If there are no non-trivial fragments, any unstable matching can lead to any stable matching. Non-trivial fragments are the only thing that can limit the stabilization process. They're not common in large random markets, so in most cases, the process is flexible enough to reach any stable matching. \n\nThis suggests that stable matchings can be fragile. A small disruption to a stable matching can lead to any other stable matching, whether it's similar to or different from the original one. The Deferred Acceptance mechanism, which is used to match doctors to hospitals and students to schools, aims to achieve the best stable matching for one side of the market. However, any disruption to this outcome doesn't necessarily lead back to the intended stable matching. Instead, it can lead to the other extreme stable matching, or anything in between.",
              "rank": 6
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "We're looking at how stable matchings, or pairings, hold up under various changes. These changes could be small or large and can happen in both decentralized and centralized settings. Over time, people's preferences might change, the makeup of the market might shift, or matched pairs might decide to split up. For example, in job markets, a family event might cause a worker to prefer a different location; new jobs might open up and workers might become available or retire; a company and an employee might mutually decide to end their working relationship. While these changes might also affect the set of stable matchings, they could all lead to instability. Specifically, some people who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, decentralized interactions can't break certain pairings. \n\nLet's imagine a market split into two submarkets, New York and Los Angeles, where everyone in LA prefers any LA resident to any New Yorker, and vice versa. Let's say that the New York submarket has one stable matching and the LA submarket has two. The whole market naturally has two stable matchings as well. If all LA residents are paired according to one of the two stable matchings for the LA submarket, and therefore for the whole market, decentralized interactions can't unmatch any such pair of matched LA residents. As a result, the stabilization dynamics can't reach another stable matching in the whole market. In this case, LA residents can be matched stably within the group so that everyone inside the group prefers their stable partner to anyone outside LA. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the whole market match participants inside the fragment in the same way. In this case, LA residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings can't lead to certain stable matchings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to reach any stable matching. \n\nTheorem 1 suggests one type of fragility of stable matchings. A small change in a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable matching, the best stable matching for one side of the market, doctors and students, respectively. Our results imply that any change in such a clearinghouse’s outcome doesn't have to revert back to the intended extreme stable matching; decentralized interactions can instead lead to the other extreme stable matching, or anything in between.",
              "rank": 7
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"We examine the stability of matchings under potential perturbations. These disruptions can occur in both decentralized and centralized settings. Participants' preferences may change, the market composition may alter, or existing partnerships may dissolve. For example, in labor markets, a family event may create new geographical preferences for workers; new positions may emerge and workers may become available or retire; an employer and employee may agree to end their employment. These changes can affect the set of stable matchings and cause instability. Some participants that should be paired under current market conditions might be mismatched or unmatched. In some cases, decentralized interactions cannot dissolve certain partnerships. Consider a two-sided market divided into New York and Los Angeles submarkets, where every resident prefers any local to any outsider. If the New York submarket has a unique stable matching and the Los Angeles submarket has two, the entire market naturally has two stable matchings. If all Los Angeles residents are paired according to one of the two stable matchings for their submarket, decentralized interactions cannot unpair any such pair. Consequently, the stabilization dynamics cannot achieve another stable matching in the entire market. Here, Los Angeles residents can be stably matched within the group so that every member prefers their stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. Los Angeles residents constitute a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. Without nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process can achieve any stable matching. Theorem 1 suggests one type of fragility of stable matchings. A small disruption of a stable matching may result in any stable matching, near or far from the original stable matching. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any disruption of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.\"",
              "rank": 8
            },
            {
              "id": "source",
              "content": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
              "rank": 9
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "We're looking at how stable pairings hold up when changes occur. These changes can happen in different ways and in different settings. Over time, people's likes and dislikes can change, the makeup of a group can change, or paired individuals might decide to part ways. For example, in job markets, a family event might make a worker want to move; new jobs might come up and workers might become available or retire; a boss and an employee might agree to end their work relationship. While these changes might also affect the stable pairings, they could all lead to instability. Specifically, some individuals who should be paired based on the current conditions might be wrongly paired or not paired at all. In some cases, certain pairings can't be broken up through normal interactions. Imagine a market split into two smaller markets, New York and Los Angeles, where everyone in each city prefers someone from their own city over someone from the other city. If the New York market has one stable pairing and the Los Angeles market has two, the whole market naturally has two stable pairings as well. If all people in Los Angeles are paired according to one of the two stable pairings for the Los Angeles market, and therefore for the whole market, normal interactions can't break up any such pair of matched people in Los Angeles. As a result, the dynamics of stabilizing can't achieve another stable pairing in the whole market. In this case, people in Los Angeles can be paired in a stable way within the group so that everyone inside prefers their stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable pairings in the whole market pair individuals inside the fragment in the same way. Specifically, people in Los Angeles make up a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings can't lead to certain stable pairings. Our first theorem shows that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can lead to any stable pairing through normal interactions. Non-trivial fragments are the only limits on the dynamics of stabilizing. The lack of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the normal process is flexible enough to achieve any stable pairing. Our first theorem suggests one type of weakness of stable pairings. A small change in a stable pairing may result in any stable pairing, close to or far from the original stable pairing. The famous Deferred Acceptance method, which is used to match doctors to hospitals and students to schools, aims at achieving the best stable pairing for one side of the market, doctors and students, respectively. Our results imply that any change in such a clearinghouse’s outcome doesn't have to go back to the intended best stable pairing; normal interactions can instead lead to the other best stable pairing, or anything in between.",
              "rank": 10
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "We're looking at how stable matches can handle changes. These changes can happen in different ways. For example, people's likes and dislikes might change, the number of people in a group might change, or people who were matched might decide to stop being matched. This can happen in job markets. For example, a worker might need to move because of family reasons, new jobs might open up, or a worker might retire. Sometimes, a worker and their boss might agree to end the job. These changes can also change the stable matches. They can even make the matches unstable. This means that some people who should be matched might not be, or they might be matched with the wrong person. \n\nLet's imagine a market that's split into two parts, like New York and Los Angeles. Everyone in Los Angeles prefers to be matched with someone else in Los Angeles, and the same goes for New York. Let's say that there's only one stable match in New York, but two in Los Angeles. This means that there are two stable matches for the whole market. If everyone in Los Angeles is matched according to one of the two stable matches, then no one in Los Angeles can be unmatched. This means that the matches can't change to the other stable match. In this case, people in Los Angeles can be matched within their group, and they prefer this to being matched with someone outside of Los Angeles. We call this a \"fragment\". In this example, New York is also a fragment. A fragment is \"trivial\" if all the stable matches are the same within the fragment. In this case, Los Angeles is a non-trivial fragment, and New York is a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matches can't become stable matches. But, if there are no non-trivial fragments, any unstable match can become a stable match. Non-trivial fragments are the only thing that can stop this from happening. But, they don't happen very often in big, random markets. So, in most markets, the matches can change to any stable match. \n\nThis shows that stable matches can be fragile. A small change to a stable match can lead to any other stable match. The famous \"Deferred Acceptance\" method, which is used to match doctors to hospitals and students to schools, tries to make the best stable match for one side (like the doctors or the students). But, our results show that any change to this match doesn't have to go back to the original match. Instead, it can lead to the other best match, or anything in between.",
              "rank": 11
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "\"We delve into the resilience of stable pairings in the face of unpredictable, potentially minor, disruptions. These disruptions can occur in both decentralized and centralized environments. As time passes, the preferences of participants may evolve, the makeup of the market may transform, or paired participants may dissolve their partnerships. For example, in the job market, a change in a worker's family situation may create new geographical preferences; new roles may emerge and workers may enter or exit the workforce; an employer and an employee may mutually decide to end their professional relationship. While these changes may also modify the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might end up mismatched or unpaired. In certain scenarios, decentralized interactions cannot dissolve certain partnerships. Consider a two-sided market split into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. Let's say that the New York submarket has a unique stable pairing and the Los Angeles submarket has two stable pairings. The entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Los Angeles residents. Consequently, the stabilization dynamics cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be paired stably within the group so that every insider prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable pairings in the entire market pair participants within the fragment in the same way. Specifically, Los Angeles residents constitute a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can lead to any stable pairing through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The lack of non-trivial fragments is a mild condition: simulations suggest that they are uncommon in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. Theorem 1 suggests a type of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether it's close to or far from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to pair doctors with hospitals and students with schools, aims to implement an extremal stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse’s outcome does not necessarily have to revert back to the intended extremal stable pairing; decentralized interactions can instead lead to the other extremal stable pairing, or anything in between.\"",
              "rank": 12
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- The study focuses on the robustness of stable matchings in the face of arbitrary or minimal perturbations.\n- These perturbations can occur in both decentralized and centralized settings, and can be due to shifts in participants' preferences, changes in market composition, or termination of partnerships.\n- Such changes can lead to instability, with participants being mismatched or unmatched under current market conditions.\n- In some situations, decentralized interactions cannot break certain partnerships, leading to the concept of a 'fragment'.\n- A fragment is a group where members can be stably matched within the group, with each member preferring their stable partner to anyone outside the group.\n- A fragment is 'trivial' if all stable matchings in the entire market match participants inside the fragment in the same way.\n- If a market has a 'non-trivial' fragment, some unstable matchings cannot yield certain stable matchings.\n- The absence of non-trivial fragments allows any unstable matching to yield any stable matching through decentralized interactions.\n- Non-trivial fragments are the only restraints on stabilization dynamics and are rare in large random markets.\n- Theorem 1 suggests that stable matchings are fragile, with a small perturbation potentially leading to any stable matching.\n- The Deferred Acceptance mechanism aims to implement an extremal stable matching, but any perturbation of the outcome may not revert back to the intended extremal stable matching.",
              "rank": 13
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "We are looking at how changes can affect stable matches. Stable matches are when things or people are paired together in a way that works best for everyone. But sometimes, things can change. People's likes and dislikes can change. The number of people or things available can change. Or, pairs that were matched can decide to stop being a pair. \n\nFor example, think about a job market. A worker might need to move because of their family, so they want a job in a new place. New jobs might open up, or workers might retire. A worker and their boss might decide to end their work relationship. All these changes can mess up the stable matches. Some pairs that should be together might not be, or some things or people might not be paired at all. \n\nSometimes, these changes can't break up certain pairs. Let's imagine a market split into two parts, like New York and Los Angeles. Everyone in Los Angeles prefers to be paired with someone else in Los Angeles, and the same goes for New York. If all the people in Los Angeles are paired in a way that works best for them, these pairs can't be broken up. This means that the whole market can't reach a new set of stable matches. \n\nIn this case, the people in Los Angeles form a group that we call a \"fragment\". A fragment is a group where everyone prefers to be paired with someone else in the group. The people in New York also form a fragment. But there's a difference. In Los Angeles, the pairs can change. In New York, the pairs always stay the same. We call the New York type of fragment \"trivial\", and the Los Angeles type \"non-trivial\". \n\nIf a market has a non-trivial fragment, it can't reach certain stable matches. But if there are no non-trivial fragments, any pair can change to reach a new set of stable matches. Non-trivial fragments are the only thing that can stop this. But don't worry, they are not common. \n\nThis tells us that stable matches can be fragile. A small change can lead to a big change in the matches. For example, a system used to match doctors to hospitals and students to schools tries to make the best matches for the doctors and students. But if there's a small change, the matches might not go back to the best ones. They could end up being the worst ones, or anything in between.",
              "rank": 14
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The study examines the stability of matchings in various settings, such as labor markets, and how they can be affected by changes like shifts in preferences, market composition, or termination of partnerships. The research introduces the concept of a \"fragment\", a group where members can be stably matched within the group and prefer their stable partner to anyone outside the group. The study finds that if a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. However, in the absence of non-trivial fragments, any unstable matching can yield any stable matching. The study suggests that non-trivial fragments are rare in large random markets, meaning that in most markets, the decentralized process can attain any stable matching. The study also suggests that a small perturbation of a stable matching may result in any stable matching, close to or distant from the original stable matching.",
              "rank": 15
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of mathematics, the paragraph could be represented as follows:\n\nLet M be a set of stable matchings in a market. We study the robustness of M under arbitrary perturbations, denoted as P. P can be minimal and can occur in both decentralized (D) and centralized (C) settings. Over time, participants' preferences (Pr), market composition (Mc), or matched participants (Mp) might change. For example, in labor markets, a family shock (Fs) may generate new geographical preferences (Gp) for workers; new positions (Np) may appear and workers may either become available (A) or retire (R); an employer and an employee might mutually agree to terminate the employment relationship (Te). While M may also be altered by such changes, they could all lead to instability (I). In particular, some participants that should be paired under current market conditions might be mismatched (Mm) or unmatched (Um). In some situations, D cannot break certain partnerships (Pp). Consider a two-sided market divided into two submarkets, New York (Ny) and Los Angeles (La), in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that Ny has a unique stable matching (Usm) and La has two stable matchings (Tsm). The entire market naturally has Tsm as well. If all Angelenos are paired in accordance with one of the Tsm for La, and therefore for the entire market, D cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics (Sd) cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside La. We call such a group a fragment (F). In this example, New Yorkers form a F as well. A F is trivial (Tf) if all stable matchings in the entire market match participants inside the F in the same way. In particular, Angelenos constitute a non-trivial fragment (Ntf), while New Yorkers form a Tf. If a market has a Ntf, some unstable matchings (Um) cannot yield certain stable matchings (Csm). Theorem 1 (T1) shows that the reverse is also true. When there are no Ntf, any Um can yield any Csm through D. Ntf are the only restraints on the Sd. The absence of Ntf is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the D process is fluid enough to attain any Csm. T1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
              "rank": 16
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "We delve into the resilience of stable pairings in the face of unpredictable, and potentially minor, disruptions. These disturbances can emerge in both decentralized and centralized environments. As time progresses, the preferences of participants may evolve, the composition of the market may transform, or paired participants may choose to dissolve their partnerships. For example, in the realm of employment, a sudden change in a worker's family situation may create new geographical preferences; new job opportunities may arise and workers may either enter or exit the workforce; an employer and an employee may mutually decide to end their working relationship. While these changes may also modify the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might find themselves mismatched or not matched at all. In certain scenarios, decentralized interactions are unable to dissolve certain partnerships. \n\nConsider a market split into two submarkets, New York and Los Angeles, where every inhabitant of Los Angeles prefers any fellow Angeleno to any New Yorker, and vice versa. Let's say that the New York submarket has a unique stable pairing and the Los Angeles submarket has two stable pairings. The entire market naturally has two stable pairings as well. If all Angelenos are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Angelenos. Consequently, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Angelenos can be paired stably within their group such that every member prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable pairings in the entire market pair participants within the fragment in the same manner. Specifically, Angelenos form a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can lead to any stable pairing through decentralized interactions. Non-trivial fragments are the sole constraints on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are infrequent in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. \n\nTheorem 1 hints at a type of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether it's close to or far from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to pair doctors with hospitals and students with schools, aims to implement an extremal stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings suggest that any disruption of such a clearinghouse’s outcome does not necessarily have to revert back to the intended extremal stable pairing; decentralized interactions can instead lead to the other extremal stable pairing, or anything in between.",
              "rank": 17
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12147.pdf",
      "paragraphs": [
        {
          "source": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]",
          "translations": [
            {
              "id": "09-simplify-level12.ts",
              "content": "When electrons move within a crystal lattice made up of ions with opposite charges, they cause distortions in the lattice, known as phonons, in their immediate vicinity. These distortions then interact with the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. This combined entity is referred to as a polaron. When multiple electrons are involved, the term used is multi-polarons.\n\nH. Fröhlich introduced a mathematical model in 1954 that describes the behavior of multi-polarons. In this model, electrons are considered as non-relativistic quantum mechanical particles without spin, while phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nThe study of polaron models has primarily used functional integration, a technique first introduced by Feynman in 1955. This technique has been widely used in both theoretical physics and mathematics. This article will provide numerous references to mathematical papers that have used various Feynman–Kac formulas to calculate the expected values of members of the semigroup generated by Fröhlich’s model.\n\nThis article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space. This is done under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, which still allows for the definition of the Hamiltonian via semibounded quadratic forms.\n\nIn some studies, the electrons are confined to open regions of Euclidean space for technical reasons, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions on the electron-phonon interaction that cover these two situations as well as the original Fröhlich model.\n\nThe Feynman–Kac formulas established in this article, along with the inequalities, can serve as a foundation for further studies of the semigroup and ground state eigenvectors (if any) in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 1
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine electrons moving within a crystal lattice, a structure made up of oppositely charged ions. As they move, they cause distortions in the lattice, creating what we call phonons. These phonons, in turn, affect the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. We refer to this electron-phonon combination as a polaron. When we consider multiple electrons, we talk about multi-polarons.\n\nH. Fröhlich, in his 1954 work, introduced a mathematical model, a Hamiltonian, to describe the behavior of these multi-polarons. In his model, electrons are considered as non-relativistic quantum mechanical particles without spin, while the phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the pioneering work of Feynman in 1955, functional integration has been a key method in studying polaron models, both in theoretical physics and mathematics. We will soon delve into numerous mathematical papers that utilize various Feynman–Kac formulas to understand the behavior of the semigroup generated by Fröhlich’s Hamiltonian.\n\nIn this article, we will focus on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model. We will consider the interaction of electrons via repulsive Coulomb potentials and the behavior of polarons in external electric and magnetic fields. We will work under optimal conditions for the electrostatic and magnetic vector potentials to define the Hamiltonian.\n\nIn some studies, the electrons are confined to open regions of space, and sometimes both the electrons and the phonons are confined. We will consider these scenarios as well as the original Fröhlich model. \n\nOur work here, along with the inequalities we establish, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 2
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"When electrons move in a crystal lattice of oppositely charged ions, they create lattice distortions (phonons) that react back on the electrons through their polarization. This results in each electron being surrounded by a phonon cloud, reducing its mobility. This composite object is known as a polaron; when multiple electrons are involved, we refer to multi-polarons. H. Fröhlich introduced a Hamiltonian in [Frö54] that governs the dynamics of multi-polarons. In his model, the electrons are non-relativistic quantum mechanical particles without spin degrees of freedom, while the phonons, which can be created and annihilated over time, are described by a non-relativistic bosonic quantum field. \n\nFunctional integration has been a primary technique in the study of polaron models since Feynman's seminal work [Fey55], both in theoretical physics and mathematics. In Section 1.3, we will provide references to mathematical papers that use various Feynman–Kac formulas for vacuum expectation values of semigroup members generated by Fröhlich’s Hamiltonian. \n\nThis article, based on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space. We will work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, allowing us to define the Hamiltonian through semibounded quadratic forms. \n\nIn some studies, the electrons are confined to open regions of Euclidean space [AL13, FLST11], and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we will work under general hypotheses on the electron-phonon interaction, covering these situations as well as the original Fröhlich model. \n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors (if any) in polaron models, similar to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22].\"",
              "rank": 3
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "This passage discusses the movement of electrons within a crystal lattice made up of oppositely charged ions. As these electrons move, they create lattice distortions, known as phonons, in their surrounding areas. These phonons then interact with the electrons, resulting in each electron being surrounded by a cloud of phonons that reduce its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich introduced a mathematical model in 1954 to explain the dynamics of multi-polarons. In this model, electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom. Phonons, on the other hand, are described as non-relativistic bosonic quantum fields that can be created and annihilated over time.\n\nThe study of polaron models has primarily involved functional integration, a technique first used by Feynman in 1955. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. It focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space.\n\nThe article also considers the interaction of electrons via repulsive Coulomb potentials and the treatment of polarons in external electric and magnetic fields. It works under optimal conditions on the electrostatic potential and the magnetic vector potential. In some cases, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. The article takes these situations into account, as well as the original Fröhlich model.\n\nThe Feynman–Kac formulas established in this article, along with the inequalities, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 4
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "Electrons move in a structure made of charged particles. This movement changes the structure around them. This change also affects the electrons. Because of this, each electron moves with a group of changes, which slows it down. We call this group of changes a 'polaron'. If there are many electrons, we call it 'multi-polarons'. \n\nH. Fröhlich made a rule to understand how multi-polarons work. In his rule, he thought of electrons as tiny particles that don't move very fast. He also thought of the changes in the structure as something that can come and go over time. \n\nPeople have used a method called 'functional integration' to understand polaron models. This method was first used by Feynman. We will talk about many papers that used this method to understand the rule made by Fröhlich. \n\nWe will use this paper to explain how to use Feynman's method in Fröhlich's multi-polaron model. We will use this method for any type of particle in the structure. \n\nElectrons push each other away and polarons can be affected by electric and magnetic fields. We will work with the best conditions for the electric field and the magnetic field. \n\nIn some papers, the electrons are limited to certain areas. Sometimes, both the electrons and the changes in the structure are limited. So, we will work with any type of interaction between the electron and the changes in the structure. \n\nWith the rules we make in this paper, we can study more about the group of changes and the slowest moving electron in polaron models. This is similar to the study of magnetic groups and its extensions to other models.",
              "rank": 5
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "When electrons move within a crystal structure made up of ions with opposite charges, they cause distortions in the structure. These distortions, known as phonons, in turn affect the electrons due to the polarization they carry. This results in each electron being surrounded by a cloud of phonons, which reduces its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, we refer to it as multi-polarons.\n\nH. Fröhlich introduced a mathematical model in 1954 to explain the behavior of multi-polarons. In his model, electrons are considered as non-relativistic quantum particles without spin, while the phonons, which can be created and destroyed over time, are described by a non-relativistic quantum field.\n\nSince the introduction of Fröhlich's model, one of the main methods used to study polaron models has been functional integration. This method has been widely used in both theoretical physics and mathematics. We will provide numerous references to mathematical papers that have used various formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s model.\n\nIn this article, we will focus on deriving formulas in Fröhlich’s multi-polaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. We will work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential.\n\nIn some studies, the electrons are confined to open regions of space for technical reasons, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions on the electron-phonon interaction covering these situations as well as the original Fröhlich model.\n\nTogether with the inequalities established in this article, our formulas can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 6
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The author is discussing the movement of electrons within a crystal lattice, which is a structure made up of positively and negatively charged ions. When these electrons move, they cause distortions in the lattice, known as phonons. These phonons then interact with the electrons, causing each electron to be surrounded by a cloud of phonons, which reduces its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich proposed a model to explain the dynamics of these multi-polarons. In his model, the electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom. The phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nThe study of polaron models has primarily used functional integration, a technique first introduced by Feynman. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. The aim is to derive Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space.\n\nThe author also discusses the interaction of electrons via repulsive Coulomb potentials and the treatment of polarons exposed to external electric and magnetic fields. The author plans to work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential. \n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, the author will work under general hypotheses on the electron-phonon interaction covering these situations as well as the original Fröhlich model.\n\nThe author concludes by stating that the Feynman–Kac formulas established in this article can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 7
            },
            {
              "id": "06-simplify-college.ts",
              "content": "Electrons moving within a crystal lattice, composed of ions with opposing charges, generate lattice distortions, or phonons, in their immediate vicinity. These phonons, in turn, influence the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a phonon cloud, which reduces its mobility. This combined entity is referred to as a polaron, and when multiple electrons are involved, the term multi-polarons is used. \n\nH. Fröhlich proposed a Hamiltonian in [Frö54] to govern the dynamics of multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field. \n\nFunctional integration has been a primary method in the study of polaron models since Feynman's pioneering work [Fey55], both in theoretical physics and mathematics. In Section 1.3, we will provide numerous references to mathematical papers that utilize various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian. \n\nThis article is dedicated to deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. This is based on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a]. \n\nGiven that electrons interact through repulsive Coulomb potentials and polarons often interact with external electric and magnetic fields, we will work under nearly optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential. This allows us to define the Hamiltonian via semibounded quadratic forms. \n\nIn some studies, electrons are confined to open regions of Euclidean space [AL13, FLST11], often for technical reasons, and sometimes both electrons and phonons are confined [FS21, BM23]. Therefore, we will operate under general assumptions on the electron-phonon interaction that cover these two situations as well as the original Fröhlich model. \n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors (if any) in polaron models. This is analogous to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22].",
              "rank": 8
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "Electrons moving in a crystal lattice create lattice distortions, or phonons, which affect the electrons' mobility. This composite object is known as a polaron, and when multiple electrons are involved, they are referred to as multi-polarons. H. Fröhlich introduced a Hamiltonian that governs the dynamics of multi-polarons, treating electrons as non-relativistic quantum mechanical particles and phonons as a non-relativistic bosonic quantum field. The investigation of polaron models often uses functional integration, a technique introduced by Feynman. This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. The study also considers the effects of repulsive Coulomb potentials and external electric and magnetic fields on polarons. The findings can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models.",
              "rank": 9
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Electrons moving in a crystal lattice create lattice distortions (phonons) which affect the electrons' mobility.\n- These composite objects of electrons and phonons are called polarons; multiple electrons create multi-polarons.\n- H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons, treating electrons as non-relativistic quantum mechanical particles without spin degrees of freedom.\n- Phonons, which can be created and annihilated over time, are described by a non-relativistic bosonic quantum field.\n- Functional integration has been a main technique in investigating polaron models, both in theoretical physics and mathematics.\n- This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space.\n- The study considers the interaction of electrons via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields.\n- The study also considers situations where electrons are confined to open regions of Euclidean space and where both electrons and phonons are confined.\n- The derived Feynman–Kac formulas, along with established inequalities, can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models.",
              "rank": 10
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The passage discusses the dynamics of multi-polarons, composite objects formed when electrons moving in a crystal lattice create lattice distortions (phonons) that lower the electrons' mobility. H. Fröhlich introduced a Hamiltonian model to govern these dynamics, treating electrons as non-relativistic quantum mechanical particles and phonons as a non-relativistic bosonic quantum field. The study of polaron models has largely relied on functional integration, a technique introduced by Feynman. The article aims to derive Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. The authors also consider the effects of repulsive Coulomb potentials and external electric and magnetic fields on polarons. The findings can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models.",
              "rank": 11
            },
            {
              "id": "source",
              "content": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]",
              "rank": 12
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In the realm of crystal lattices, composed of ions of opposing charges, the movement of electrons creates a fascinating phenomenon. These electrons, in their journey, generate lattice distortions, or phonons, in their immediate vicinity. These phonons, in turn, react to the electrons through the polarization they carry. The result? Each electron is enveloped by a cloud of phonons, which hampers its mobility. This electron-phonon entity is known as a polaron, and when multiple electrons are involved, we refer to them as multi-polarons.\n\nH. Fröhlich, in his 1954 work, introduced a Hamiltonian that governs the dynamics of these multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles devoid of spin degrees of freedom. The phonons, on the other hand, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field.\n\nThe investigation of polaron models has been largely influenced by the pioneering work of Feynman in 1955. His technique of functional integration has been widely used in both theoretical physics and mathematics. In the following section, we will provide numerous references to mathematical papers that utilize various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.\n\nThis article is dedicated to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. This is based on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. We will be working under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, which still allows us to define the Hamiltonian via semibounded quadratic forms.\n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will be working under general hypotheses on the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is analogous to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 13
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "When tiny particles called electrons move within a structure made up of charged particles, they cause changes in the structure around them. These changes can affect the electrons themselves. This results in each electron being surrounded by a group of these changes, which can slow down the electron's movement. This combination of an electron and the changes it causes is called a polaron. When we talk about multiple electrons, we refer to them as multi-polarons.\n\nH. Fröhlich, in his 1954 work, introduced a mathematical model to explain the behavior of multi-polarons. In his model, the electrons are treated as particles that follow the rules of quantum mechanics, but without considering their spin. The changes caused by the electrons, which can appear and disappear over time, are described by a quantum field.\n\nSince the pioneering work of Feynman in 1955, one main method in studying polaron models has been functional integration, used in both theoretical physics and mathematics. In this article, we will discuss the derivation of mathematical formulas in Fröhlich's multi-polaron model.\n\nElectrons interact with each other through repulsive forces and polarons can be affected by external electric and magnetic fields. In some studies, the electrons are limited to certain areas of space, and sometimes both the electrons and the changes they cause are limited. Therefore, we will consider general assumptions on the interaction between the electrons and the changes they cause.\n\nThe mathematical formulas we discuss in this article, along with the established inequalities, can be used as a foundation for further studies of the behavior and properties of polarons. This is similar to the theory of magnetic Schrödinger groups and its extensions to related models in quantum electrodynamics and nuclear interactions.",
              "rank": 14
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "When electrons move in a crystal made of ions with different charges, they cause changes in the crystal's structure. This makes the electrons move slower because they are surrounded by these changes. This is called a polaron. When there are many electrons, we call it multi-polarons. H. Fröhlich created a model to explain how multi-polarons work. In his model, electrons are treated as particles that don't move at the speed of light and don't have spin. The changes in the crystal, which can be created and destroyed over time, are described by a field that doesn't move at the speed of light.\n\nSince the work of Feynman, one main way to study polaron models has been functional integration. This is used in both physics and math. We will give many examples of math papers that use different Feynman–Kac formulas for the values of members of the group created by Fröhlich’s model.\n\nIn this article, we will use recent math studies of Feynman–Kac formulas to explain Fröhlich’s multipolaron model. We will do this for any vectors in the space we are studying. We will also consider how electrons interact with each other and how polarons react to electric and magnetic fields. \n\nIn some studies, the electrons are limited to certain areas of space. Sometimes, both the electrons and the changes in the crystal are limited. So, we will consider these situations as well as the original model by Fröhlich. \n\nWith the formulas we create in this article, we can study the group and the lowest energy states in polaron models. This is similar to the study of groups in magnetic Schrödinger models and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 15
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the intricate dance of electrons within a crystal lattice, a structure composed of ions of opposing charges, the movement of these electrons generates distortions in the lattice, known as phonons. These phonons, in turn, influence the electrons through the polarization they carry, resulting in each electron being surrounded by a cloud of phonons that reduces its mobility. This composite entity is referred to as a polaron, and when multiple electrons are involved, we speak of multi-polarons.\n\nIn the mid-20th century, H. Fröhlich proposed a mathematical model, a Hamiltonian, to describe the dynamics of these multi-polarons. In his model, electrons are depicted as non-relativistic quantum mechanical particles, devoid of spin degrees of freedom, while the phonons, capable of being created and annihilated over time, are represented by a non-relativistic bosonic quantum field.\n\nThe exploration of polaron models has been greatly influenced by the pioneering work of Richard Feynman, who introduced the technique of functional integration, a method that has found widespread use in both theoretical physics and mathematics. In this article, we will delve into the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model, building on recent mathematical studies in non- and semi-relativistic quantum field theory.\n\nElectrons interact through repulsive Coulomb potentials, and polarons, when subjected to external electric and magnetic fields, are often treated under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential. In some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions on the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further exploration of the semigroup and ground state eigenvectors in polaron models. This is analogous to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 16
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of Quantum Physics:\n\n\"Electrons in a crystal lattice of oppositely charged ions create lattice distortions, or phonons, which in turn affect the electrons due to the polarization they carry. This results in each electron being surrounded by a cloud of phonons, reducing its mobility. This electron-phonon composite is known as a polaron, and when multiple electrons are involved, we refer to them as multi-polarons. H. Fröhlich proposed a Hamiltonian in [Frö54] to govern the dynamics of multi-polarons. In his model, electrons are non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are described by a non-relativistic bosonic quantum field. \n\nFunctional integration has been a key technique in the study of polaron models since Feynman's seminal work [Fey55], both in theoretical physics and mathematics. In Section 1.3, we will provide numerous references to mathematical papers that utilize various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian. \n\nThis article is dedicated to deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space, building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a]. \n\nGiven that electrons interact via repulsive Coulomb potentials and polarons often interact with external electric and magnetic fields – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we will work under nearly optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential, still allowing for the definition of the Hamiltonian via semibounded quadratic forms. \n\nIn some studies, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we will work under general hypotheses on the electron-phonon interaction that cover these two situations as well as the original Fröhlich model. \n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models, similar to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22].\"",
              "rank": 17
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12869.pdf",
      "paragraphs": [
        {
          "source": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation.",
          "translations": [
            {
              "id": "09-simplify-level12.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, we can easily create a comprehensive schedule that can solve most of the problems within the time limit set by the CASC competition. This implies that to prevent CASC from becoming a memory contest, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can compete with the best approaches. Given a time limit of about 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time limits, creating an optimal schedule becomes impractical, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main focus of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the quick runtime and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration.",
              "rank": 1
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "In this study, we independently evaluated the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each optimized for a single problem. These strategies allowed us to build a comprehensive schedule covering most solvable problems within the CASC competition's budget. This implies that CASC should incorporate a significant set of new problems each year to avoid becoming a memorization contest. \n\nWe suggested a greedy schedule construction method to create robust schedules using the discovered strategies, which can rival optimal methods. With a time budget of about 2 minutes, the greedy algorithm takes less than a minute to generate a schedule that solves nearly as many problems as an optimal schedule, which requires over 16 hours to create. For shorter time budgets, optimal schedule construction becomes unfeasible, while the greedy method continues to produce strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be enhanced by various regularization mechanisms, which are the main contribution of this study. A well-selected regularization allows us to surpass the optimal schedule on new problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 2
            },
            {
              "id": "06-simplify-college.ts",
              "content": "In this study, we independently assessed the Spider-style [33] approach to strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, constructing a comprehensive schedule that addresses most solvable problems within the CASC competition's budget is straightforward. This implies that to prevent CASC from becoming primarily a memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can rival optimal approaches. Given a time budget of roughly two minutes, the greedy algorithm requires less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time budgets, constructing an optimal schedule becomes impractical, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected efficacy of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization enables us to surpass the optimal schedule in solving unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration.",
              "rank": 3
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library and gathered over a thousand Vampire proving strategies. Each of these strategies was optimized in advance to perform well on a single problem. With these strategies, we could easily build a comprehensive schedule that covers most of the solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a memory-based competition, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the discovered strategies, we suggested a greedy schedule construction method that can compete with optimal approaches. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule can solve, which takes more than 16 hours to generate. For shorter time budgets, creating an optimal schedule is no longer viable, but the greedy construction still produces relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 4
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, using over a thousand Vampire proving strategies from the FOF fragment of the TPTP library. The study suggests that the CASC competition should use a substantial set of previously unseen problems each year to avoid being a competition in memorization. A proposed greedy schedule construction procedure can compete with optimal approaches, taking less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate. The strength of the greedy scheduler can be reinforced by regularization mechanisms, which are the main contribution of this work. These mechanisms allow us to outperform the optimal schedule on unseen problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 5
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Conducted an independent evaluation of Spider-style strategy discovery and schedule creation.\n- Collected over a thousand Vampire proving strategies, each optimized for a single problem.\n- Constructed a single schedule covering most solvable problems within the CASC competition budget.\n- Emphasized the need for CASC to use a substantial set of unseen problems each year to avoid being a memorization competition.\n- Proposed a greedy schedule construction procedure that can compete with optimal approaches.\n- The greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate.\n- For shorter time budgets, optimal schedule construction is not feasible, but greedy construction still produces strong schedules.\n- The strength of the greedy scheduler can be reinforced by various regularization mechanisms, which is the main contribution of this work.\n- An appropriately chosen regularization can outperform the optimal schedule on unseen problems.\n- The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 6
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. Over a thousand Vampire proving strategies were collected, each optimized for a single problem. These strategies can be used to construct a single schedule that covers most solvable problems within the CASC competition's budget. The study suggests that CASC should use a substantial set of new problems each year to avoid becoming a memorization competition. A greedy schedule construction procedure was proposed, which can compete with optimal approaches. The greedy algorithm takes less than a minute to produce a schedule that solves over 99% of problems, compared to an optimal schedule that takes over 16 hours to generate. The strength of the greedy scheduler can be enhanced by regularization mechanisms, which is the main contribution of this work. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 7
            },
            {
              "id": "source",
              "content": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation.",
              "rank": 8
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "In this study, we independently assessed a strategy known as Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, gathering over a thousand problem-solving strategies, each one specifically designed to excel at a single problem. With these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems solvable within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust plans using the strategies we discovered, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the best plan is no longer possible, while our quick method still produces relatively strong plans. \n\nThe surprising effectiveness of our quick planning method can be further enhanced by various adjustment mechanisms, which are the main focus of this study. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the speed and simplicity of our quick planning method and the adjustment techniques make them appealing for reuse and further exploration.",
              "rank": 9
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "In this project, we independently assessed a strategy called Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, known as the FOF fragment. We gathered over a thousand problem-solving strategies from a tool called Vampire, each one specifically designed to excel at a single problem. \n\nWith these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo create robust plans using the strategies we discovered, we suggested a method called greedy schedule construction. This method can compete with the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan can, which takes over 16 hours to create. For shorter time limits, creating the best plan is no longer possible, while the greedy method still produces relatively strong plans.\n\nThe surprising effectiveness of the greedy method can be further enhanced by various adjustment mechanisms, which are the main focus of this project. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy method and the adjustment techniques make them appealing for reuse and further exploration.",
              "rank": 10
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "In this study, we tested a method called Spider-style strategy discovery and schedule creation. We focused on a specific part of the TPTP library, gathering over a thousand strategies from a system called Vampire. Each of these strategies was designed to work well on one specific problem. With these strategies, we can easily create a single, comprehensive plan that can solve most of the problems that can be solved within the time limit set by the CASC competition. This suggests that to keep the CASC competition from being just a memory test, it's important to use a lot of new problems each year. \n\nTo create strong plans using the strategies we found, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves almost as many problems as the best plan, which takes over 16 hours to create. For shorter time limits, creating the best plan isn't possible, but our quick method still creates relatively strong plans. \n\nThis surprising effectiveness of our quick method can be improved even more by using various adjustment mechanisms, which is the main focus of this study. Choosing the right adjustment allows us to do better than the best plan on new problems. Lastly, the quickness and simplicity of our quick plan creation method and the adjustment techniques make them good options for reuse and further testing.",
              "rank": 11
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In this study, we independently assessed the Spider-style [33] method of strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each one a priori optimized for a single problem. With these strategies, it's simple to build a single, comprehensive schedule that can solve most of the problems solvable within the CASC competition's budget. This implies that to prevent CASC from becoming primarily a memory contest, it's crucial to introduce a significant number of new problems each year. To build robust schedules using the discovered strategies, we suggested a greedy schedule construction process that can compete with optimal methods. Given a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to generate. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. The surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 12
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In our exploration, we embarked on an autonomous assessment of the Spider-style [33] approach to strategy discovery and schedule creation. Our focus was the FOF fragment of the TPTP library, from which we gathered over a thousand Vampire proving strategies, each one meticulously optimized to excel on a singular problem. With these strategies in hand, the construction of a single, all-encompassing schedule that covers the majority of solvable problems within the CASC competition's budget becomes a straightforward task. This implies that to prevent CASC from becoming a mere memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the unearthed strategies, we put forth a greedy schedule construction method, capable of holding its own against optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm takes less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which requires over 16 hours to create, can solve. As the time budget shrinks, the feasibility of constructing an optimal schedule diminishes, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected potency of the greedy scheduler can be further bolstered by various regularization mechanisms, which form the crux of our contribution in this study. A carefully selected regularization enables us to surpass the optimal schedule's performance on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm, coupled with the regularization techniques, make them appealing for reuse and further exploration.",
              "rank": 13
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to assist you in comprehending this complex subject matter. \n\nIn our study, we independently assessed the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each individually optimized for a single problem. With these strategies, we can easily create a comprehensive schedule that addresses most solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a mere memory contest, it's crucial to introduce a significant set of new problems each year. \n\nWe suggested a greedy schedule construction method to build robust schedules using the discovered strategies, which can rival optimal methods. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to create. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization allows us to surpass the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 14
            },
            {
              "id": "12-role-teacher.ts",
              "content": "In this study, we independently assessed a method known as the Spider-style strategy for problem-solving and planning. We focused on a specific section of the TPTP library, known as the FOF fragment. From this, we gathered over a thousand strategies from a system called Vampire, each one fine-tuned to excel at solving a single problem. \n\nWith these strategies, it's quite straightforward to build a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to prevent the CASC from becoming a mere memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo build robust plans using the strategies we discovered, we suggested a method called the greedy schedule construction procedure. This method can hold its own against the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that an ideal plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the ideal plan is no longer practical, while the greedy method continues to produce relatively strong plans.\n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various balancing mechanisms, which are the main focus of this study. A carefully selected balancing mechanism allows us to outperform the ideal plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy plan creation method and the balancing techniques make them appealing for reuse and further exploration.",
              "rank": 15
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "In this project, we tested a strategy called Spider-style [33] to solve problems and make plans. We used a part of the TPTP library called the FOF fragment and gathered over a thousand different ways to solve problems using a tool called Vampire. Each of these ways was designed to work best on one specific problem. With these strategies, we can make a big plan that can solve most of the problems that we know can be solved within the time limit set by the CASC competition. This shows that for the CASC competition to be fair, it's important to use new problems each year that haven't been seen before. \n\nTo make strong plans using the strategies we found, we suggested a method that picks the best option available at each step, which we call a greedy schedule construction procedure. This method can compete with the best possible approaches. If we have about 2 minutes, this method takes less than a minute to make a plan that solves almost as many problems as the best possible plan, which takes over 16 hours to make. If we have less time, making the best possible plan isn't doable, but our method can still make pretty good plans. \n\nThe surprising power of our method can be made even stronger by using different techniques, which is the main thing we're showing in this project. If we pick the right technique, we can do better than the best possible plan on problems we haven't seen before. Lastly, our method is fast and simple, which makes it a good choice for use in other projects and for more testing.",
              "rank": 16
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "In this study, we tested a method called Spider-style to find and make plans. We used a part of the TPTP library called FOF and gathered over a thousand strategies from a tool called Vampire. Each strategy was made to work well on one problem. With these strategies, we can make a big plan that can solve most of the problems that can be solved within the time limit of the CASC competition. This shows that the CASC competition should use new problems each year, so it's not just about remembering old solutions. \n\nTo make good plans with the strategies we found, we suggested a method that quickly makes a plan. This method can compete with the best methods. If we have about 2 minutes, this quick method takes less than a minute to make a plan that solves almost as many problems as the best plan, which takes more than 16 hours to make. If we have less time, making the best plan is not possible, but our quick method still makes good plans. \n\nThe strength of our quick method can be made even better with some adjustments. These adjustments are the main part of our study. If we choose the right adjustment, we can do better than the best plan on new problems. Lastly, our quick method and the adjustments are easy to use and fast, which makes them good for using again and trying new things.",
              "rank": 17
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In this cosmic endeavor, we embarked on an autonomous exploration of the Spider-style [33] strategy discovery and schedule creation, akin to a solitary spacecraft traversing the vast expanse of the universe. Our focus was the FOF fragment of the TPTP library, a celestial body of knowledge, from which we harvested over a thousand Vampire proving strategies. Each of these strategies, like a star in the cosmos, was uniquely optimized to shine brightly on a single problem.\n\nWith these strategies in our cosmic toolkit, we found it straightforward to assemble a single, monolithic schedule, much like a constellation that encompasses most of the problems known to be solvable within the budgetary constraints of the CASC competition. This observation led us to the realization that the CASC competition, to avoid becoming a mere contest of memory, must incorporate a substantial set of fresh, uncharted problems each year.\n\nTo construct robust schedules using the unearthed strategies, we proposed a greedy schedule construction procedure, akin to a black hole, consuming all in its path, yet capable of competing with the most optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm, with the speed of a comet, takes less than a minute to produce a schedule that solves over 99.0% of the problems that an optimal schedule, which requires more than 16 hours to generate, can solve. As the time budget shrinks, optimal schedule construction becomes as unfeasible as traveling faster than light, while the greedy construction continues to produce relatively strong schedules.\n\nThe surprising potency of the greedy scheduler, much like the unexpected power of a supernova, can be further amplified by various regularization mechanisms. These mechanisms, like the gravitational forces that hold our universe together, form the core contribution of this work. An astutely selected regularization, like a well-placed telescope, allows us to outperform the optimal schedule on unseen problems.\n\nIn conclusion, the runtime speed and simplicity of the greedy schedule construction algorithm, along with the regularization techniques, are as attractive for reuse and further experimentation as the mysteries of the cosmos are to an astronomer.",
              "rank": 18
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12768.pdf",
      "paragraphs": [
        {
          "source": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
          "translations": [
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"To promote divergent thinking and customization, educators can create different scripts and use alternative prompts, broadening the potential to tailor content to specific learning objectives. When an item is selected in the Stickers panel, the associated prompt is displayed in the Refine panel. Educators can modify the prompt and create new stickers to suit the context, reducing the risk of inappropriate content. Additionally, using various themes and contexts, educators can make learning content more engaging, reinforcing vocabulary and promoting linguistic adaptability, as shown in Fig. 4. Providing a range of learning materials can encourage dynamic vocabulary engagement and strengthen learners’ language connections.\"",
              "rank": 1
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "Educators can encourage divergent thinking by generating variant scripts and using alternative prompts to customize learning content. They can edit prompts in the Stickers panel to better fit the context and avoid inappropriate content. By using varied themes, educators can create engaging content that reinforces vocabulary and promotes linguistic adaptability. This diverse array of learning materials can stimulate dynamic vocabulary engagement and deepen learners' connections with the language.",
              "rank": 2
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Educators can encourage divergent thinking and customization by generating variant scripts and using alternative prompts.\n- This expands possibilities for tailoring output content to specific learning objectives.\n- The current prompt associated with a selected item in the Stickers panel is displayed in the Refine panel.\n- Educators can edit the prompt and regenerate stickers to better fit the intended context and mitigate the risk of generating inappropriate content.\n- By using varied themes and contexts, educators can create engaging learning contents that reinforce vocabulary acquisition and promote linguistic adaptability.\n- The diverse array of learning materials can stimulate dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
              "rank": 3
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "Educators can encourage divergent thinking and customization by generating different scripts and using alternative prompts to tailor content to specific learning objectives. They can select an item in the Stickers panel, edit the prompt in the Refine panel, and regenerate stickers to better fit the context, while avoiding potentially inappropriate content. By using varied themes and contexts, educators can create engaging learning content that reinforces vocabulary acquisition and promotes linguistic adaptability. This diverse array of learning materials can stimulate dynamic vocabulary engagement and deepen learners' connections with the language.",
              "rank": 4
            },
            {
              "id": "source",
              "content": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
              "rank": 5
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "To promote creative thinking and allow for personalization, teachers can create different scripts and use various prompts, broadening the scope for customizing the content to meet specific educational goals. When a sticker is chosen from the Stickers panel, the current prompt linked to that sticker appears in the Refine panel. Teachers can modify this prompt and create new stickers to better suit the intended setting, while also reducing the chance of creating potentially unsuitable content. Furthermore, by using a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages language adaptability, as illustrated in Fig. 4. By providing a wide variety of learning resources, teachers can encourage more active vocabulary engagement and strengthen students' relationship with the language.",
              "rank": 6
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "To make learning more creative and personalized, teachers can create different scripts and use various prompts. This allows them to adjust the learning content to meet specific learning goals. When a teacher selects an item from the Stickers panel, the prompt linked to that sticker shows up in the Refine panel. Teachers can then modify this prompt and create new stickers that better suit the learning context, while also avoiding any inappropriate content. \n\nFurthermore, by using a range of themes and contexts, teachers can make learning more interesting. This not only helps students learn new words but also encourages them to adapt to different language uses, as shown in Fig. 4. By providing a wide variety of learning materials, teachers can make vocabulary learning more interactive and help students form a deeper connection with the language.",
              "rank": 7
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "To promote creative thinking and allow for personalization, teachers can create different scripts and use a variety of prompts. This will allow them to tailor the learning material to meet specific educational goals. When a teacher selects an item from the Stickers panel, the prompt related to that sticker appears in the Refine panel. Teachers can then modify this prompt and create new stickers that better suit the learning context, while also reducing the chance of creating inappropriate content. \n\nFurthermore, by using a range of themes and contexts, teachers can make the learning material more engaging. This not only strengthens vocabulary learning but also encourages students to adapt to different linguistic situations, as demonstrated in Figure 4. By providing a wide variety of learning resources, teachers can encourage more active engagement with vocabulary and strengthen students' relationship with the language.",
              "rank": 8
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "To boost creative thinking and allow for personalization, teachers can create different scripts and use various prompts. This increases the chances of adjusting the content to match specific learning goals. When a teacher selects an item in the Stickers panel, the current prompt linked to that sticker shows up in the Refine panel. Teachers can modify this prompt and create new stickers that better suit the intended context, while also reducing the chance of creating potentially unsuitable content. \n\nFurthermore, by using a range of themes and contexts, teachers can make learning more interesting. This not only strengthens vocabulary learning but also encourages flexibility in language use, as shown in Fig. 4. By providing a wide variety of learning materials, teachers can encourage more active vocabulary use and strengthen students' relationship with the language.",
              "rank": 9
            },
            {
              "id": "06-simplify-college.ts",
              "content": "To stimulate creative thinking and promote personalization, teachers can create diverse scripts and use different prompts, thereby broadening the scope for customizing the content to align with specific educational goals. When a sticker is chosen from the Stickers panel, the prompt linked to that sticker appears in the Refine panel. Teachers have the ability to modify this prompt and create new stickers that are more suitable for the intended context, while also reducing the chance of producing potentially unsuitable content. Furthermore, by utilizing a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages linguistic flexibility, as demonstrated in Figure 4. By providing a wide variety of learning resources, teachers can encourage more interactive vocabulary engagement and strengthen students' relationship with the language.",
              "rank": 10
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "To help students think creatively and personalize their learning, teachers can create different lesson plans and use a variety of questions. This allows them to adjust the lessons to meet specific learning goals. When a teacher chooses an item from the Stickers panel, the current question related to that sticker shows up in the Refine panel. Teachers can change this question and create new stickers to better suit the lesson, while also avoiding any potentially unsuitable content. Furthermore, by using different themes and situations, teachers can make learning more interesting. This not only helps students learn new words but also helps them adapt to different language uses, as shown in Fig. 4. By providing a wide range of learning materials, teachers can encourage students to engage more actively with vocabulary and strengthen their relationship with the language.",
              "rank": 11
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "To help students think creatively and make learning more personal, teachers can create different lesson plans and use different questions. This allows them to make the lessons fit the students' learning goals better. When a teacher picks an item from the Stickers panel, the question linked to that sticker shows up in the Refine panel. Teachers can change this question and make new stickers that better match the lesson, while also making sure they don't accidentally create content that isn't suitable. \n\nAlso, by using different themes and topics, teachers can make learning more fun. This not only helps students learn new words but also helps them get better at using the language in different situations, as shown in Fig. 4. By providing a wide range of learning materials, teachers can make vocabulary learning more active and help students feel more connected to the language.",
              "rank": 12
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "Teachers can make learning more fun and personal by using different methods and questions. This can help students learn in a way that suits them best. For example, when a teacher picks a sticker from the Stickers panel, a question related to that sticker shows up in the Refine panel. The teacher can change this question to make it better for the lesson. This also helps avoid any content that might not be suitable. \n\nTeachers can also use different topics and situations to make learning more interesting. This can help students learn new words and how to use them in different ways, as shown in Fig. 4. By using a variety of learning materials, teachers can make word learning more active and help students feel more connected to the language.",
              "rank": 13
            },
            {
              "id": "12-role-teacher.ts",
              "content": "As a renowned educator, I would like to share some strategies to inspire creative thinking and allow for personalization in learning. Teachers can create a variety of lesson plans and use different questions or tasks, broadening the scope for adapting the learning material to meet specific educational goals. \n\nImagine you're selecting an item from the 'Stickers' panel. The current task linked to the chosen sticker will appear in the 'Refine' panel. Teachers can modify this task and create new stickers that are more suitable for the intended learning environment. This also helps to avoid the creation of content that may not be suitable. \n\nFurthermore, by using a range of themes and contexts, teachers can develop more captivating learning materials. These materials not only strengthen vocabulary learning but also encourage flexibility in language use, as illustrated in Figure 4. By providing a wide variety of learning resources, teachers can encourage more active engagement with vocabulary and strengthen students' relationship with the language.",
              "rank": 14
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In a visual language like a flowchart:\n\n1. Start: Encourage divergent thinking and foster customization\n2. Action: Educators generate variant scripts and utilize alternative prompts\n3. Result: Expanding possibilities for tailoring output content to specific learning objectives\n4. Action: Select an item in the Stickers panel\n5. Result: The current prompt associated with the chosen sticker is displayed in the Refine panel\n6. Action: Educators edit the prompt and regenerate stickers\n7. Result: Better fit the intended context and mitigate the risk of generating potentially inappropriate content\n8. Action: Leverage varied themes and contexts\n9. Result: Create more engaging learning contents that reinforce vocabulary acquisition and promote linguistic adaptability (See Fig. 4)\n10. Action: Offer a diverse array of learning materials\n11. Result: Stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language\n12. End\n\nTranslation to English: \n\nStart by encouraging divergent thinking and fostering customization. Educators can then generate variant scripts and utilize alternative prompts, which expands the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can then edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content. Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
              "rank": 15
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the grand cosmos of education, the nurturing of divergent thought and the cultivation of customization can be achieved by educators who dare to venture into the creation of variant scripts and the employment of alternative prompts. This expands the universe of possibilities, allowing the tailoring of the output content to orbit around specific learning objectives. \n\nWhen an item is selected in the Stickers panel, the current prompt associated with the chosen sticker materializes in the Refine panel, much like a star emerging from the cosmic dust. Educators, acting as the architects of this educational cosmos, can modify the prompt and breathe new life into the stickers, ensuring they align better with the intended context. This process also serves as a protective shield, mitigating the risk of generating content that may stray into inappropriate territories.\n\nMoreover, by harnessing the power of varied themes and contexts, educators can craft more engaging learning content. This content not only strengthens the gravitational pull of vocabulary acquisition but also fosters linguistic adaptability, as illustrated in our own constellation, Fig. 4. \n\nBy providing a diverse galaxy of learning materials, educators can ignite more dynamic forms of vocabulary engagement, deepening learners' bonds with the language, much like the gravitational pull that keeps our planets in orbit. In this vast universe of learning, every word is a star, waiting to be discovered and understood.",
              "rank": 16
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In the spirit of Saul Kahn, we can say that the key to fostering a culture of divergent thinking and personalization in education lies in the ability to generate a variety of scripts and use different prompts. This approach broadens the scope for tailoring the content to meet specific learning goals. \n\nImagine a scenario where an educator selects an item from the Stickers panel. The prompt linked to the chosen sticker then appears in the Refine panel. The educator has the freedom to modify this prompt and create new stickers that are more in line with the intended context. This process not only reduces the chances of producing unsuitable content but also adds a layer of customization.\n\nFurthermore, educators can harness the power of diverse themes and contexts to craft more captivating learning materials. These materials do more than just bolster vocabulary acquisition. They also encourage linguistic flexibility, as illustrated in Fig. 4. \n\nBy providing a wide range of learning resources, educators can ignite more vibrant forms of vocabulary interaction and strengthen the bond learners have with the language. This approach, in essence, deepens the learning experience and makes it more meaningful.",
              "rank": 17
            }
          ]
        }
      ]
    }
  ]
}
