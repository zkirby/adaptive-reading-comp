{
  "papers": [
    {
      "id": "https://arxiv.org/pdf/2403.12122.pdf",
      "paragraphs": [
        {
          "source": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
          "translations": [
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine a spinning disk of primordial planetesimals, the building blocks of planets. The way this disk disperses influences the orbital evolution of the giant planets in our solar system. Let's take a look at Table 3, where we've noted some important factors in the final layouts of these giant planets after 4 billion years of evolution.\n\nIn the first column, we've noted the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48. However, in our simulated systems, we've seen a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this primarily impacts the inner solar system, not the Kuiper belt.\n\nNow, the structure of the Kuiper belt is influenced by the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97 to 2.31, with an average of 2.13.\n\nWe've also looked at a large number of simulations from previous studies, and found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain, or simply a matter of chance among our 12 simulated systems.\n\nHowever, it's important to note that if Uranus and Neptune's orbits cross at a 2:1 ratio, it can destabilize much of the Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a desirable feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 1
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "The movement of an original planet disk can cause changes in the orbits of other giant planets. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of changes. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1–2.7, with an average value of 2.39. While the exact ratio can affect the locations of various resonances, this mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the ratio of Neptune’s to Uranus’ orbital period, which we've listed in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these initial systems ranged from 1.97– 2.31, with an average of 2.13. Looking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of under 2. It's unclear whether this is a problem with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations meant to study Kuiper belt formation.",
              "rank": 2
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The movement of an original planet-like disk influences the development of other giant planets' orbits. In Table 3, we've noted important factors in the final structures of these giant planets after 4 billion years of development. The first column shows the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is about 2.48, but our simulated systems show a range of ratios from 2.1 to 2.7, with an average of 2.39. While this exact ratio affects the locations of various resonances, it mainly impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We've listed these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. These original systems had ratios ranging from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we found that about half of all 4-planet instability outcomes ended with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. \n\nHowever, the crossing of Uranus and Neptune's 2:1 resonance can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations meant to study the formation of the Kuiper belt.",
              "rank": 3
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. Key parameters in the final architectures of these planets after 4 Gyrs of evolution include the ratio of Saturn’s to Jupiter’s orbital period and the ratio of Neptune’s to Uranus’ orbital period. The former has dynamical consequences primarily for the inner solar system, while the latter affects the Kuiper belt’s architecture. In simulations, the Saturn to Jupiter period ratio ranged from 2.1–2.7, with a median of 2.39. The Neptune to Uranus period ratio in original systems ranged from 1.97– 2.31, with a median of 2.13. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize the resonant Kuiper belt population, limiting the utility of simulation results.",
              "rank": 4
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets. The study lists key parameters in the final architectures of these planets after 4 billion years of evolution. The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48, but the simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39. This ratio primarily affects the inner solar system rather than the Kuiper belt. However, the Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period. In the simulations, all but one system evolved above a period ratio of 2. The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.",
              "rank": 5
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- The dispersal of a primordial planetesimal disk influences the orbital evolution of giant planets.\n- Key parameters of the final architectures of the giant planets after 4 Gyrs of evolution are listed in Table 3.\n- The ratio of Saturn’s to Jupiter’s orbital period in the real solar system is approximately 2.48.\n- The simulated systems have a range of ratios from 2.1–2.7, with a median value of 2.39.\n- The exact ratio has dynamical consequences primarily for the inner solar system, not the Kuiper belt.\n- The Kuiper belt’s architecture depends on the ratio of Neptune’s to Uranus’ orbital period.\n- In the original simulations, all but one system evolved above a period ratio of 2, with values ranging from 1.97– 2.31 and a median of 2.13.\n- About half of all 4-planet instability outcomes from prior works finish with a Neptune-to-Uranus period ratio of under 2.\n- The crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of the simulation results.\n- This crossing is a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 6
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of the other giant planets too. In Table 3, we provide important parameters in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn’s orbital period to Jupiter’s. In the actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. Although the positions of various secular resonances depend on this exact ratio, this mainly affects the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune’s orbital period to Uranus’. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, the crossing of Uranus and Neptune’s 2:1 mean motion resonance (MMR) can destabilize much of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 7
            },
            {
              "id": "06-simplify-college.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital evolution of other giant planets. Table 3 outlines crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems display a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the exact ratio impacts the locations of various secular resonances, this primarily affects the inner solar system rather than the Kuiper belt. \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. The second column of Table 3 presents these final ratios for our systems. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon reviewing numerous simulations from previous studies, we found that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's uncertain whether this is a systematic issue with our specific 5-planet resonant chain or simply an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 mean motion resonance (MMR) can destabilize a significant portion of the resonant Kuiper belt population. This limits the usefulness of our simulation results and is therefore an undesirable feature for simulations intended to study Kuiper belt formation.",
              "rank": 8
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "The scattering of an original planetesimal disk instigates orbital evolution among the remaining giant planets. Table 3 outlines the crucial parameters in the final structures of these planets after 4 billion years of evolution. The first column of the table presents the ratio of Saturn's orbital period to Jupiter's. In our actual solar system, this ratio is approximately 2.48. However, our final simulated systems exhibit a range of ratios from 2.1 to 2.7, with a median value of 2.39. While the precise locations of various secular resonances hinge on this exact ratio, the dynamical implications are primarily for the inner solar system, not the Kuiper belt (see Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020). \n\nOn the other hand, the structure of the Kuiper belt relies on the ratio of Neptune's orbital period to Uranus'. We present these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The values in these original systems varied from 1.97 to 2.31, with a median of 2.13. Upon reviewing a significant number of approximately 1000-particle simulations from previous studies (Clement et al. 2018, 2021c,a), we discovered that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains uncertain whether this is an unobserved systematic issue with our specific 5-planet resonant chain or merely an unfortunate coincidence among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize a large portion of the resonant Kuiper belt population, thereby limiting the usefulness of our simulation results. Consequently, it is an undesirable feature for simulations intended to investigate Kuiper belt formation (Graham & Volk 2024).",
              "rank": 9
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"The scattering of an initial planetesimal disk influences the orbital evolution of other giant planets. Table 3 shows key parameters in the final structures of these planets after 4 Gyrs of evolution. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In the actual solar system, this ratio is about 2.48, but our simulated systems range from 2.1–2.7, with a median of 2.39. This ratio mainly affects the inner solar system's dynamics rather than the Kuiper belt. However, the Kuiper belt’s structure relies on the ratio of Neptune’s to Uranus’ orbital period, listed in Table 3's second column. In our initial simulations, all but one system (2500Pb) had a period ratio above 2. These original systems ranged from 1.97– 2.31, with a median of 2.13. Reviewing numerous ∼1000-particle simulations from previous studies, we find about half of all 4-planet instability results end with a Neptune-to-Uranus period ratio below 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. However, Uranus and Neptune’s 2:1 MMR crossing can destabilize much of the resonant Kuiper belt population, limiting our simulation results' usefulness. Therefore, it's not ideal for simulations studying Kuiper belt formation.\"",
              "rank": 10
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "The scattering of an original planetesimal disk influences the orbital changes of other giant planets too. Table 3 outlines the main factors in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn's to Jupiter's orbital period. In our actual solar system, this ratio is approximately 2.48, but our final simulated systems show a range of ratios from 2.1 to 2.7, with an average value of 2.39. While the exact ratio affects the locations of various secular resonances, this primarily impacts the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune's to Uranus' orbital period. We provide these final ratios for our systems in the second column of Table 3. In our initial simulations, all but one system (2500Pb) evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with an average of 2.13. \n\nLooking at a large number of simulations from previous studies, we find that about half of all 4-planet instability outcomes end with a Neptune-to-Uranus period ratio of less than 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or just bad luck among our 12 simulated systems. Regardless, the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. Therefore, it's not a good feature for simulations intended to study Kuiper belt formation.",
              "rank": 11
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to guide you in understanding the following passage:\n\nThe movement of an original planetesimal disk influences the orbital evolution of the giant planets. In Table 3, we present crucial parameters in the final structures of the giant planets after 4 billion years of evolution. The first column shows the ratio of Saturn’s to Jupiter’s orbital period. In our solar system, this ratio is approximately 2.48, but our simulated systems show a range of ratios from 2.1–2.7, with a median value of 2.39. While the exact ratio impacts the locations of various secular resonances, this primarily affects the inner solar system rather than the Kuiper belt. \n\nHowever, the structure of the Kuiper belt relies on the ratio of Neptune’s to Uranus’ orbital period, which we present in the second column of Table 3. In our initial simulations, all but one system evolved above a period ratio of 2. The ratios in these original systems varied from 1.97– 2.31, with a median of 2.13. After reviewing numerous simulations from previous studies, we found that about half of all 4-planet instability outcomes ended with a Neptune-to-Uranus period ratio of under 2. It's unclear whether this is a systematic issue with our specific 5-planet resonant chain or simply an unfortunate outcome among our 12 simulated systems. However, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results, and is therefore not ideal for simulations intended to study Kuiper belt formation.",
              "rank": 12
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "When a planet breaks apart, it can change the paths of other big planets too. In Table 3, we show important details about how the big planets changed after 4 billion years. In the first column, we show how long it takes Saturn to orbit the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times longer, with an average of 2.39 times. This can affect the inner solar system more than the outer part, where the Kuiper belt is.\n\nHowever, the structure of the Kuiper belt depends on how long it takes Neptune to orbit the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times longer, with an average of 2.13 times. \n\nLooking at other computer models with about 1000 particles, we found that about half of all 4-planet systems ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our 5-planet model or just bad luck in our 12 models. \n\nBut when Neptune and Uranus cross each other's paths, it can mess up a lot of the Kuiper belt. This makes our models less useful for studying how the Kuiper belt was formed.",
              "rank": 13
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In the cosmic ballet of the planets, the scattering of an ancient disk of planetesimals, the building blocks of planets, influences the orbital dance of the gas giants as well. In the cosmic ledger we call Table 3, we've noted the defining characteristics of the final arrangements of these gas giants after 4 billion years of cosmic evolution. \n\nFirstly, we've noted the rhythm of their dance - the ratio of Saturn's orbital period to Jupiter's. In our own cosmic neighborhood, this ratio is approximately 2.48. However, in our digital cosmos, the ratios vary, ranging from 2.1 to 2.7, with a median value of 2.39. While the exact ratio influences the locations of various gravitational harmonies, its impact is felt primarily in the inner solar system, rather than the distant Kuiper belt.\n\nHowever, the architectural blueprint of the Kuiper belt is influenced by the rhythm of Neptune's dance with Uranus, and we've noted these final ratios in the second column of our cosmic ledger. In our initial digital cosmos, all but one system evolved above a period ratio of 2. The ratios in these initial systems varied from 1.97 to 2.31, with a median of 2.13. \n\nUpon examining a multitude of simulations from previous works, we find that about half of all 4-planet instability outcomes conclude with a Neptune-to-Uranus period ratio of less than 2. It remains unclear whether this is a systematic issue with our specific 5-planet resonant chain or simply a stroke of cosmic misfortune among our 12 simulated systems. \n\nRegardless, the crossing of Uranus and Neptune's 2:1 gravitational harmony can destabilize much of the resonant Kuiper belt population, limiting the usefulness of our simulation results. This makes it a less than ideal feature for simulations intended to study the birth of the Kuiper belt.",
              "rank": 14
            },
            {
              "id": "source",
              "content": "The dispersal of a primordial planetesimal disk drives orbital evolution among the rest of the giant planets as well. In Table 3, we list key parameters in the final architectures of the giant planets after 4 Gyrs of evolution. In the first column we list the ratio of Saturn’s to Jupiter’s orbital period. In the real solar system, this ratio is ∼2.48, but our final simulated systems have a range of ratios spanning from 2.1–2.7, with a median value of 2.39. Although the locations of various secular resonances depend on this exact ratio, this has dynamical consequences primarily for the inner solar system rather than the Kuiper belt (e.g. Brasser et al. 2009; Walsh & Morbidelli 2011; Clement et al. 2020).  However, the Kuiper belt’s architecture is dependent upon the ratio of Neptune’s to Uranus’ orbital period, and we list these final ratios for our systems in the second column of Table 3. In our original simulations, all but one system (2500Pb) evolved above a period ratio of 2. Values in these original systems ranged from 1.97– 2.31, with a median of 2.13. Consulting a large number of ∼1000-particle simulations from prior works (Clement et al. 2018, 2021c,a), we find roughly half of all 4-planet instability outcomes finish with a Neptune-to-Uranus period ratio of under 2, so it is not clear whether this is an unrealized systematic issue with our particular 5-planet resonant chain or whether it is simply an issue of bad luck among our 12 simulated systems. Nevertheless, the crossing of Uranus and Neptune’s 2:1 MMR can destabilize much of the resonant Kuiper belt population, limiting the utility of our simulation results, and is therefore a poor feature for simulations meant to study Kuiper belt formation (Graham & Volk 2024).",
              "rank": 15
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "The movement of an early group of small planets can affect the paths of larger planets too. In Table 3, we've noted important details about the final positions of the large planets after 4 billion years. The first column shows the comparison between the time it takes Saturn and Jupiter to orbit the sun. In our actual solar system, Saturn takes about 2.48 times longer than Jupiter, but in our computer models, this time varied between 2.1 and 2.7 times, with an average of 2.39 times. This difference can affect the inner solar system more than the outer region known as the Kuiper belt. \n\nHowever, the structure of the Kuiper belt is influenced by the comparison between the time it takes Neptune and Uranus to orbit the sun. We've noted these comparisons for our models in the second column of Table 3. In our initial models, all but one showed Neptune taking more than twice as long as Uranus. The times varied between 1.97 and 2.31 times, with an average of 2.13 times. Looking at many previous models, we found that about half ended with Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our specific model or just bad luck in our 12 models. However, if Neptune and Uranus cross paths at a 2:1 ratio, it can disrupt much of the Kuiper belt, which makes our model results less useful. This is not a good feature for models meant to study the formation of the Kuiper belt.",
              "rank": 16
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "A group of early planets can change the paths of other big planets. In Table 3, we show important details about the final paths of the big planets after 4 billion years. The first column shows how long it takes Saturn to go around the sun compared to Jupiter. In our real solar system, Saturn takes about 2.48 times longer than Jupiter. But in our computer models, this time can be anywhere from 2.1 to 2.7 times, with an average of 2.39 times. This difference mainly affects the inner solar system, not the outer area called the Kuiper belt.\n\nHowever, the Kuiper belt is affected by how long Neptune takes to go around the sun compared to Uranus. We show these times in the second column of Table 3. In our computer models, all but one system had Neptune taking more than twice as long as Uranus. The times ranged from 1.97 to 2.31 times, with an average of 2.13 times. \n\nLooking at many other computer models, we found that about half of them had Neptune taking less than twice as long as Uranus. We're not sure if this is a problem with our model or just bad luck in our 12 systems. But if Neptune and Uranus get too close, it can mess up the Kuiper belt. This makes our model less useful for studying how the Kuiper belt was formed.",
              "rank": 17
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In the language of scientific diagrams, the paragraph could be represented as follows:\n\n[Diagram 1: A timeline showing 4 Gyrs of evolution, with a primordial planetesimal disk at the start and the final architectures of the giant planets at the end.]\n\n[Table 3: A table with two columns. The first column is labeled \"Ratio of Saturn's to Jupiter's orbital period\" and lists a range of ratios from 2.1 to 2.7, with a median value of 2.39. The second column is labeled \"Ratio of Neptune's to Uranus' orbital period\" and lists a range of ratios from 1.97 to 2.31, with a median value of 2.13.]\n\n[Diagram 2: A graph showing the locations of various secular resonances, with a note indicating that these have dynamical consequences primarily for the inner solar system.]\n\n[Diagram 3: A graph showing the architecture of the Kuiper belt, with a note indicating that this is dependent on the ratio of Neptune's to Uranus' orbital period.]\n\n[Diagram 4: A pie chart showing the outcomes of ~1000-particle simulations, with roughly half finishing with a Neptune-to-Uranus period ratio of under 2.]\n\n[Diagram 5: A graph showing the crossing of Uranus and Neptune's 2:1 MMR, with a note indicating that this can destabilize much of the resonant Kuiper belt population.]\n\n[Text Box: A note stating that the crossing of Uranus and Neptune's 2:1 MMR is a poor feature for simulations meant to study Kuiper belt formation.]\n\nTranslation to English: The timeline, table, and diagrams represent the evolution of a primordial planetesimal disk and the final architectures of the giant planets after 4 Gyrs. The table lists key parameters, including the ratios of Saturn's to Jupiter's orbital period and Neptune's to Uranus' orbital period. The diagrams show the locations of various secular resonances, the architecture of the Kuiper belt, the outcomes of ~1000-particle simulations, and the crossing of Uranus and Neptune's 2:1 MMR. The text box notes that the crossing of Uranus and Neptune's 2:1 MMR can destabilize much of the resonant Kuiper belt population and is therefore a poor feature for simulations meant to study Kuiper belt formation.",
              "rank": 18
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12183.pdf",
      "paragraphs": [
        {
          "source": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
          "translations": [
            {
              "id": "04-demographics-author-reader.ts",
              "content": "We're looking at how stable matchings, or pairings, hold up under various changes. These changes could be small or large and can happen in different settings. Over time, people's preferences might change, the makeup of the market might shift, or paired participants might decide to end their partnerships. For example, in job markets, a change in a worker's family situation might cause them to prefer a different location; new jobs might open up and workers might become available or retire; an employer and an employee might mutually decide to end their working relationship. While these changes might also affect the set of stable matchings, they could all lead to instability. Specifically, some participants that should be paired under the current market conditions might be mismatched or not matched at all. In some cases, certain partnerships can't be broken up by decentralized interactions. \n\nLet's imagine a market split into two submarkets, New York and Los Angeles, where everyone in LA prefers any LA resident to any New Yorker, and vice versa. Let's say that the New York submarket has one stable matching and the LA submarket has two. The entire market naturally has two stable matchings as well. If all LA residents are paired according to one of the two stable matchings for the LA submarket, and therefore for the entire market, decentralized interactions can't unmatch any such pair of matched LA residents. As a result, the stabilization dynamics can't reach another stable matching in the entire market. In this case, LA residents can be matched in a stable way within the group so that every insider prefers their stable partner to anyone outside LA. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In this case, LA residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings can't lead to certain stable matchings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to reach any stable matching. \n\nTheorem 1 suggests one type of fragility of stable matchings. A small change in a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any change in such a clearinghouse’s outcome doesn't have to revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
              "rank": 1
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "We examine the resilience of stable pairings when faced with any kind of disruption, even minor ones. These disruptions can occur in both individual and collective settings. Over time, the preferences of participants may change, the makeup of the market may shift, or existing partnerships may dissolve. For example, in job markets, a personal event may cause a worker to prefer a different location; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might be mismatched or not matched at all. In some cases, individual interactions cannot break certain partnerships. \n\nConsider a market split into two submarkets, New York and Los Angeles, where everyone in each city prefers someone from their own city over someone from the other city. If the New York submarket has one stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for their submarket, and therefore for the entire market, individual interactions cannot unpair any such Los Angeles pair. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within their group so that everyone prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable pairings in the entire market pair participants within the fragment in the same way. Specifically, Los Angeles residents make up a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable pairings cannot lead to certain stable pairings. Theorem 1 shows that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can lead to any stable pairing through individual interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The lack of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the individual process is flexible enough to achieve any stable pairing. \n\nTheorem 1 suggests one type of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extreme stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse’s outcome does not necessarily have to revert back to the intended extreme stable pairing; individual interactions can instead lead to the other extreme stable pairing, or anything in between.",
              "rank": 2
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Let's consider the concept of stable matchings and their resilience to changes, which could be minor or significant. These changes can occur in both individual and collective scenarios. Over time, the preferences of the participants may change, the composition of the market may shift, or the matched participants might decide to part ways. For example, in job markets, a change in a worker's family situation might lead to new location preferences; new job roles may emerge and workers may either become available or retire; an employer and an employee might mutually agree to end their work relationship. While these changes might also affect the set of stable matchings, they could all potentially lead to instability. Specifically, some participants who should be paired under the current market conditions might end up being mismatched or not matched at all. In some cases, individual interactions cannot break certain partnerships. \n\nLet's imagine a market divided into two submarkets, New York and Los Angeles, where every person from Los Angeles prefers any other person from Los Angeles over any New Yorker, and vice versa. Let's say that the New York submarket has one unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all people from Los Angeles are paired according to one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, individual interactions cannot unpair any such pair of matched people from Los Angeles. As a result, the dynamics of stabilization cannot achieve another stable matching in the entire market. In this scenario, people from Los Angeles can be matched stably within the group so that every person prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers form a fragment as well. A fragment is considered trivial if all stable matchings in the entire market match participants within the fragment in the same way. Specifically, people from Los Angeles constitute a non-trivial fragment, while New Yorkers form a trivial fragment. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot lead to certain stable matchings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable matching can lead to any stable matching through individual interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the individual process is flexible enough to achieve any stable matching. \n\nTheorem 1 suggests one type of vulnerability of stable matchings. A minor change in a stable matching may result in any stable matching, whether it's close to or far from the original stable matching. The famous Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extreme stable matching, the best stable matching for one side of the market, doctors and students, respectively. Our findings imply that any change in such a clearinghouse’s outcome does not necessarily have to revert back to the intended extreme stable matching; individual interactions can instead lead to the other extreme stable matching, or anything in between.",
              "rank": 3
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study examines the stability of matchings in response to potential perturbations, such as changes in participants' preferences or market composition. These changes could lead to instability, with participants being mismatched or unmatched. The study introduces the concept of a 'fragment', a group where members prefer to be matched within the group. Fragments can be 'trivial' or 'non-trivial', with non-trivial fragments being the only restraints on stabilization dynamics. The absence of non-trivial fragments suggests that most markets can attain any stable matching. The study also suggests that a small perturbation of a stable matching could result in any stable matching, and that decentralized interactions can lead to different outcomes than intended.",
              "rank": 4
            },
            {
              "id": "06-simplify-college.ts",
              "content": "We examine the resilience of stable pairings in the face of arbitrary, potentially minor, disruptions. These disruptions can occur in both decentralized and centralized environments. Over time, changes in participants' preferences, market composition, or the dissolution of matched partnerships can occur. For example, in job markets, a change in a worker's family situation may lead to new geographical preferences; new job openings may arise and workers may become available or retire; an employer and employee may mutually agree to end their working relationship. While these changes may also affect the set of stable pairings, they could all potentially lead to instability. Specifically, some participants who should be matched under current market conditions may be mismatched or not matched at all. In some cases, decentralized interactions cannot dissolve certain partnerships. Consider a two-sided market split into two submarkets, New York and Los Angeles, where every resident of each city prefers any fellow resident to any resident of the other city. If the New York submarket has one stable pairing and the Los Angeles submarket has two, the entire market naturally has two stable pairings as well. If all Los Angeles residents are paired according to one of the two stable pairings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unpair any such pair of matched Los Angeles residents. As a result, the dynamics of stabilization cannot achieve another stable pairing in the entire market. In this scenario, Los Angeles residents can be stably paired within the group so that every member prefers their stable partner to anyone outside Los Angeles. We refer to such a group as a fragment. In this example, New Yorkers also form a fragment. A fragment is considered trivial if all stable pairings in the entire market match participants within the fragment in the same way. Specifically, Los Angeles residents constitute a non-trivial fragment, while New Yorkers form a trivial fragment. If a market has a non-trivial fragment, some unstable pairings cannot result in certain stable pairings. Theorem 1 demonstrates that the opposite is also true. When there are no nontrivial fragments, any unstable pairing can result in any stable pairing through decentralized interactions. Non-trivial fragments are the only limitations on the dynamics of stabilization. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Therefore, in most markets, the decentralized process is flexible enough to achieve any stable pairing. Theorem 1 suggests one form of vulnerability of stable pairings. A minor disruption of a stable pairing may result in any stable pairing, whether similar to or different from the original stable pairing. The renowned Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims to implement an extremal stable pairing, the best stable pairing for one side of the market, doctors and students, respectively. Our findings imply that any disruption of such a clearinghouse's outcome does not necessarily have to revert back to the intended extremal stable pairing; decentralized interactions can instead lead to the other extremal stable pairing, or anything in between.",
              "rank": 5
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"We examine the stability of matchings under any perturbations, which can occur in both decentralized and centralized settings. Changes such as shifts in participants' preferences, alterations in market composition, or termination of partnerships can affect the set of stable matchings, potentially leading to instability. Some participants may be mismatched or unmatched under the current market conditions. In certain situations, decentralized interactions cannot dissolve certain partnerships. \n\nConsider a two-sided market split into two submarkets, New York and Los Angeles, where every resident prefers any local to any outsider. If the New York submarket has one stable matching and the Los Angeles submarket has two, the entire market naturally has two stable matchings. If all Los Angeles residents are paired according to one of the two stable matchings for their submarket, decentralized interactions cannot unpair any such matched pair. Consequently, the stabilization dynamics cannot achieve another stable matching in the entire market. \n\nIn this scenario, Los Angeles residents can be stably matched within the group, with every member preferring their stable partner to anyone outside Los Angeles. We term such a group a fragment. New Yorkers also form a fragment. A fragment is trivial if all stable matchings in the entire market match participants within the fragment in the same way. Los Angeles residents form a non-trivial fragment, while New Yorkers form a trivial one. \n\nIf a market has a non-trivial fragment, some unstable matchings cannot produce certain stable matchings. Theorem 1 shows the reverse is also true. Without nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only limitations on the stabilization dynamics. Their absence is a mild condition, as simulations suggest they are rare in large random markets. Therefore, in most markets, the decentralized process can achieve any stable matching. \n\nTheorem 1 indicates one type of fragility of stable matchings. A minor perturbation of a stable matching may result in any stable matching, near or far from the original one. The renowned Deferred Acceptance mechanism, used to match doctors to hospitals and students to schools, aims to implement an extremal stable matching, the best one for one market side. Our results suggest that any perturbation of such a clearinghouse’s outcome doesn't have to revert back to the intended extremal stable matching; decentralized interactions can lead to the other extremal stable matching, or anything in between.\"",
              "rank": 6
            },
            {
              "id": "source",
              "content": "We study the robustness of stable matchings with respect to arbitrary, possibly minimal, perturbations. Such perturbations may arise in both decentralized and centralized settings.  Over time, participants’ preferences may shift, market composition may change, or matched participants might end their partnerships. For instance, in labor markets, a family shock may generate new geographical preferences for workers; new positions may appear and workers may either become available or retire; an employer and an employee might mutually agree to terminate the employment relationship. While the set of stable matchings may also be altered by such changes, they could all lead to instability. In particular, some participants that should be paired under current market conditions might be mismatched or unmatched. In some situations, decentralized interactions cannot break certain partnerships. Imagine a two-sided market divided into two submarkets, New York and Los Angeles, in which every Angeleno prefers any Angeleno to any New Yorker, and vice versa. Suppose that the New York submarket has a unique stable matching and the Los Angeles submarket has two stable matchings. The entire market naturally has two stable matchings as well. If all Angelenos are paired in accordance with one of the two stable matchings for the Los Angeles submarket, and therefore for the entire market, decentralized interactions cannot unmatch any such pair of matched Angelenos. As a result, the stabilization dynamics cannot attain another stable matching in the entire market. Here, Angelenos can be matched in a stable way inside the group so that every insider prefers her stable partner to anyone outside Los Angeles. We call such a group a fragment. In this example, New Yorkers form a fragment as well. A fragment is trivial if all stable matchings in the entire market match participants inside the fragment in the same way. In particular, Angelenos constitute a non-trivial fragment, while New Yorkers form a trivial fragment.  If a market has a non-trivial fragment, some unstable matchings cannot yield certain stable matchings. Theorem 1 shows that the reverse is also true. When there are no nontrivial fragments, any unstable matching can yield any stable matching through decentralized interactions. Non-trivial fragments are the only restraints on the stabilization dynamics. The absence of non-trivial fragments is a mild condition: simulations suggest that they are rare in large random markets. Thus, in most markets, the decentralized process is fluid enough to attain any stable matching.  Theorem 1 suggests one type of fragility of stable matchings. A small perturbation of a stable matching may culminate in any stable matching, close to or distant from the original stable matching. The celebrated Deferred Acceptance mechanism of Gale and Shapley (1962), which is used to match doctors to hospitals and students to schools, aims at implementing an extremal stable matching, the best stable matching for one market side, doctors and students, respectively. Our results imply that any perturbation of such a clearinghouse’s outcome need not revert back to the intended extremal stable matching; decentralized interactions can instead lead to the other extremal stable matching, or anything in between.",
              "rank": 7
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12147.pdf",
      "paragraphs": [
        {
          "source": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]",
          "translations": [
            {
              "id": "04-demographics-author-reader.ts",
              "content": "The author is discussing the movement of electrons within a crystal lattice, which is made up of ions with opposite charges. When these electrons move, they cause distortions in the lattice, known as phonons. These phonons then interact with the electrons, causing each electron to be surrounded by a cloud of phonons, which reduces its mobility. This combination of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich proposed a model in 1954 to explain the dynamics of multi-polarons. In this model, the electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom, while the phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the introduction of Fröhlich's model, functional integration has been a key technique in the study of polaron models, both in theoretical physics and mathematics. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory, and focuses on deriving Feynman–Kac formulas in Fröhlich's multipolaron model.\n\nThe author also discusses the interaction of electrons via repulsive Coulomb potentials and the treatment of polarons in external electric and magnetic fields. The author notes that in some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, the author works under general assumptions about the interaction between electrons and phonons.\n\nThe author concludes by stating that the Feynman–Kac formulas established in this article can form the basis for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 1
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "Electrons moving within a crystal lattice, made up of ions with opposite charges, cause distortions in the lattice, known as phonons. These phonons, in turn, affect the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. This combined entity of an electron and its surrounding phonons is referred to as a polaron. When multiple electrons are involved, the term multi-polarons is used.\n\nH. Fröhlich introduced a mathematical model in 1954 to explain the behavior of multi-polarons. In this model, electrons are considered as non-relativistic quantum mechanical particles without spin, while phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nThe study of polaron models has primarily involved functional integration, a technique first used by Feynman in 1955. This method has been widely used in both theoretical physics and mathematics. This article builds on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory. We focus on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space.\n\nElectrons interact through repulsive Coulomb potentials and polarons are often exposed to external electric and magnetic fields. We will work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential. In some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. We will work under general assumptions on the electron-phonon interaction covering these situations as well as the original Fröhlich model.\n\nOur Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 2
            },
            {
              "id": "12-role-teacher.ts",
              "content": "Imagine electrons moving within a crystal lattice, a structure made up of oppositely charged ions. As they move, they cause distortions in the lattice, creating what we call phonons. These phonons, in turn, affect the electrons due to the polarization they carry. This interaction results in each electron being surrounded by a cloud of phonons, which reduces its mobility. We refer to this electron-phonon combination as a polaron. When we consider multiple electrons, we talk about multi-polarons.\n\nIn 1954, H. Fröhlich introduced a model to explain the dynamics of these multi-polarons. In his model, electrons are considered as non-relativistic quantum mechanical particles without spin degrees of freedom. The phonons, which can be created and destroyed over time, are described by a non-relativistic bosonic quantum field.\n\nSince the pioneering work of Feynman in 1955, functional integration has been a key technique in studying polaron models, both in theoretical physics and mathematics. We will soon provide numerous references to mathematical papers that use various Feynman–Kac formulas to calculate the vacuum expectation values of members of the semigroup generated by Fröhlich’s model.\n\nIn this article, we will focus on deriving Feynman–Kac formulas for Fröhlich’s multipolaron model. We will consider semigroup members applied to any vectors in the underlying Hilbert space. We will work under optimal conditions for the electrostatic potential and the magnetic vector potential, which are important as electrons interact via repulsive Coulomb potentials and polarons are often exposed to external electric and magnetic fields.\n\nIn some studies, the electrons are confined to open regions of Euclidean space, and sometimes both the electrons and the phonons are confined. Therefore, we will work under general assumptions about the electron-phonon interaction that cover these situations as well as the original Fröhlich model.\n\nWith the inequalities established in this article, our Feynman–Kac formulas can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models. This is similar to the theory of magnetic Schrödinger semigroups and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics and Nelson’s model for nucleonmeson interactions.",
              "rank": 3
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"Electrons moving in a crystal lattice of oppositely charged ions create lattice distortions, or phonons, which affect the electrons through their polarization. This results in each electron being surrounded by a phonon cloud, reducing its mobility. This composite object is known as a polaron; when multiple electrons are involved, we refer to multi-polarons. H. Fröhlich introduced a Hamiltonian in [Frö54] to govern multi-polaron dynamics. In his model, electrons are non-relativistic quantum mechanical particles without spin degrees of freedom, while phonons, which can be created and annihilated over time, are represented by a non-relativistic bosonic quantum field. Functional integration has been a key technique in polaron model investigation since Feynman's work [Fey55]. We will soon reference mathematical papers that use various Feynman–Kac formulas for vacuum expectation values of semigroup members generated by Fröhlich’s Hamiltonian. This article focuses on deriving Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to any vectors in the underlying Hilbert space. We will work under optimal conditions on the electrostatic potential and the magnetic vector potential, allowing us to define the Hamiltonian through semibounded quadratic forms. In some studies, electrons are confined to open regions of Euclidean space [AL13, FLST11], and sometimes both electrons and phonons are confined [FS21, BM23]. Therefore, we will work under general assumptions on the electron-phonon interaction, covering these situations and the original Fröhlich model. Our Feynman–Kac formulas, along with the inequalities established in this article, can serve as a foundation for further studies of the semigroup and ground state eigenvectors in polaron models, similar to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22].\"",
              "rank": 4
            },
            {
              "id": "source",
              "content": "When electrons move in a crystal lattice comprised of oppositely charged ions they create lattice distortions (phonons) in their neighbourhoods, which back-react on the electrons via the polarization they carry. This results in each electron being accompanied by a cloud of phonons lowering its mobility. Such a composite object is called a polaron; when several electrons are considered we speak of multi-polarons. In [Frö54], H. Fröhlich introduced a Hamiltonian governing the dynamics of multi-polarons. In his model the electrons are treated as non-relativistic quantum mechanical particles without spin degrees of freedom whereas the phonons, which can be created and annihilated along the time evolution, are described by a non-relativistic bosonic quantum field.  Starting with the seminal work of Feynman [Fey55], one main technique in the investigation of polaron models has been functional integration, both in theoretical physics and mathematics. Shortly, in Section 1.3, we shall give numerous references to mathematical papers exploiting various Feynman–Kac formulas for vacuum expectation values of members of the semigroup generated by Fröhlich’s Hamiltonian.  Building on recent mathematical studies of Feynman–Kac formulas in non- and semi-relativistic quantum field theory [GMM17, MM18, Mat21, HM23a], we devote this article to the derivation of Feynman–Kac formulas in Fröhlich’s multipolaron model for semigroup members applied to arbitrary vectors in the underlying Hilbert space. Since electrons interact via repulsive Coulomb potentials and polarons exposed to external electric and magnetic fields are often treated – see [AG14, Gha21, GW13, Löw88] for mathematical results on polarons in magnetic fields – we shall in fact work under almost optimal conditions on the electrostatic potential and optimal conditions on the magnetic vector potential still permitting to define the Hamiltonian via semibounded quadratic forms. In some articles, the electrons are confined to open regions of Euclidean space [AL13, FLST11], for technical reasons at least, and sometimes both the electrons and the phonons are confined [FS21, BM23]. Therefore, we shall work under general hypotheses on the electron-phonon interaction covering the latter two situations as well as the original Fröhlich model.  Together with the inequalities established in this article, our Feynman–Kac formulas can form the basis for further studies of the semigroup and ground state eigenvectors (if any) in polaron models in analogy to the theory of magnetic Schrödinger semigroups [BHL00, Sim82] and its extensions to the related Pauli–Fierz model of non-relativistic quantum electrodynamics [Mat16] and Nelson’s model for nucleonmeson interactions [MM18, HM22]",
              "rank": 5
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12869.pdf",
      "paragraphs": [
        {
          "source": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation.",
          "translations": [
            {
              "id": "09-simplify-level12.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, we can easily create a comprehensive schedule that can solve most of the problems within the time limit set by the CASC competition. This implies that to prevent CASC from becoming a memory contest, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can compete with the best approaches. Given a time limit of about 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time limits, creating an optimal schedule becomes impractical, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main focus of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the quick runtime and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration.",
              "rank": 1
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "In this study, we independently evaluated the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each optimized for a single problem. These strategies allowed us to build a comprehensive schedule covering most solvable problems within the CASC competition's budget. This implies that CASC should incorporate a significant set of new problems each year to avoid becoming a memorization contest. \n\nWe suggested a greedy schedule construction method to create robust schedules using the discovered strategies, which can rival optimal methods. With a time budget of about 2 minutes, the greedy algorithm takes less than a minute to generate a schedule that solves nearly as many problems as an optimal schedule, which requires over 16 hours to create. For shorter time budgets, optimal schedule construction becomes unfeasible, while the greedy method continues to produce strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be enhanced by various regularization mechanisms, which are the main contribution of this study. A well-selected regularization allows us to surpass the optimal schedule on new problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 2
            },
            {
              "id": "06-simplify-college.ts",
              "content": "In this study, we independently assessed the Spider-style [33] approach to strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each individually optimized for a specific problem. With these strategies, constructing a comprehensive schedule that addresses most solvable problems within the CASC competition's budget is straightforward. This implies that to prevent CASC from becoming primarily a memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the identified strategies, we suggested a greedy schedule construction method that can rival optimal approaches. Given a time budget of roughly two minutes, the greedy algorithm requires less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which takes over 16 hours to create, can solve. For shorter time budgets, constructing an optimal schedule becomes impractical, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected efficacy of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization enables us to surpass the optimal schedule in solving unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further exploration.",
              "rank": 3
            },
            {
              "id": "01-writing-simple-improve.ts",
              "content": "In this study, we independently assessed the Spider-style strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library and gathered over a thousand Vampire proving strategies. Each of these strategies was optimized in advance to perform well on a single problem. With these strategies, we could easily build a comprehensive schedule that covers most of the solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a memory-based competition, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the discovered strategies, we suggested a greedy schedule construction method that can compete with optimal approaches. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems that an optimal schedule can solve, which takes more than 16 hours to generate. For shorter time budgets, creating an optimal schedule is no longer viable, but the greedy construction still produces relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 4
            },
            {
              "id": "02-writing-distill-and-condense.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, using over a thousand Vampire proving strategies from the FOF fragment of the TPTP library. The study suggests that the CASC competition should use a substantial set of previously unseen problems each year to avoid being a competition in memorization. A proposed greedy schedule construction procedure can compete with optimal approaches, taking less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate. The strength of the greedy scheduler can be reinforced by regularization mechanisms, which are the main contribution of this work. These mechanisms allow us to outperform the optimal schedule on unseen problems. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 5
            },
            {
              "id": "13-transpile-bulletpoints.ts",
              "content": "- Conducted an independent evaluation of Spider-style strategy discovery and schedule creation.\n- Collected over a thousand Vampire proving strategies, each optimized for a single problem.\n- Constructed a single schedule covering most solvable problems within the CASC competition budget.\n- Emphasized the need for CASC to use a substantial set of unseen problems each year to avoid being a memorization competition.\n- Proposed a greedy schedule construction procedure that can compete with optimal approaches.\n- The greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0% as many problems as an optimal schedule, which takes over 16 hours to generate.\n- For shorter time budgets, optimal schedule construction is not feasible, but greedy construction still produces strong schedules.\n- The strength of the greedy scheduler can be reinforced by various regularization mechanisms, which is the main contribution of this work.\n- An appropriately chosen regularization can outperform the optimal schedule on unseen problems.\n- The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 6
            },
            {
              "id": "14-transpile-summary.ts",
              "content": "The study conducted an independent evaluation of Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. Over a thousand Vampire proving strategies were collected, each optimized for a single problem. These strategies can be used to construct a single schedule that covers most solvable problems within the CASC competition's budget. The study suggests that CASC should use a substantial set of new problems each year to avoid becoming a memorization competition. A greedy schedule construction procedure was proposed, which can compete with optimal approaches. The greedy algorithm takes less than a minute to produce a schedule that solves over 99% of problems, compared to an optimal schedule that takes over 16 hours to generate. The strength of the greedy scheduler can be enhanced by regularization mechanisms, which is the main contribution of this work. The runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them suitable for reuse and further experimentation.",
              "rank": 7
            },
            {
              "id": "source",
              "content": "In this work, we conducted an independent evaluation of Spider-style [33] strategy discovery and schedule creation. Focusing on the FOF fragment of the TPTP library, we collected over a thousand Vampire proving strategies, each a priori optimized to perform well on a single problem. Using these strategies, it is easy to construct a single monolithic schedule which covers most of the problems known to be solvable within the budget used by the CASC competition. This suggests that for CASC not to be mainly a competition in memorization, using a substantial set of previously unseen problems each year is essential.  To construct strong schedules using the discovered strategies, we proposed a greedy schedule construction procedure, which can compete with optimal approaches. For a time budget of approximately 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves more than 99.0 % as many problems as an optimal schedule, which takes more than 16 hours to generate. For shorter time budgets, optimal schedule construction is no longer feasible, while greedy construction still produces relatively strong schedules.  This surprising strength of the greedy scheduler can be further reinforced by various regularization mechanisms, which constitute the main contribution of this work. An appropriately chosen regularization allows us to outperform the optimal schedule on unseen problems. Finally, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them attractive for reuse and further experimentation.",
              "rank": 8
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "In this study, we independently assessed a strategy known as Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, gathering over a thousand problem-solving strategies, each one specifically designed to excel at a single problem. With these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems solvable within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year. \n\nTo build robust plans using the strategies we discovered, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the best plan is no longer possible, while our quick method still produces relatively strong plans. \n\nThe surprising effectiveness of our quick planning method can be further enhanced by various adjustment mechanisms, which are the main focus of this study. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the speed and simplicity of our quick planning method and the adjustment techniques make them appealing for reuse and further exploration.",
              "rank": 9
            },
            {
              "id": "05-demographics-reader.ts",
              "content": "In this project, we independently assessed a strategy called Spider-style for problem-solving and planning. We focused on a specific part of the TPTP library, known as the FOF fragment. We gathered over a thousand problem-solving strategies from a tool called Vampire, each one specifically designed to excel at a single problem. \n\nWith these strategies, it's straightforward to create a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to keep the CASC competition from being primarily a memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo create robust plans using the strategies we discovered, we suggested a method called greedy schedule construction. This method can compete with the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that the best plan can, which takes over 16 hours to create. For shorter time limits, creating the best plan is no longer possible, while the greedy method still produces relatively strong plans.\n\nThe surprising effectiveness of the greedy method can be further enhanced by various adjustment mechanisms, which are the main focus of this project. A carefully selected adjustment allows us to perform better than the best plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy method and the adjustment techniques make them appealing for reuse and further exploration.",
              "rank": 10
            },
            {
              "id": "07-simplify-ignorant.ts",
              "content": "In this study, we tested a method called Spider-style strategy discovery and schedule creation. We focused on a specific part of the TPTP library, gathering over a thousand strategies from a system called Vampire. Each of these strategies was designed to work well on one specific problem. With these strategies, we can easily create a single, comprehensive plan that can solve most of the problems that can be solved within the time limit set by the CASC competition. This suggests that to keep the CASC competition from being just a memory test, it's important to use a lot of new problems each year. \n\nTo create strong plans using the strategies we found, we suggested a method that quickly builds a plan, which can compete with the best methods. For a time limit of about 2 minutes, this quick method takes less than a minute to create a plan that solves almost as many problems as the best plan, which takes over 16 hours to create. For shorter time limits, creating the best plan isn't possible, but our quick method still creates relatively strong plans. \n\nThis surprising effectiveness of our quick method can be improved even more by using various adjustment mechanisms, which is the main focus of this study. Choosing the right adjustment allows us to do better than the best plan on new problems. Lastly, the quickness and simplicity of our quick plan creation method and the adjustment techniques make them good options for reuse and further testing.",
              "rank": 11
            },
            {
              "id": "16-experimental-diff-representation.ts",
              "content": "In this study, we independently assessed the Spider-style [33] method of strategy discovery and schedule creation. We focused on the FOF fragment of the TPTP library, gathering over a thousand Vampire proving strategies, each one a priori optimized for a single problem. With these strategies, it's simple to build a single, comprehensive schedule that can solve most of the problems solvable within the CASC competition's budget. This implies that to prevent CASC from becoming primarily a memory contest, it's crucial to introduce a significant number of new problems each year. To build robust schedules using the discovered strategies, we suggested a greedy schedule construction process that can compete with optimal methods. Given a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to create a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to generate. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. The surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the main contribution of this study. A carefully selected regularization allows us to outperform the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 12
            },
            {
              "id": "11-role-saul-kahn.ts",
              "content": "In our exploration, we embarked on an autonomous assessment of the Spider-style [33] approach to strategy discovery and schedule creation. Our focus was the FOF fragment of the TPTP library, from which we gathered over a thousand Vampire proving strategies, each one meticulously optimized to excel on a singular problem. With these strategies in hand, the construction of a single, all-encompassing schedule that covers the majority of solvable problems within the CASC competition's budget becomes a straightforward task. This implies that to prevent CASC from becoming a mere memory contest, it is crucial to introduce a significant number of new problems each year. \n\nTo build robust schedules using the unearthed strategies, we put forth a greedy schedule construction method, capable of holding its own against optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm takes less than a minute to generate a schedule that solves over 99.0% of the problems that an optimal schedule, which requires over 16 hours to create, can solve. As the time budget shrinks, the feasibility of constructing an optimal schedule diminishes, while the greedy method continues to yield relatively strong schedules. \n\nThe unexpected potency of the greedy scheduler can be further bolstered by various regularization mechanisms, which form the crux of our contribution in this study. A carefully selected regularization enables us to surpass the optimal schedule's performance on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm, coupled with the regularization techniques, make them appealing for reuse and further exploration.",
              "rank": 13
            },
            {
              "id": "17-combine-12-13-03-06.ts",
              "content": "As an esteemed educator, I would like to assist you in comprehending this complex subject matter. \n\nIn our study, we independently assessed the Spider-style strategy discovery and schedule creation, focusing on the FOF fragment of the TPTP library. We gathered over a thousand Vampire proving strategies, each individually optimized for a single problem. With these strategies, we can easily create a comprehensive schedule that addresses most solvable problems within the CASC competition's budget. This implies that to prevent CASC from becoming a mere memory contest, it's crucial to introduce a significant set of new problems each year. \n\nWe suggested a greedy schedule construction method to build robust schedules using the discovered strategies, which can rival optimal methods. For a time budget of roughly 2 minutes, the greedy algorithm takes less than a minute to produce a schedule that solves over 99.0% of the problems an optimal schedule can, which takes over 16 hours to create. For shorter time budgets, constructing an optimal schedule becomes unfeasible, while the greedy method continues to produce relatively strong schedules. \n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various regularization mechanisms, which are the primary contribution of this study. A carefully selected regularization allows us to surpass the optimal schedule on unseen problems. Lastly, the runtime speed and simplicity of the greedy schedule construction algorithm and the regularization techniques make them appealing for reuse and further experimentation.",
              "rank": 14
            },
            {
              "id": "12-role-teacher.ts",
              "content": "In this study, we independently assessed a method known as the Spider-style strategy for problem-solving and planning. We focused on a specific section of the TPTP library, known as the FOF fragment. From this, we gathered over a thousand strategies from a system called Vampire, each one fine-tuned to excel at solving a single problem. \n\nWith these strategies, it's quite straightforward to build a comprehensive plan that can tackle most of the problems that can be solved within the time limit set by the CASC competition. This implies that to prevent the CASC from becoming a mere memory test, it's crucial to introduce a significant number of new problems each year.\n\nTo build robust plans using the strategies we discovered, we suggested a method called the greedy schedule construction procedure. This method can hold its own against the best approaches. Given a time limit of about 2 minutes, the greedy method takes less than a minute to create a plan that solves over 99.0% of the problems that an ideal plan, which takes over 16 hours to create, can solve. For shorter time limits, creating the ideal plan is no longer practical, while the greedy method continues to produce relatively strong plans.\n\nThe surprising effectiveness of the greedy scheduler can be further enhanced by various balancing mechanisms, which are the main focus of this study. A carefully selected balancing mechanism allows us to outperform the ideal plan on unseen problems. Lastly, the quick execution time and simplicity of the greedy plan creation method and the balancing techniques make them appealing for reuse and further exploration.",
              "rank": 15
            },
            {
              "id": "08-simplify-level6.ts",
              "content": "In this project, we tested a strategy called Spider-style [33] to solve problems and make plans. We used a part of the TPTP library called the FOF fragment and gathered over a thousand different ways to solve problems using a tool called Vampire. Each of these ways was designed to work best on one specific problem. With these strategies, we can make a big plan that can solve most of the problems that we know can be solved within the time limit set by the CASC competition. This shows that for the CASC competition to be fair, it's important to use new problems each year that haven't been seen before. \n\nTo make strong plans using the strategies we found, we suggested a method that picks the best option available at each step, which we call a greedy schedule construction procedure. This method can compete with the best possible approaches. If we have about 2 minutes, this method takes less than a minute to make a plan that solves almost as many problems as the best possible plan, which takes over 16 hours to make. If we have less time, making the best possible plan isn't doable, but our method can still make pretty good plans. \n\nThe surprising power of our method can be made even stronger by using different techniques, which is the main thing we're showing in this project. If we pick the right technique, we can do better than the best possible plan on problems we haven't seen before. Lastly, our method is fast and simple, which makes it a good choice for use in other projects and for more testing.",
              "rank": 16
            },
            {
              "id": "15-experimental-ai-generated.ts",
              "content": "In this study, we tested a method called Spider-style to find and make plans. We used a part of the TPTP library called FOF and gathered over a thousand strategies from a tool called Vampire. Each strategy was made to work well on one problem. With these strategies, we can make a big plan that can solve most of the problems that can be solved within the time limit of the CASC competition. This shows that the CASC competition should use new problems each year, so it's not just about remembering old solutions. \n\nTo make good plans with the strategies we found, we suggested a method that quickly makes a plan. This method can compete with the best methods. If we have about 2 minutes, this quick method takes less than a minute to make a plan that solves almost as many problems as the best plan, which takes more than 16 hours to make. If we have less time, making the best plan is not possible, but our quick method still makes good plans. \n\nThe strength of our quick method can be made even better with some adjustments. These adjustments are the main part of our study. If we choose the right adjustment, we can do better than the best plan on new problems. Lastly, our quick method and the adjustments are easy to use and fast, which makes them good for using again and trying new things.",
              "rank": 17
            },
            {
              "id": "10-role-carl-sagan.ts",
              "content": "In this cosmic endeavor, we embarked on an autonomous exploration of the Spider-style [33] strategy discovery and schedule creation, akin to a solitary spacecraft traversing the vast expanse of the universe. Our focus was the FOF fragment of the TPTP library, a celestial body of knowledge, from which we harvested over a thousand Vampire proving strategies. Each of these strategies, like a star in the cosmos, was uniquely optimized to shine brightly on a single problem.\n\nWith these strategies in our cosmic toolkit, we found it straightforward to assemble a single, monolithic schedule, much like a constellation that encompasses most of the problems known to be solvable within the budgetary constraints of the CASC competition. This observation led us to the realization that the CASC competition, to avoid becoming a mere contest of memory, must incorporate a substantial set of fresh, uncharted problems each year.\n\nTo construct robust schedules using the unearthed strategies, we proposed a greedy schedule construction procedure, akin to a black hole, consuming all in its path, yet capable of competing with the most optimal approaches. Given a time budget of roughly 2 minutes, this greedy algorithm, with the speed of a comet, takes less than a minute to produce a schedule that solves over 99.0% of the problems that an optimal schedule, which requires more than 16 hours to generate, can solve. As the time budget shrinks, optimal schedule construction becomes as unfeasible as traveling faster than light, while the greedy construction continues to produce relatively strong schedules.\n\nThe surprising potency of the greedy scheduler, much like the unexpected power of a supernova, can be further amplified by various regularization mechanisms. These mechanisms, like the gravitational forces that hold our universe together, form the core contribution of this work. An astutely selected regularization, like a well-placed telescope, allows us to outperform the optimal schedule on unseen problems.\n\nIn conclusion, the runtime speed and simplicity of the greedy schedule construction algorithm, along with the regularization techniques, are as attractive for reuse and further experimentation as the mysteries of the cosmos are to an astronomer.",
              "rank": 18
            }
          ]
        }
      ]
    },
    {
      "id": "https://arxiv.org/pdf/2403.12768.pdf",
      "paragraphs": [
        {
          "source": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
          "translations": [
            {
              "id": "source",
              "content": "To encourage divergent thinking and foster customization, educators can further generate variant scripts and utilize alternative prompts, expanding the possibilities for tailoring the output content to specific learning objectives. Upon selecting an item in the Stickers panel, the current prompt associated with the chosen sticker is displayed in the Refine panel. Educators can edit the prompt and regenerate stickers to better fit the intended context, while simultaneously mitigating the risk of generating potentially inappropriate content.  Moreover, by leveraging varied themes and contexts, educators can create more engaging learning contents that not only reinforce vocabulary acquisition but also promote linguistic adaptability, as shown in Fig. 4. By offering a diverse array of learning materials, educators can stimulate more dynamic forms of vocabulary engagement and deepen learners’ connections with the language.",
              "rank": 1
            },
            {
              "id": "03-writing-technical-improve.ts",
              "content": "\"To promote divergent thinking and customization, educators can create different scripts and use alternative prompts, broadening the potential to tailor content to specific learning objectives. When an item is selected in the Stickers panel, the associated prompt is displayed in the Refine panel. Educators can modify the prompt and create new stickers to suit the context, reducing the risk of inappropriate content. Additionally, using various themes and contexts, educators can make learning content more engaging, reinforcing vocabulary and promoting linguistic adaptability, as shown in Fig. 4. Providing a range of learning materials can encourage dynamic vocabulary engagement and strengthen learners’ language connections.\"",
              "rank": 2
            },
            {
              "id": "04-demographics-author-reader.ts",
              "content": "To make learning more creative and personalized, teachers can create different scripts and use various prompts. This allows them to adjust the content to match specific learning goals. When a teacher selects an item in the Stickers panel, the current prompt linked to that sticker shows up in the Refine panel. Teachers can change this prompt and create new stickers that better suit the context they're teaching in. This also helps avoid creating content that might be inappropriate. \n\nFurthermore, by using a range of themes and contexts, teachers can make learning more interesting. This not only helps students learn new words but also helps them adapt to different language uses, as shown in Fig. 4. By providing a wide variety of learning materials, teachers can encourage students to engage with vocabulary in more dynamic ways and strengthen their relationship with the language.",
              "rank": 3
            },
            {
              "id": "09-simplify-level12.ts",
              "content": "To promote creative thinking and allow for personalization, teachers can create different scripts and use various prompts, broadening the scope for customizing the material to meet specific educational goals. When a sticker is chosen from the Stickers panel, the current prompt linked to that sticker appears in the Refine panel. Teachers can modify this prompt and create new stickers to better suit the intended setting, while also reducing the chance of creating potentially unsuitable content. Furthermore, by using a range of themes and contexts, teachers can develop more captivating educational content that not only strengthens vocabulary learning but also encourages language adaptability, as demonstrated in Fig. 4. By providing a wide variety of learning resources, teachers can encourage more active vocabulary engagement and strengthen students' relationship with the language.",
              "rank": 4
            },
            {
              "id": "12-role-teacher.ts",
              "content": "As a distinguished educator, I would like to share some strategies to inspire creative thinking and promote personalization in learning. We, as teachers, can create a variety of lesson plans and use different questions or prompts, thereby broadening the scope for customizing the learning content to meet specific educational goals. \n\nLet's take an example. When you choose an item from the Stickers panel, the current question or prompt linked to that sticker appears in the Refine panel. As teachers, we have the ability to modify this prompt and create new stickers that are more suitable for the context we're teaching in. This not only helps us to ensure the content is relevant and appropriate, but also reduces the chance of generating any content that may be unsuitable.\n\nFurthermore, by using a range of themes and contexts, we can make the learning content more interesting and engaging. This not only strengthens vocabulary learning but also encourages students to adapt their language use to different situations, as illustrated in Figure 4. \n\nBy providing a wide variety of learning resources, we can encourage more active engagement with vocabulary and strengthen students' relationship with the language.",
              "rank": 5
            }
          ]
        }
      ]
    }
  ]
}